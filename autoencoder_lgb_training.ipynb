{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('py38': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3da45907e5cd41bdac7b692b424c19725633d98a853f93b11b400b1d8951ac30"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                        StudyInstanceUID  PatientID         0  \\\n0      1.2.826.0.1.3680043.8.498.26697628953273228189...  ec89415d1  0.169359   \n1      1.2.826.0.1.3680043.8.498.46302891597398758759...  bf4c6da3c  0.365546   \n2      1.2.826.0.1.3680043.8.498.23819260719748494858...  3fc1c97e5  0.211337   \n3      1.2.826.0.1.3680043.8.498.68286643202323212801...  c31019814  0.520994   \n4      1.2.826.0.1.3680043.8.498.10050203009225938259...  207685cd1  0.383775   \n...                                                  ...        ...       ...   \n30078  1.2.826.0.1.3680043.8.498.74257566841157531124...  5b5b9ac30  0.152407   \n30079  1.2.826.0.1.3680043.8.498.46510939987173529969...  7192404d8  0.303968   \n30080  1.2.826.0.1.3680043.8.498.43173270582850645437...  d4d1b066d  0.316939   \n30081  1.2.826.0.1.3680043.8.498.95092491950130838685...  01a6602b8  0.239194   \n30082  1.2.826.0.1.3680043.8.498.99518162226171269731...  e692d316c  0.038298   \n\n              1         2         3         4         5         6         7  \\\n0      0.324016  0.173553  0.095382  0.063119  0.165738  0.181906  0.437814   \n1      0.175262  0.286057  0.205560  0.388747  0.452293  0.021495  0.460131   \n2      0.303350  0.184402  0.248883  0.202594  0.301177  0.208046  0.416033   \n3      0.385111  0.367907  0.247489  0.097945  0.448468  0.377076  0.108094   \n4      0.214603  0.263212  0.429333  0.619226  0.365313  0.264323  0.201577   \n...         ...       ...       ...       ...       ...       ...       ...   \n30078  0.273989  0.429204  0.095627  0.143194  0.151159  0.190642  0.151711   \n30079  0.299640  0.232293  0.115276  0.309395  0.338309  0.164800  0.221858   \n30080  0.351076  0.589629  0.291154  0.185748  0.232997  0.113028  0.353113   \n30081  0.393650  0.438603  0.363903  0.144577  0.259132  0.366278  0.193645   \n30082  0.476334  0.110016  0.089027  0.106914  0.169113  0.072551  0.151309   \n\n       ...      2550      2551      2552      2553      2554      2555  \\\n0      ...  0.302277  0.070963  0.354162  0.284205  0.057408  0.101549   \n1      ...  0.232927  0.223938  0.135535  0.042117  0.169312  0.383767   \n2      ...  0.370746  0.315548  0.384991  0.181793  0.134341  0.147335   \n3      ...  0.355882  0.156018  0.316916  0.141020  0.041345  0.297808   \n4      ...  0.302169  0.318546  0.097627  0.337127  0.124034  0.266738   \n...    ...       ...       ...       ...       ...       ...       ...   \n30078  ...  0.417215  0.330231  0.257251  0.193722  0.070763  0.415085   \n30079  ...  0.509008  0.143263  0.214838  0.167794  0.075009  0.163606   \n30080  ...  0.542412  0.236196  0.170918  0.283692  0.270579  0.275420   \n30081  ...  0.387640  0.075916  0.316283  0.119015  0.101108  0.645644   \n30082  ...  0.394486  0.098980  0.253622  0.046012  0.068931  0.088179   \n\n           2556      2557      2558      2559  \n0      0.200983  0.098618  0.276763  0.473490  \n1      0.095033  0.151809  0.048868  0.117311  \n2      0.313144  0.356484  0.112562  0.404958  \n3      0.273713  0.329452  0.203884  0.219169  \n4      0.595066  0.150074  0.454537  0.071115  \n...         ...       ...       ...       ...  \n30078  0.388353  0.156808  0.239679  0.161568  \n30079  0.415589  0.059742  0.148673  0.164074  \n30080  0.285688  0.203561  0.118533  0.222272  \n30081  0.305596  0.319096  0.330187  0.278469  \n30082  0.348486  0.140222  0.289026  0.741221  \n\n[30083 rows x 2562 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>PatientID</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>2550</th>\n      <th>2551</th>\n      <th>2552</th>\n      <th>2553</th>\n      <th>2554</th>\n      <th>2555</th>\n      <th>2556</th>\n      <th>2557</th>\n      <th>2558</th>\n      <th>2559</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.8.498.26697628953273228189...</td>\n      <td>ec89415d1</td>\n      <td>0.169359</td>\n      <td>0.324016</td>\n      <td>0.173553</td>\n      <td>0.095382</td>\n      <td>0.063119</td>\n      <td>0.165738</td>\n      <td>0.181906</td>\n      <td>0.437814</td>\n      <td>...</td>\n      <td>0.302277</td>\n      <td>0.070963</td>\n      <td>0.354162</td>\n      <td>0.284205</td>\n      <td>0.057408</td>\n      <td>0.101549</td>\n      <td>0.200983</td>\n      <td>0.098618</td>\n      <td>0.276763</td>\n      <td>0.473490</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.8.498.46302891597398758759...</td>\n      <td>bf4c6da3c</td>\n      <td>0.365546</td>\n      <td>0.175262</td>\n      <td>0.286057</td>\n      <td>0.205560</td>\n      <td>0.388747</td>\n      <td>0.452293</td>\n      <td>0.021495</td>\n      <td>0.460131</td>\n      <td>...</td>\n      <td>0.232927</td>\n      <td>0.223938</td>\n      <td>0.135535</td>\n      <td>0.042117</td>\n      <td>0.169312</td>\n      <td>0.383767</td>\n      <td>0.095033</td>\n      <td>0.151809</td>\n      <td>0.048868</td>\n      <td>0.117311</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.8.498.23819260719748494858...</td>\n      <td>3fc1c97e5</td>\n      <td>0.211337</td>\n      <td>0.303350</td>\n      <td>0.184402</td>\n      <td>0.248883</td>\n      <td>0.202594</td>\n      <td>0.301177</td>\n      <td>0.208046</td>\n      <td>0.416033</td>\n      <td>...</td>\n      <td>0.370746</td>\n      <td>0.315548</td>\n      <td>0.384991</td>\n      <td>0.181793</td>\n      <td>0.134341</td>\n      <td>0.147335</td>\n      <td>0.313144</td>\n      <td>0.356484</td>\n      <td>0.112562</td>\n      <td>0.404958</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.8.498.68286643202323212801...</td>\n      <td>c31019814</td>\n      <td>0.520994</td>\n      <td>0.385111</td>\n      <td>0.367907</td>\n      <td>0.247489</td>\n      <td>0.097945</td>\n      <td>0.448468</td>\n      <td>0.377076</td>\n      <td>0.108094</td>\n      <td>...</td>\n      <td>0.355882</td>\n      <td>0.156018</td>\n      <td>0.316916</td>\n      <td>0.141020</td>\n      <td>0.041345</td>\n      <td>0.297808</td>\n      <td>0.273713</td>\n      <td>0.329452</td>\n      <td>0.203884</td>\n      <td>0.219169</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.8.498.10050203009225938259...</td>\n      <td>207685cd1</td>\n      <td>0.383775</td>\n      <td>0.214603</td>\n      <td>0.263212</td>\n      <td>0.429333</td>\n      <td>0.619226</td>\n      <td>0.365313</td>\n      <td>0.264323</td>\n      <td>0.201577</td>\n      <td>...</td>\n      <td>0.302169</td>\n      <td>0.318546</td>\n      <td>0.097627</td>\n      <td>0.337127</td>\n      <td>0.124034</td>\n      <td>0.266738</td>\n      <td>0.595066</td>\n      <td>0.150074</td>\n      <td>0.454537</td>\n      <td>0.071115</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>1.2.826.0.1.3680043.8.498.74257566841157531124...</td>\n      <td>5b5b9ac30</td>\n      <td>0.152407</td>\n      <td>0.273989</td>\n      <td>0.429204</td>\n      <td>0.095627</td>\n      <td>0.143194</td>\n      <td>0.151159</td>\n      <td>0.190642</td>\n      <td>0.151711</td>\n      <td>...</td>\n      <td>0.417215</td>\n      <td>0.330231</td>\n      <td>0.257251</td>\n      <td>0.193722</td>\n      <td>0.070763</td>\n      <td>0.415085</td>\n      <td>0.388353</td>\n      <td>0.156808</td>\n      <td>0.239679</td>\n      <td>0.161568</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>1.2.826.0.1.3680043.8.498.46510939987173529969...</td>\n      <td>7192404d8</td>\n      <td>0.303968</td>\n      <td>0.299640</td>\n      <td>0.232293</td>\n      <td>0.115276</td>\n      <td>0.309395</td>\n      <td>0.338309</td>\n      <td>0.164800</td>\n      <td>0.221858</td>\n      <td>...</td>\n      <td>0.509008</td>\n      <td>0.143263</td>\n      <td>0.214838</td>\n      <td>0.167794</td>\n      <td>0.075009</td>\n      <td>0.163606</td>\n      <td>0.415589</td>\n      <td>0.059742</td>\n      <td>0.148673</td>\n      <td>0.164074</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>1.2.826.0.1.3680043.8.498.43173270582850645437...</td>\n      <td>d4d1b066d</td>\n      <td>0.316939</td>\n      <td>0.351076</td>\n      <td>0.589629</td>\n      <td>0.291154</td>\n      <td>0.185748</td>\n      <td>0.232997</td>\n      <td>0.113028</td>\n      <td>0.353113</td>\n      <td>...</td>\n      <td>0.542412</td>\n      <td>0.236196</td>\n      <td>0.170918</td>\n      <td>0.283692</td>\n      <td>0.270579</td>\n      <td>0.275420</td>\n      <td>0.285688</td>\n      <td>0.203561</td>\n      <td>0.118533</td>\n      <td>0.222272</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>1.2.826.0.1.3680043.8.498.95092491950130838685...</td>\n      <td>01a6602b8</td>\n      <td>0.239194</td>\n      <td>0.393650</td>\n      <td>0.438603</td>\n      <td>0.363903</td>\n      <td>0.144577</td>\n      <td>0.259132</td>\n      <td>0.366278</td>\n      <td>0.193645</td>\n      <td>...</td>\n      <td>0.387640</td>\n      <td>0.075916</td>\n      <td>0.316283</td>\n      <td>0.119015</td>\n      <td>0.101108</td>\n      <td>0.645644</td>\n      <td>0.305596</td>\n      <td>0.319096</td>\n      <td>0.330187</td>\n      <td>0.278469</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>1.2.826.0.1.3680043.8.498.99518162226171269731...</td>\n      <td>e692d316c</td>\n      <td>0.038298</td>\n      <td>0.476334</td>\n      <td>0.110016</td>\n      <td>0.089027</td>\n      <td>0.106914</td>\n      <td>0.169113</td>\n      <td>0.072551</td>\n      <td>0.151309</td>\n      <td>...</td>\n      <td>0.394486</td>\n      <td>0.098980</td>\n      <td>0.253622</td>\n      <td>0.046012</td>\n      <td>0.068931</td>\n      <td>0.088179</td>\n      <td>0.348486</td>\n      <td>0.140222</td>\n      <td>0.289026</td>\n      <td>0.741221</td>\n    </tr>\n  </tbody>\n</table>\n<p>30083 rows × 2562 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "input_dir=\"../input/efficientnet_output_straight/\"\n",
    "dataset_dir=\"../input/ranzcr-clip-catheter-line-classification/\"\n",
    "\n",
    "input_df=pd.read_csv(\"./models/efficientnet_output_normalized.csv\")\n",
    "display(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 1800)              4609800   \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               180100    \n_________________________________________________________________\ndense_2 (Dense)              (None, 1800)              181800    \n_________________________________________________________________\ndropout (Dropout)            (None, 1800)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 2560)              4610560   \n=================================================================\nTotal params: 9,582,260\nTrainable params: 9,582,260\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=load_model(\"./models/autoencoder_splits10/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_score():\n",
    "    group_kfold=GroupKFold(n_splits=10)\n",
    "    for n,(train_idx,test_idx) in enumerate(group_kfold.split(input_df.iloc[:,2:],groups=input_df.iloc[:,2])):\n",
    "        X_train=input_df.iloc[train_idx,2:]\n",
    "        X_test=input_df.iloc[test_idx,2:]\n",
    "        score=model.evaluate(X_test,X_test,verbose=0)\n",
    "        print(f\"fold{n} score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                        StudyInstanceUID    0         1  \\\n0      1.2.826.0.1.3680043.8.498.26697628953273228189...  0.0  3.523285   \n1      1.2.826.0.1.3680043.8.498.46302891597398758759...  0.0  2.752369   \n2      1.2.826.0.1.3680043.8.498.23819260719748494858...  0.0  5.127551   \n3      1.2.826.0.1.3680043.8.498.68286643202323212801...  0.0  2.767875   \n4      1.2.826.0.1.3680043.8.498.10050203009225938259...  0.0  5.906751   \n...                                                  ...  ...       ...   \n30078  1.2.826.0.1.3680043.8.498.74257566841157531124...  0.0  2.509894   \n30079  1.2.826.0.1.3680043.8.498.46510939987173529969...  0.0  3.314908   \n30080  1.2.826.0.1.3680043.8.498.43173270582850645437...  0.0  5.604525   \n30081  1.2.826.0.1.3680043.8.498.95092491950130838685...  0.0  5.239021   \n30082  1.2.826.0.1.3680043.8.498.99518162226171269731...  0.0  2.697263   \n\n              2         3         4         5         6    7         8  ...  \\\n0      2.151111  3.073467  3.363639  3.601960  1.177225  0.0  3.399705  ...   \n1      2.418850  2.875795  3.922628  5.291559  4.010684  0.0  4.176135  ...   \n2      4.384488  8.046625  5.567223  6.195307  7.095205  0.0  4.194753  ...   \n3      1.731207  5.098400  2.484329  3.534876  4.957430  0.0  6.703936  ...   \n4      2.528070  5.296381  2.448073  2.876235  8.141816  0.0  4.746807  ...   \n...         ...       ...       ...       ...       ...  ...       ...  ...   \n30078  2.170190  3.057890  5.210413  4.008926  4.459599  0.0  4.928145  ...   \n30079  2.372607  1.440810  2.099609  3.282451  4.099072  0.0  3.791726  ...   \n30080  2.085924  4.015664  4.236988  6.777108  4.505717  0.0  5.942400  ...   \n30081  1.398003  3.633277  4.615515  3.244748  3.063015  0.0  6.512488  ...   \n30082  2.477276  3.014830  3.863428  2.657923  2.588749  0.0  2.194187  ...   \n\n        90        91        92        93        94        95        96  \\\n0      0.0  1.328145  1.331692  1.144717  2.307059  2.733480  2.879669   \n1      0.0  1.365769  3.142598  6.856853  2.623820  4.440138  6.352470   \n2      0.0  1.316996  5.129144  5.434055  5.875688  4.957835  5.093932   \n3      0.0  0.875055  3.810669  2.164724  1.225728  1.915580  2.081527   \n4      0.0  1.980754  6.254673  1.629905  3.840276  2.863812  4.631470   \n...    ...       ...       ...       ...       ...       ...       ...   \n30078  0.0  2.131990  4.075906  4.149724  2.756427  2.651626  5.739603   \n30079  0.0  1.977184  6.820914  5.900064  3.299438  3.492918  4.771842   \n30080  0.0  1.665230  5.557427  4.285787  2.033923  4.248517  4.583074   \n30081  0.0  1.476729  3.201416  1.321704  2.968237  3.272295  2.854025   \n30082  0.0  1.923943  2.995123  2.970642  3.018536  6.120244  4.012001   \n\n              97        98        99  \n0       4.131839  3.130108  2.915718  \n1       7.112836  8.021735  6.673033  \n2       4.814246  1.765945  7.694550  \n3       1.964060  2.458131  4.171708  \n4       4.077279  5.729913  6.588160  \n...          ...       ...       ...  \n30078   5.785378  3.671273  3.416882  \n30079   4.394142  3.231114  3.616853  \n30080  10.027946  7.137775  5.121286  \n30081   0.896478  3.066891  2.736391  \n30082   7.346872  2.355797  3.723385  \n\n[30083 rows x 101 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>...</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.8.498.26697628953273228189...</td>\n      <td>0.0</td>\n      <td>3.523285</td>\n      <td>2.151111</td>\n      <td>3.073467</td>\n      <td>3.363639</td>\n      <td>3.601960</td>\n      <td>1.177225</td>\n      <td>0.0</td>\n      <td>3.399705</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.328145</td>\n      <td>1.331692</td>\n      <td>1.144717</td>\n      <td>2.307059</td>\n      <td>2.733480</td>\n      <td>2.879669</td>\n      <td>4.131839</td>\n      <td>3.130108</td>\n      <td>2.915718</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.8.498.46302891597398758759...</td>\n      <td>0.0</td>\n      <td>2.752369</td>\n      <td>2.418850</td>\n      <td>2.875795</td>\n      <td>3.922628</td>\n      <td>5.291559</td>\n      <td>4.010684</td>\n      <td>0.0</td>\n      <td>4.176135</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.365769</td>\n      <td>3.142598</td>\n      <td>6.856853</td>\n      <td>2.623820</td>\n      <td>4.440138</td>\n      <td>6.352470</td>\n      <td>7.112836</td>\n      <td>8.021735</td>\n      <td>6.673033</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.8.498.23819260719748494858...</td>\n      <td>0.0</td>\n      <td>5.127551</td>\n      <td>4.384488</td>\n      <td>8.046625</td>\n      <td>5.567223</td>\n      <td>6.195307</td>\n      <td>7.095205</td>\n      <td>0.0</td>\n      <td>4.194753</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.316996</td>\n      <td>5.129144</td>\n      <td>5.434055</td>\n      <td>5.875688</td>\n      <td>4.957835</td>\n      <td>5.093932</td>\n      <td>4.814246</td>\n      <td>1.765945</td>\n      <td>7.694550</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.8.498.68286643202323212801...</td>\n      <td>0.0</td>\n      <td>2.767875</td>\n      <td>1.731207</td>\n      <td>5.098400</td>\n      <td>2.484329</td>\n      <td>3.534876</td>\n      <td>4.957430</td>\n      <td>0.0</td>\n      <td>6.703936</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.875055</td>\n      <td>3.810669</td>\n      <td>2.164724</td>\n      <td>1.225728</td>\n      <td>1.915580</td>\n      <td>2.081527</td>\n      <td>1.964060</td>\n      <td>2.458131</td>\n      <td>4.171708</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.8.498.10050203009225938259...</td>\n      <td>0.0</td>\n      <td>5.906751</td>\n      <td>2.528070</td>\n      <td>5.296381</td>\n      <td>2.448073</td>\n      <td>2.876235</td>\n      <td>8.141816</td>\n      <td>0.0</td>\n      <td>4.746807</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.980754</td>\n      <td>6.254673</td>\n      <td>1.629905</td>\n      <td>3.840276</td>\n      <td>2.863812</td>\n      <td>4.631470</td>\n      <td>4.077279</td>\n      <td>5.729913</td>\n      <td>6.588160</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>1.2.826.0.1.3680043.8.498.74257566841157531124...</td>\n      <td>0.0</td>\n      <td>2.509894</td>\n      <td>2.170190</td>\n      <td>3.057890</td>\n      <td>5.210413</td>\n      <td>4.008926</td>\n      <td>4.459599</td>\n      <td>0.0</td>\n      <td>4.928145</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2.131990</td>\n      <td>4.075906</td>\n      <td>4.149724</td>\n      <td>2.756427</td>\n      <td>2.651626</td>\n      <td>5.739603</td>\n      <td>5.785378</td>\n      <td>3.671273</td>\n      <td>3.416882</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>1.2.826.0.1.3680043.8.498.46510939987173529969...</td>\n      <td>0.0</td>\n      <td>3.314908</td>\n      <td>2.372607</td>\n      <td>1.440810</td>\n      <td>2.099609</td>\n      <td>3.282451</td>\n      <td>4.099072</td>\n      <td>0.0</td>\n      <td>3.791726</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.977184</td>\n      <td>6.820914</td>\n      <td>5.900064</td>\n      <td>3.299438</td>\n      <td>3.492918</td>\n      <td>4.771842</td>\n      <td>4.394142</td>\n      <td>3.231114</td>\n      <td>3.616853</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>1.2.826.0.1.3680043.8.498.43173270582850645437...</td>\n      <td>0.0</td>\n      <td>5.604525</td>\n      <td>2.085924</td>\n      <td>4.015664</td>\n      <td>4.236988</td>\n      <td>6.777108</td>\n      <td>4.505717</td>\n      <td>0.0</td>\n      <td>5.942400</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.665230</td>\n      <td>5.557427</td>\n      <td>4.285787</td>\n      <td>2.033923</td>\n      <td>4.248517</td>\n      <td>4.583074</td>\n      <td>10.027946</td>\n      <td>7.137775</td>\n      <td>5.121286</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>1.2.826.0.1.3680043.8.498.95092491950130838685...</td>\n      <td>0.0</td>\n      <td>5.239021</td>\n      <td>1.398003</td>\n      <td>3.633277</td>\n      <td>4.615515</td>\n      <td>3.244748</td>\n      <td>3.063015</td>\n      <td>0.0</td>\n      <td>6.512488</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.476729</td>\n      <td>3.201416</td>\n      <td>1.321704</td>\n      <td>2.968237</td>\n      <td>3.272295</td>\n      <td>2.854025</td>\n      <td>0.896478</td>\n      <td>3.066891</td>\n      <td>2.736391</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>1.2.826.0.1.3680043.8.498.99518162226171269731...</td>\n      <td>0.0</td>\n      <td>2.697263</td>\n      <td>2.477276</td>\n      <td>3.014830</td>\n      <td>3.863428</td>\n      <td>2.657923</td>\n      <td>2.588749</td>\n      <td>0.0</td>\n      <td>2.194187</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.923943</td>\n      <td>2.995123</td>\n      <td>2.970642</td>\n      <td>3.018536</td>\n      <td>6.120244</td>\n      <td>4.012001</td>\n      <td>7.346872</td>\n      <td>2.355797</td>\n      <td>3.723385</td>\n    </tr>\n  </tbody>\n</table>\n<p>30083 rows × 101 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "layer_name=\"dense_1\"\n",
    "hidden_layer_model=Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "pred=hidden_layer_model.predict(input_df.iloc[:,2:])\n",
    "features=pd.concat([input_df.iloc[:,:1],pd.DataFrame(pred)],axis=1)\n",
    "display(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir=\"../input/ranzcr-clip-catheter-line-classification/\"\n",
    "models_dir=\"./models/\"\n",
    "\n",
    "n_folds=4\n",
    "num_features=100\n",
    "target_cols=['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal', 'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', 'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal', 'Swan Ganz Catheter Present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                        StudyInstanceUID  ETT - Abnormal  \\\n0      1.2.826.0.1.3680043.8.498.26697628953273228189...               0   \n1      1.2.826.0.1.3680043.8.498.46302891597398758759...               0   \n2      1.2.826.0.1.3680043.8.498.23819260719748494858...               0   \n3      1.2.826.0.1.3680043.8.498.68286643202323212801...               0   \n4      1.2.826.0.1.3680043.8.498.10050203009225938259...               0   \n...                                                  ...             ...   \n30078  1.2.826.0.1.3680043.8.498.74257566841157531124...               0   \n30079  1.2.826.0.1.3680043.8.498.46510939987173529969...               0   \n30080  1.2.826.0.1.3680043.8.498.43173270582850645437...               0   \n30081  1.2.826.0.1.3680043.8.498.95092491950130838685...               0   \n30082  1.2.826.0.1.3680043.8.498.99518162226171269731...               0   \n\n       ETT - Borderline  ETT - Normal  NGT - Abnormal  NGT - Borderline  \\\n0                     0             0               0                 0   \n1                     0             1               0                 0   \n2                     0             0               0                 0   \n3                     0             0               0                 0   \n4                     0             0               0                 0   \n...                 ...           ...             ...               ...   \n30078                 0             1               0                 0   \n30079                 0             0               0                 0   \n30080                 0             1               0                 0   \n30081                 0             0               0                 0   \n30082                 0             1               0                 0   \n\n       NGT - Incompletely Imaged  NGT - Normal  CVC - Abnormal  \\\n0                              0             1               0   \n1                              1             0               0   \n2                              0             0               0   \n3                              0             0               1   \n4                              0             0               0   \n...                          ...           ...             ...   \n30078                          0             0               0   \n30079                          0             0               0   \n30080                          1             0               1   \n30081                          0             0               0   \n30082                          0             0               0   \n\n       CVC - Borderline  CVC - Normal  Swan Ganz Catheter Present  PatientID  \\\n0                     0             0                           0  ec89415d1   \n1                     0             1                           0  bf4c6da3c   \n2                     1             0                           0  3fc1c97e5   \n3                     0             0                           0  c31019814   \n4                     0             1                           0  207685cd1   \n...                 ...           ...                         ...        ...   \n30078                 1             1                           0  5b5b9ac30   \n30079                 0             1                           0  7192404d8   \n30080                 0             1                           0  d4d1b066d   \n30081                 1             0                           0  01a6602b8   \n30082                 0             1                           0  e692d316c   \n\n       fold  \n0         2  \n1         2  \n2         0  \n3         0  \n4         1  \n...     ...  \n30078     2  \n30079     2  \n30080     3  \n30081     1  \n30082     3  \n\n[30083 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>ETT - Abnormal</th>\n      <th>ETT - Borderline</th>\n      <th>ETT - Normal</th>\n      <th>NGT - Abnormal</th>\n      <th>NGT - Borderline</th>\n      <th>NGT - Incompletely Imaged</th>\n      <th>NGT - Normal</th>\n      <th>CVC - Abnormal</th>\n      <th>CVC - Borderline</th>\n      <th>CVC - Normal</th>\n      <th>Swan Ganz Catheter Present</th>\n      <th>PatientID</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.8.498.26697628953273228189...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ec89415d1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.8.498.46302891597398758759...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>bf4c6da3c</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.8.498.23819260719748494858...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3fc1c97e5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.8.498.68286643202323212801...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>c31019814</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.8.498.10050203009225938259...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>207685cd1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>1.2.826.0.1.3680043.8.498.74257566841157531124...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5b5b9ac30</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>1.2.826.0.1.3680043.8.498.46510939987173529969...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7192404d8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>1.2.826.0.1.3680043.8.498.43173270582850645437...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>d4d1b066d</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>1.2.826.0.1.3680043.8.498.95092491950130838685...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>01a6602b8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>1.2.826.0.1.3680043.8.498.99518162226171269731...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>e692d316c</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>30083 rows × 14 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "train=pd.read_csv(dataset_dir+\"train.csv\")\n",
    "\n",
    "group_kfold=GroupKFold(n_splits=n_folds)\n",
    "folds=train.copy()\n",
    "\n",
    "for n,(train_index,test_index) in enumerate(group_kfold.split(train,groups=train[\"PatientID\"].values)):\n",
    "    folds.loc[test_index,\"fold\"]=n\n",
    "\n",
    "folds[\"fold\"]=folds[\"fold\"].astype(int)\n",
    "display(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.merge(folds,features,on=\"StudyInstanceUID\")\n",
    "\n",
    "fold=0\n",
    "\n",
    "dataset_train=dataset[dataset[\"fold\"]!=fold]\n",
    "dataset_test=dataset[dataset[\"fold\"]==fold]\n",
    "X_train=dataset_train.iloc[:,-num_features:]\n",
    "X_test=dataset_test.iloc[:,-num_features:]"
   ]
  },
  {
   "source": [
    "## パラメータを最適化する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "1965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.82697\n",
      "regularization_factors, val_score: 0.838443:  25%|##5       | 5/20 [00:05<00:16,  1.11s/it]\u001b[32m[I 2021-02-21 21:40:27,315]\u001b[0m Trial 44 finished with value: 0.8299267889363666 and parameters: {'lambda_l1': 0.00029121314800858777, 'lambda_l2': 0.005770751689903003}. Best is trial 42 with value: 0.838443346226144.\u001b[0m\n",
      "regularization_factors, val_score: 0.838443:  25%|##5       | 5/20 [00:05<00:16,  1.11s/it][100]\tvalid_0's auc: 0.829927\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.825974\n",
      "regularization_factors, val_score: 0.838443:  30%|###       | 6/20 [00:06<00:15,  1.12s/it]\u001b[32m[I 2021-02-21 21:40:28,466]\u001b[0m Trial 45 finished with value: 0.8272873746248758 and parameters: {'lambda_l1': 1.4686469013345195e-08, 'lambda_l2': 1.7476043478717658}. Best is trial 42 with value: 0.838443346226144.\u001b[0m\n",
      "regularization_factors, val_score: 0.838443:  30%|###       | 6/20 [00:06<00:15,  1.12s/it][100]\tvalid_0's auc: 0.827287\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.832017\n",
      "regularization_factors, val_score: 0.838443:  35%|###5      | 7/20 [00:07<00:14,  1.13s/it]\u001b[32m[I 2021-02-21 21:40:29,609]\u001b[0m Trial 46 finished with value: 0.8309555834036078 and parameters: {'lambda_l1': 0.27968663562272605, 'lambda_l2': 7.503612427943817}. Best is trial 42 with value: 0.838443346226144.\u001b[0m\n",
      "regularization_factors, val_score: 0.838443:  35%|###5      | 7/20 [00:07<00:14,  1.13s/it][100]\tvalid_0's auc: 0.830956\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.833318\n",
      "regularization_factors, val_score: 0.838444:  40%|####      | 8/20 [00:08<00:13,  1.12s/it]\u001b[32m[I 2021-02-21 21:40:30,702]\u001b[0m Trial 47 finished with value: 0.838443935118226 and parameters: {'lambda_l1': 1.145837373809189e-06, 'lambda_l2': 3.1893967712350658e-06}. Best is trial 47 with value: 0.838443935118226.\u001b[0m\n",
      "regularization_factors, val_score: 0.838444:  40%|####      | 8/20 [00:08<00:13,  1.12s/it][100]\tvalid_0's auc: 0.838444\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.832394\n",
      "regularization_factors, val_score: 0.838444:  45%|####5     | 9/20 [00:10<00:12,  1.12s/it]\u001b[32m[I 2021-02-21 21:40:31,820]\u001b[0m Trial 48 finished with value: 0.8340590446757089 and parameters: {'lambda_l1': 0.0011408850349205251, 'lambda_l2': 5.831629293814252e-06}. Best is trial 47 with value: 0.838443935118226.\u001b[0m\n",
      "regularization_factors, val_score: 0.838444:  45%|####5     | 9/20 [00:10<00:12,  1.12s/it][100]\tvalid_0's auc: 0.834059\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008884 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.827283\n",
      "regularization_factors, val_score: 0.838444:  50%|#####     | 10/20 [00:11<00:11,  1.12s/it]\u001b[32m[I 2021-02-21 21:40:32,940]\u001b[0m Trial 49 finished with value: 0.8283674027032503 and parameters: {'lambda_l1': 0.24791395356702842, 'lambda_l2': 2.0078235804017451e-07}. Best is trial 47 with value: 0.838443935118226.\u001b[0m\n",
      "regularization_factors, val_score: 0.838444:  50%|#####     | 10/20 [00:11<00:11,  1.12s/it][100]\tvalid_0's auc: 0.828367\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008941 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.833322\n",
      "regularization_factors, val_score: 0.838444:  55%|#####5    | 11/20 [00:12<00:10,  1.12s/it]\u001b[32m[I 2021-02-21 21:40:34,057]\u001b[0m Trial 50 finished with value: 0.8380570330203568 and parameters: {'lambda_l1': 2.334400394615693e-06, 'lambda_l2': 0.0002161285659815168}. Best is trial 47 with value: 0.838443935118226.\u001b[0m\n",
      "regularization_factors, val_score: 0.838444:  55%|#####5    | 11/20 [00:12<00:10,  1.12s/it][100]\tvalid_0's auc: 0.838057\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.833318\n",
      "regularization_factors, val_score: 0.838444:  60%|######    | 12/20 [00:13<00:08,  1.12s/it]\u001b[32m[I 2021-02-21 21:40:35,178]\u001b[0m Trial 51 finished with value: 0.838443346226144 and parameters: {'lambda_l1': 2.2380410147766536e-08, 'lambda_l2': 1.5139394481975353e-08}. Best is trial 47 with value: 0.838443935118226.\u001b[0m\n",
      "regularization_factors, val_score: 0.838444:  60%|######    | 12/20 [00:13<00:08,  1.12s/it][100]\tvalid_0's auc: 0.838443\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.833321\n",
      "regularization_factors, val_score: 0.838445:  65%|######5   | 13/20 [00:14<00:07,  1.12s/it]\u001b[32m[I 2021-02-21 21:40:36,304]\u001b[0m Trial 52 finished with value: 0.838444524010308 and parameters: {'lambda_l1': 4.460511944814478e-07, 'lambda_l2': 6.0988014207911946e-05}. Best is trial 52 with value: 0.838444524010308.\u001b[0m\n",
      "regularization_factors, val_score: 0.838445:  65%|######5   | 13/20 [00:14<00:07,  1.12s/it][100]\tvalid_0's auc: 0.838445\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.833321\n",
      "regularization_factors, val_score: 0.838445:  70%|#######   | 14/20 [00:15<00:06,  1.12s/it]\u001b[32m[I 2021-02-21 21:40:37,428]\u001b[0m Trial 53 finished with value: 0.838444524010308 and parameters: {'lambda_l1': 3.854385273549246e-06, 'lambda_l2': 7.139143912259794e-05}. Best is trial 52 with value: 0.838444524010308.\u001b[0m\n",
      "regularization_factors, val_score: 0.838445:  70%|#######   | 14/20 [00:15<00:06,  1.12s/it][100]\tvalid_0's auc: 0.838445\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008015 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.833323\n",
      "regularization_factors, val_score: 0.838445:  75%|#######5  | 15/20 [00:16<00:05,  1.12s/it]\u001b[32m[I 2021-02-21 21:40:38,539]\u001b[0m Trial 54 finished with value: 0.8380570330203568 and parameters: {'lambda_l1': 1.5901091817789512e-05, 'lambda_l2': 0.00023044582054523792}. Best is trial 52 with value: 0.838444524010308.\u001b[0m\n",
      "regularization_factors, val_score: 0.838445:  75%|#######5  | 15/20 [00:16<00:05,  1.12s/it][100]\tvalid_0's auc: 0.838057\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.82364\n",
      "regularization_factors, val_score: 0.838445:  80%|########  | 16/20 [00:17<00:04,  1.12s/it]\u001b[32m[I 2021-02-21 21:40:39,671]\u001b[0m Trial 55 finished with value: 0.8232999863377037 and parameters: {'lambda_l1': 2.0190920638706713e-05, 'lambda_l2': 0.01225544308110017}. Best is trial 52 with value: 0.838444524010308.\u001b[0m\n",
      "regularization_factors, val_score: 0.838445:  80%|########  | 16/20 [00:17<00:04,  1.12s/it][100]\tvalid_0's auc: 0.8233\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.833321\n",
      "regularization_factors, val_score: 0.838445:  85%|########5 | 17/20 [00:19<00:03,  1.15s/it]\u001b[32m[I 2021-02-21 21:40:40,879]\u001b[0m Trial 56 finished with value: 0.83844511290239 and parameters: {'lambda_l1': 1.4866700650505548e-07, 'lambda_l2': 4.260830218763872e-05}. Best is trial 56 with value: 0.83844511290239.\u001b[0m\n",
      "regularization_factors, val_score: 0.838445:  85%|########5 | 17/20 [00:19<00:03,  1.15s/it][100]\tvalid_0's auc: 0.838445\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.823366\n",
      "regularization_factors, val_score: 0.838445:  90%|######### | 18/20 [00:20<00:02,  1.15s/it]\u001b[32m[I 2021-02-21 21:40:42,018]\u001b[0m Trial 57 finished with value: 0.8254111644516472 and parameters: {'lambda_l1': 4.955764883237419e-08, 'lambda_l2': 0.003620271452439022}. Best is trial 56 with value: 0.83844511290239.\u001b[0m\n",
      "regularization_factors, val_score: 0.838445:  90%|######### | 18/20 [00:20<00:02,  1.15s/it][100]\tvalid_0's auc: 0.825411\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008331 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.833319\n",
      "regularization_factors, val_score: 0.838445:  95%|#########5| 19/20 [00:21<00:01,  1.15s/it]\u001b[32m[I 2021-02-21 21:40:43,163]\u001b[0m Trial 58 finished with value: 0.83844511290239 and parameters: {'lambda_l1': 2.871115372362623e-07, 'lambda_l2': 1.9818984644319858e-05}. Best is trial 56 with value: 0.83844511290239.\u001b[0m\n",
      "regularization_factors, val_score: 0.838445:  95%|#########5| 19/20 [00:21<00:01,  1.15s/it][100]\tvalid_0's auc: 0.838445\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.829367\n",
      "regularization_factors, val_score: 0.838445: 100%|##########| 20/20 [00:22<00:00,  1.15s/it]\u001b[32m[I 2021-02-21 21:40:44,317]\u001b[0m Trial 59 finished with value: 0.8296482429815841 and parameters: {'lambda_l1': 1.01599465339792e-08, 'lambda_l2': 0.07424970083323755}. Best is trial 56 with value: 0.83844511290239.\u001b[0m\n",
      "regularization_factors, val_score: 0.838445: 100%|##########| 20/20 [00:22<00:00,  1.13s/it]\n",
      "min_data_in_leaf, val_score: 0.838445:   0%|          | 0/5 [00:00<?, ?it/s][100]\tvalid_0's auc: 0.829648\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.830468\n",
      "min_data_in_leaf, val_score: 0.838445:  20%|##        | 1/5 [00:01<00:04,  1.03s/it]\u001b[32m[I 2021-02-21 21:40:45,366]\u001b[0m Trial 60 finished with value: 0.8369563937191126 and parameters: {'min_child_samples': 100}. Best is trial 60 with value: 0.8369563937191126.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.838445:  20%|##        | 1/5 [00:01<00:04,  1.03s/it][100]\tvalid_0's auc: 0.836956\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.823626\n",
      "min_data_in_leaf, val_score: 0.838445:  40%|####      | 2/5 [00:02<00:03,  1.09s/it]\u001b[32m[I 2021-02-21 21:40:46,504]\u001b[0m Trial 61 finished with value: 0.8267190937657528 and parameters: {'min_child_samples': 5}. Best is trial 60 with value: 0.8369563937191126.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.838445:  40%|####      | 2/5 [00:02<00:03,  1.09s/it][100]\tvalid_0's auc: 0.826719\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.828456\n",
      "min_data_in_leaf, val_score: 0.838445:  60%|######    | 3/5 [00:03<00:02,  1.10s/it]\u001b[32m[I 2021-02-21 21:40:47,609]\u001b[0m Trial 62 finished with value: 0.8309349721807381 and parameters: {'min_child_samples': 25}. Best is trial 60 with value: 0.8369563937191126.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.838445:  60%|######    | 3/5 [00:03<00:02,  1.10s/it][100]\tvalid_0's auc: 0.830935\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.838545\n",
      "min_data_in_leaf, val_score: 0.838445:  80%|########  | 4/5 [00:04<00:01,  1.08s/it]\u001b[32m[I 2021-02-21 21:40:48,666]\u001b[0m Trial 63 finished with value: 0.8371436614011862 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.8371436614011862.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.838445:  80%|########  | 4/5 [00:04<00:01,  1.08s/it][100]\tvalid_0's auc: 0.837144\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.823861\n",
      "min_data_in_leaf, val_score: 0.838445: 100%|##########| 5/5 [00:05<00:00,  1.10s/it]\u001b[32m[I 2021-02-21 21:40:49,793]\u001b[0m Trial 64 finished with value: 0.8298620108073474 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 0.8371436614011862.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.838445: 100%|##########| 5/5 [00:05<00:00,  1.09s/it][100]\tvalid_0's auc: 0.829862\n",
      "Wall time: 19min 49s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from optuna.integration import lightgbm as lgb\n",
    "\n",
    "def optimize_params():\n",
    "    for i in range(1,12):\n",
    "        y_train=dataset_train.iloc[:,i]\n",
    "        y_test=dataset_test.iloc[:,i]\n",
    "\n",
    "        lgb_train=lgb.Dataset(X_train,label=y_train)\n",
    "        lgb_test=lgb.Dataset(X_test,label=y_test,reference=lgb_train)\n",
    "        \n",
    "        params={\n",
    "            \"task\":\"train\",\n",
    "            \"boosting_type\":\"gbdt\",\n",
    "            \"objective\":\"binary\",\n",
    "            \"metric\":\"auc\",\n",
    "            # \"early_stopping_rounds\":200, #early_stopping_roundsを指定しないとbest_iterationは保存されない\n",
    "        }\n",
    "\n",
    "        opt=lgb.train(params,lgb_train,valid_sets=lgb_test,num_boost_round=100,verbose_eval=50)\n",
    "        pickle.dump(opt.params,open(f\"{models_dir}autoencoder/lgb_params_{i}.pickle\",\"wb\"))"
   ]
  },
  {
   "source": [
    "19min 49s"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 得られたパラメータで予測する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "         ETT - Abnormal ETT - Borderline ETT - Normal NGT - Abnormal  \\\nfold - 0       0.824774         0.818516     0.875501       0.736813   \nfold - 1       0.711973         0.827078     0.862351       0.685289   \nfold - 2       0.715566         0.801306     0.868469       0.677398   \nfold - 3       0.719348         0.824058     0.891108       0.705857   \n\n         NGT - Borderline NGT - Incompletely Imaged NGT - Normal  \\\nfold - 0         0.724868                  0.832617     0.862657   \nfold - 1         0.733265                  0.829989     0.863921   \nfold - 2         0.690667                  0.840096     0.843763   \nfold - 3         0.704454                  0.848050     0.864448   \n\n         CVC - Abnormal CVC - Borderline CVC - Normal  \\\nfold - 0       0.613031         0.589855     0.559629   \nfold - 1       0.596421         0.565049     0.557229   \nfold - 2       0.592896         0.571294     0.543987   \nfold - 3       0.590035         0.585934     0.553409   \n\n         Swan Ganz Catheter Present  \nfold - 0                   0.838445  \nfold - 1                   0.847743  \nfold - 2                   0.845872  \nfold - 3                   0.848280  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ETT - Abnormal</th>\n      <th>ETT - Borderline</th>\n      <th>ETT - Normal</th>\n      <th>NGT - Abnormal</th>\n      <th>NGT - Borderline</th>\n      <th>NGT - Incompletely Imaged</th>\n      <th>NGT - Normal</th>\n      <th>CVC - Abnormal</th>\n      <th>CVC - Borderline</th>\n      <th>CVC - Normal</th>\n      <th>Swan Ganz Catheter Present</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>fold - 0</th>\n      <td>0.824774</td>\n      <td>0.818516</td>\n      <td>0.875501</td>\n      <td>0.736813</td>\n      <td>0.724868</td>\n      <td>0.832617</td>\n      <td>0.862657</td>\n      <td>0.613031</td>\n      <td>0.589855</td>\n      <td>0.559629</td>\n      <td>0.838445</td>\n    </tr>\n    <tr>\n      <th>fold - 1</th>\n      <td>0.711973</td>\n      <td>0.827078</td>\n      <td>0.862351</td>\n      <td>0.685289</td>\n      <td>0.733265</td>\n      <td>0.829989</td>\n      <td>0.863921</td>\n      <td>0.596421</td>\n      <td>0.565049</td>\n      <td>0.557229</td>\n      <td>0.847743</td>\n    </tr>\n    <tr>\n      <th>fold - 2</th>\n      <td>0.715566</td>\n      <td>0.801306</td>\n      <td>0.868469</td>\n      <td>0.677398</td>\n      <td>0.690667</td>\n      <td>0.840096</td>\n      <td>0.843763</td>\n      <td>0.592896</td>\n      <td>0.571294</td>\n      <td>0.543987</td>\n      <td>0.845872</td>\n    </tr>\n    <tr>\n      <th>fold - 3</th>\n      <td>0.719348</td>\n      <td>0.824058</td>\n      <td>0.891108</td>\n      <td>0.705857</td>\n      <td>0.704454</td>\n      <td>0.848050</td>\n      <td>0.864448</td>\n      <td>0.590035</td>\n      <td>0.585934</td>\n      <td>0.553409</td>\n      <td>0.848280</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "fold - 0    0.752428\nfold - 1    0.734573\nfold - 2    0.726483\nfold - 3    0.739544\ndtype: float64"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0.738257044203933"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "\n",
    "def get_pred(train,test,col_name:str):\n",
    "    X_train=train.iloc[:,-num_features:]\n",
    "    X_test=test.iloc[:,-num_features:]\n",
    "    y_train=train[col_name]\n",
    "    y_test=test[col_name]\n",
    "\n",
    "    col_index=test.columns.get_loc(col_name)\n",
    "\n",
    "    lgb_train=lightgbm.Dataset(X_train,label=y_train)\n",
    "    lgb_test=lightgbm.Dataset(X_test,label=y_test,reference=lgb_train)\n",
    "\n",
    "    params=pickle.load(open(f\"{models_dir}autoencoder/lgb_params_{col_index}.pickle\",\"rb\"))\n",
    "    params[\"early_stopping_rounds\"]=100\n",
    "    params[\"verbose\"]=-1\n",
    "\n",
    "    model=lightgbm.train(params,lgb_train,valid_sets=lgb_test,verbose_eval=200)\n",
    "    pred=model.predict(X_test)\n",
    "    auc=roc_auc_score(y_test,pred)\n",
    "\n",
    "    return pred,auc\n",
    "\n",
    "\n",
    "results=pd.DataFrame(columns=target_cols)\n",
    "\n",
    "for n in range(n_folds):\n",
    "    train_n=dataset[dataset[\"fold\"]!=n]\n",
    "    test_n=dataset[dataset[\"fold\"]==n]\n",
    "\n",
    "\n",
    "    for col_name in target_cols:\n",
    "        pred,auc=get_pred(train=train_n,test=test_n,col_name=col_name)\n",
    "        \n",
    "        results.loc[f\"fold - {n}\",col_name]=auc\n",
    "\n",
    "\n",
    "display(results)\n",
    "display(results.mean(axis=1),results.mean(axis=1).mean())"
   ]
  },
  {
   "source": [
    "## モデルを保存する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[90]\tvalid_0's auc: 0.827388\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.818516\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's auc: 0.875658\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[51]\tvalid_0's auc: 0.739846\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[93]\tvalid_0's auc: 0.726099\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.832617\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.862657\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[91]\tvalid_0's auc: 0.613819\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[52]\tvalid_0's auc: 0.590471\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's auc: 0.559639\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's auc: 0.839293\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "\n",
    "for col_name in target_cols:\n",
    "    y_train=dataset_train.loc[:,col_name]\n",
    "    y_test=dataset_test.loc[:,col_name]\n",
    "\n",
    "    col_index=dataset_train.columns.get_loc(col_name)\n",
    "\n",
    "    lgb_train=lightgbm.Dataset(X_train,label=y_train)\n",
    "    lgb_test=lightgbm.Dataset(X_test,label=y_test,reference=lgb_train)\n",
    "    \n",
    "    params=pickle.load(open(f\"{models_dir}autoencoder/lgb_params_{col_index}.pickle\",\"rb\"))    \n",
    "    params[\"early_stopping_rounds\"]=100\n",
    "    params[\"verbose\"]=-1\n",
    "\n",
    "    model=lightgbm.train(params,lgb_train,valid_sets=lgb_test,verbose_eval=200)\n",
    "    pickle.dump(model,open(f\"{models_dir}autoencoder/lgb_model_{col_index}.pickle\",\"wb\"))"
   ]
  }
 ]
}