{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3da45907e5cd41bdac7b692b424c19725633d98a853f93b11b400b1d8951ac30"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "source": [
    "## CFG"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    dataset_dir=\"../input/ranzcr-clip-catheter-line-classification/\"\n",
    "    models_dir=\"./models/\"\n",
    "\n",
    "    n_folds=4\n",
    "    target_cols=['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal', 'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged',\n",
    "               'NGT - Normal', 'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal', 'Swan Ganz Catheter Present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 30083/30083 [00:51<00:00, 580.93it/s]\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv(f\"{CFG.dataset_dir}train.csv\")\n",
    "\n",
    "npz=np.load(\"../input/effnet_tuned_output.npz\")\n",
    "features_list=[npz[uid] for uid in tqdm(train[\"StudyInstanceUID\"])]\n",
    "features=np.array(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold(train):\n",
    "    fold=train.copy()\n",
    "    splitter=GroupKFold(n_splits=CFG.n_folds)\n",
    "    for n,(train_idx,val_idx) in enumerate(splitter.split(train,groups=train[\"PatientID\"])):\n",
    "        fold.loc[val_idx,\"folds\"]=n\n",
    "    fold[\"folds\"]=fold[\"folds\"].astype(int)\n",
    "    return fold\n",
    "\n",
    "fold=get_fold(train)"
   ]
  },
  {
   "source": [
    "### AutoEncoderで次元削減する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "def compress_with_autoencoder(features):\n",
    "    scaler=pickle.load(open(\"./models/minmaxscaler_effnet_tuned.pickle\",\"rb\"))\n",
    "    X=scaler.transform(features)\n",
    "\n",
    "    autoencoder_dir=f\"{CFG.models_dir}autoencoder_tuned/\"\n",
    "    with open(f\"{autoencoder_dir}model.json\",\"rt\") as f:\n",
    "        model_json=f.read()\n",
    "    autoencoder=models.model_from_json(model_json)\n",
    "    autoencoder.load_weights(f\"{autoencoder_dir}ckpt\")\n",
    "\n",
    "    layer_name=\"dense_1\"\n",
    "    compressing_model=models.Model(inputs=autoencoder.input,outputs=autoencoder.get_layer(layer_name).output)\n",
    "\n",
    "    ae_pred=compressing_model.predict(X)\n",
    "    ae_pred_df=pd.DataFrame(ae_pred)\n",
    "\n",
    "    return ae_pred_df\n",
    "\n",
    "X=compress_with_autoencoder(features)"
   ]
  },
  {
   "source": [
    "## パラメータを最適化する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fold=0\n",
    "train_idx=(fold[\"folds\"]!=target_fold)\n",
    "val_idx=(fold[\"folds\"]==target_fold)\n",
    "\n",
    "X_train,X_val=X[train_idx],X[val_idx]\n",
    "y_train,y_val=train[train_idx],train[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "es: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[100]\tvalid_0's auc: 0.997153\n",
      "regularization_factors, val_score: 0.997154:  20%|##        | 4/20 [00:01<00:07,  2.19it/s]\u001b[32m[I 2021-03-10 00:08:01,672]\u001b[0m Trial 46 finished with value: 0.9971530012296067 and parameters: {'lambda_l1': 0.023274875560142644, 'lambda_l2': 1.4526482716131758e-06}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  20%|##        | 4/20 [00:01<00:07,  2.19it/s][LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[100]\tvalid_0's auc: 0.99698\n",
      "regularization_factors, val_score: 0.997154:  25%|##5       | 5/20 [00:02<00:06,  2.19it/s]\u001b[32m[I 2021-03-10 00:08:02,126]\u001b[0m Trial 47 finished with value: 0.9969795725114599 and parameters: {'lambda_l1': 2.846359638174487e-08, 'lambda_l2': 0.13228897533679634}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  25%|##5       | 5/20 [00:02<00:06,  2.19it/s][LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009884 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[100]\tvalid_0's auc: 0.997154\n",
      "regularization_factors, val_score: 0.997154:  30%|###       | 6/20 [00:02<00:06,  2.23it/s]\u001b[32m[I 2021-03-10 00:08:02,560]\u001b[0m Trial 48 finished with value: 0.9971541790137707 and parameters: {'lambda_l1': 1.1322760812954467e-07, 'lambda_l2': 1.564189769452106e-08}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  30%|###       | 6/20 [00:02<00:06,  2.23it/s][LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "regularization_factors, val_score: 0.997154:  35%|###5      | 7/20 [00:03<00:05,  2.20it/s]\u001b[32m[I 2021-03-10 00:08:03,027]\u001b[0m Trial 49 finished with value: 0.9971530012296067 and parameters: {'lambda_l1': 0.019934052698717092, 'lambda_l2': 0.001006974686119207}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  35%|###5      | 7/20 [00:03<00:05,  2.20it/s][100]\tvalid_0's auc: 0.997153\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "regularization_factors, val_score: 0.997154:  40%|####      | 8/20 [00:03<00:05,  2.22it/s]\u001b[32m[I 2021-03-10 00:08:03,471]\u001b[0m Trial 50 finished with value: 0.9971541790137707 and parameters: {'lambda_l1': 0.0008409290894075232, 'lambda_l2': 1.781072275474448e-08}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  40%|####      | 8/20 [00:03<00:05,  2.22it/s][100]\tvalid_0's auc: 0.997154\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[100]\tvalid_0's auc: 0.997154\n",
      "regularization_factors, val_score: 0.997154:  45%|####5     | 9/20 [00:04<00:04,  2.22it/s]\u001b[32m[I 2021-03-10 00:08:03,917]\u001b[0m Trial 51 finished with value: 0.9971541790137707 and parameters: {'lambda_l1': 3.118714201258717e-06, 'lambda_l2': 4.619697435225585e-05}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  45%|####5     | 9/20 [00:04<00:04,  2.22it/s][LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[100]\tvalid_0's auc: 0.996518\n",
      "regularization_factors, val_score: 0.997154:  50%|#####     | 10/20 [00:04<00:04,  2.23it/s]\u001b[32m[I 2021-03-10 00:08:04,364]\u001b[0m Trial 52 finished with value: 0.9965181755652186 and parameters: {'lambda_l1': 3.7040951295532525e-05, 'lambda_l2': 4.376500254486298}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  50%|#####     | 10/20 [00:04<00:04,  2.23it/s][LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[100]\tvalid_0's auc: 0.997067\n",
      "regularization_factors, val_score: 0.997154:  55%|#####5    | 11/20 [00:04<00:03,  2.25it/s]\u001b[32m[I 2021-03-10 00:08:04,797]\u001b[0m Trial 53 finished with value: 0.9970673174316768 and parameters: {'lambda_l1': 1.0681450138065312, 'lambda_l2': 1.0136507813479516e-06}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  55%|#####5    | 11/20 [00:04<00:03,  2.25it/s][LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[100]\tvalid_0's auc: 0.997017\n",
      "regularization_factors, val_score: 0.997154:  60%|######    | 12/20 [00:05<00:03,  2.23it/s]\u001b[32m[I 2021-03-10 00:08:05,252]\u001b[0m Trial 54 finished with value: 0.9970172616047074 and parameters: {'lambda_l1': 0.0661076185796076, 'lambda_l2': 0.0012729969311792582}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  60%|######    | 12/20 [00:05<00:03,  2.23it/s][LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "regularization_factors, val_score: 0.997154:  65%|######5   | 13/20 [00:05<00:03,  2.21it/s]\u001b[32m[I 2021-03-10 00:08:05,721]\u001b[0m Trial 55 finished with value: 0.9971541790137707 and parameters: {'lambda_l1': 4.347827802776819e-05, 'lambda_l2': 2.8069128957761633e-05}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  65%|######5   | 13/20 [00:05<00:03,  2.21it/s]\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[100]\tvalid_0's auc: 0.996236\n",
      "regularization_factors, val_score: 0.997154:  70%|#######   | 14/20 [00:06<00:02,  2.17it/s]\u001b[32m[I 2021-03-10 00:08:06,197]\u001b[0m Trial 56 finished with value: 0.9962363907039852 and parameters: {'lambda_l1': 7.937289495987061, 'lambda_l2': 0.013430787755262496}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  70%|#######   | 14/20 [00:06<00:02,  2.17it/s][LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[100]\tvalid_0's auc: 0.997154\n",
      "regularization_factors, val_score: 0.997154:  75%|#######5  | 15/20 [00:06<00:02,  2.21it/s]\u001b[32m[I 2021-03-10 00:08:06,635]\u001b[0m Trial 57 finished with value: 0.9971541790137707 and parameters: {'lambda_l1': 0.002514241249020244, 'lambda_l2': 8.648952265916955e-07}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  75%|#######5  | 15/20 [00:06<00:02,  2.21it/s][LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010834 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "regularization_factors, val_score: 0.997154:  80%|########  | 16/20 [00:07<00:01,  2.20it/s]\u001b[32m[I 2021-03-10 00:08:07,093]\u001b[0m Trial 58 finished with value: 0.9971541790137707 and parameters: {'lambda_l1': 2.1292304498631898e-05, 'lambda_l2': 7.413478470522268e-05}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  80%|########  | 16/20 [00:07<00:01,  2.20it/s][100]\tvalid_0's auc: 0.997154\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009364 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[100]\tvalid_0's auc: 0.997035\n",
      "regularization_factors, val_score: 0.997154:  85%|########5 | 17/20 [00:07<00:01,  2.20it/s]\u001b[32m[I 2021-03-10 00:08:07,547]\u001b[0m Trial 59 finished with value: 0.9970349283671671 and parameters: {'lambda_l1': 0.29292577873145803, 'lambda_l2': 6.352139266342713e-06}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  85%|########5 | 17/20 [00:07<00:01,  2.20it/s][LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[100]\tvalid_0's auc: 0.997154\n",
      "regularization_factors, val_score: 0.997154:  90%|######### | 18/20 [00:08<00:00,  2.21it/s]\u001b[32m[I 2021-03-10 00:08:07,993]\u001b[0m Trial 60 finished with value: 0.9971541790137707 and parameters: {'lambda_l1': 1.577417407351772e-06, 'lambda_l2': 2.490094262075586e-07}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  90%|######### | 18/20 [00:08<00:00,  2.21it/s][LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010614 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[100]\tvalid_0's auc: 0.997154\n",
      "regularization_factors, val_score: 0.997154:  95%|#########5| 19/20 [00:08<00:00,  2.22it/s]\u001b[32m[I 2021-03-10 00:08:08,438]\u001b[0m Trial 61 finished with value: 0.9971541790137707 and parameters: {'lambda_l1': 3.086618288031476e-05, 'lambda_l2': 0.00017254516750374725}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154:  95%|#########5| 19/20 [00:08<00:00,  2.22it/s][LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010850 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[100]\tvalid_0's auc: 0.997154\n",
      "regularization_factors, val_score: 0.997154: 100%|##########| 20/20 [00:09<00:00,  2.24it/s]\u001b[32m[I 2021-03-10 00:08:08,874]\u001b[0m Trial 62 finished with value: 0.9971541790137707 and parameters: {'lambda_l1': 1.5364397368131374e-06, 'lambda_l2': 7.565085895316947e-08}. Best is trial 43 with value: 0.9971541790137707.\u001b[0m\n",
      "regularization_factors, val_score: 0.997154: 100%|##########| 20/20 [00:09<00:00,  2.21it/s]\n",
      "min_data_in_leaf, val_score: 0.997154:   0%|          | 0/5 [00:00<?, ?it/s][LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "min_data_in_leaf, val_score: 0.997171:  20%|##        | 1/5 [00:00<00:01,  2.20it/s]\u001b[32m[I 2021-03-10 00:08:09,344]\u001b[0m Trial 63 finished with value: 0.9971712568841484 and parameters: {'min_child_samples': 100}. Best is trial 63 with value: 0.9971712568841484.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.997171:  20%|##        | 1/5 [00:00<00:01,  2.20it/s][100]\tvalid_0's auc: 0.997171\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "min_data_in_leaf, val_score: 0.997171:  40%|####      | 2/5 [00:00<00:01,  2.20it/s]\u001b[32m[I 2021-03-10 00:08:09,798]\u001b[0m Trial 64 finished with value: 0.997121789949261 and parameters: {'min_child_samples': 25}. Best is trial 63 with value: 0.9971712568841484.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.997171:  40%|####      | 2/5 [00:00<00:01,  2.20it/s][100]\tvalid_0's auc: 0.997122\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "min_data_in_leaf, val_score: 0.997171:  60%|######    | 3/5 [00:01<00:00,  2.17it/s]\u001b[32m[I 2021-03-10 00:08:10,268]\u001b[0m Trial 65 finished with value: 0.9971712568841484 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.9971712568841484.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.997171:  60%|######    | 3/5 [00:01<00:00,  2.17it/s]\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[100]\tvalid_0's auc: 0.997154\n",
      "min_data_in_leaf, val_score: 0.997171:  80%|########  | 4/5 [00:01<00:00,  2.16it/s]\u001b[32m[I 2021-03-10 00:08:10,732]\u001b[0m Trial 66 finished with value: 0.9971541790137707 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 0.9971712568841484.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.997171:  80%|########  | 4/5 [00:01<00:00,  2.16it/s][LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009776 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18931\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "min_data_in_leaf, val_score: 0.997171: 100%|##########| 5/5 [00:02<00:00,  2.18it/s]\u001b[32m[I 2021-03-10 00:08:11,187]\u001b[0m Trial 67 finished with value: 0.9971541790137707 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.9971712568841484.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.997171: 100%|##########| 5/5 [00:02<00:00,  2.17it/s][100]\tvalid_0's auc: 0.997154\n",
      "Wall time: 14min 26s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from optuna.integration import lightgbm as lgb\n",
    "\n",
    "def optimize_params():\n",
    "    for n,col_name in enumerate(CFG.target_cols):\n",
    "        y_train_col,y_val_col=y_train[col_name],y_val[col_name]\n",
    "\n",
    "        lgb_train=lgb.Dataset(X_train,label=y_train_col)\n",
    "        lgb_val=lgb.Dataset(X_val,label=y_val_col,reference=lgb_train)\n",
    "        \n",
    "        params={\n",
    "            \"task\":\"train\",\n",
    "            \"boosting_type\":\"gbdt\",\n",
    "            \"objective\":\"binary\",\n",
    "            \"metric\":\"auc\",\n",
    "            \"learning_rate\":0.1,\n",
    "            \"num_iterations\":100\n",
    "            # \"early_stopping_rounds\":200, #early_stopping_roundsを指定しないとbest_iterationは保存されない\n",
    "        }\n",
    "\n",
    "        opt=lgb.train(params,lgb_train,valid_sets=lgb_val, verbose_eval=100)\n",
    "        pickle.dump(opt.params,open(f\"{CFG.models_dir}lgbm_effnet_tuned/params_{n}.pickle\",\"wb\"))\n",
    "\n",
    "optimize_params()"
   ]
  },
  {
   "source": [
    "### 最適化に要した時間\n",
    "lr num_it, default: 19min 49s  \n",
    "lr \\*0.1, num_it \\*10: 2h 16min 1s  \n",
    "lr num_it,default: 14min 26s"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 得られたパラメータで予測する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 903, number of negative: 21660\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18933\n",
      "[LightGBM] [Info] Number of data points in the train set: 22563, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040021 -> initscore=-3.177500\n",
      "[LightGBM] [Info] Start training from score -3.177500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 5434, number of negative: 17129\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18933\n",
      "[LightGBM] [Info] Number of data points in the train set: 22563, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240837 -> initscore=-1.148097\n",
      "[LightGBM] [Info] Start training from score -1.148097\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 225, number of negative: 22338\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18933\n",
      "[LightGBM] [Info] Number of data points in the train set: 22563, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.009972 -> initscore=-4.597944\n",
      "[LightGBM] [Info] Start training from score -4.597944\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 395, number of negative: 22168\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18933\n",
      "[LightGBM] [Info] Number of data points in the train set: 22563, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017507 -> initscore=-4.027519\n",
      "[LightGBM] [Info] Start training from score -4.027519\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 1954, number of negative: 20609\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008062 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18933\n",
      "[LightGBM] [Info] Number of data points in the train set: 22563, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.086602 -> initscore=-2.355849\n",
      "[LightGBM] [Info] Start training from score -2.355849\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 3681, number of negative: 18882\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18933\n",
      "[LightGBM] [Info] Number of data points in the train set: 22563, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.163143 -> initscore=-1.635025\n",
      "[LightGBM] [Info] Start training from score -1.635025\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 2335, number of negative: 20228\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18933\n",
      "[LightGBM] [Info] Number of data points in the train set: 22563, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.103488 -> initscore=-2.159056\n",
      "[LightGBM] [Info] Start training from score -2.159056\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 6358, number of negative: 16205\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18933\n",
      "[LightGBM] [Info] Number of data points in the train set: 22563, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.281789 -> initscore=-0.935606\n",
      "[LightGBM] [Info] Start training from score -0.935606\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 15882, number of negative: 6681\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18933\n",
      "[LightGBM] [Info] Number of data points in the train set: 22563, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.703896 -> initscore=0.865919\n",
      "[LightGBM] [Info] Start training from score 0.865919\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 540, number of negative: 22023\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18933\n",
      "[LightGBM] [Info] Number of data points in the train set: 22563, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.023933 -> initscore=-3.708274\n",
      "[LightGBM] [Info] Start training from score -3.708274\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "         ETT - Abnormal ETT - Borderline ETT - Normal NGT - Abnormal  \\\nfold - 0       0.954811         0.956827     0.991526        0.91077   \nfold - 1        0.92339         0.955895     0.989013       0.887442   \nfold - 2       0.967235         0.959108     0.991139        0.90401   \nfold - 3       0.953951         0.956494     0.992525       0.872001   \n\n         NGT - Borderline NGT - Incompletely Imaged NGT - Normal  \\\nfold - 0          0.89723                  0.971671     0.966175   \nfold - 1         0.884119                  0.964834      0.96687   \nfold - 2         0.888132                  0.974249     0.967909   \nfold - 3         0.878282                  0.970925     0.964722   \n\n         CVC - Abnormal CVC - Borderline CVC - Normal  \\\nfold - 0       0.808987         0.791711     0.833054   \nfold - 1       0.830941         0.774067     0.839155   \nfold - 2       0.817232         0.792888     0.829088   \nfold - 3       0.811299         0.775994      0.83158   \n\n         Swan Ganz Catheter Present  \nfold - 0                   0.997171  \nfold - 1                   0.998866  \nfold - 2                     0.9993  \nfold - 3                   0.998721  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ETT - Abnormal</th>\n      <th>ETT - Borderline</th>\n      <th>ETT - Normal</th>\n      <th>NGT - Abnormal</th>\n      <th>NGT - Borderline</th>\n      <th>NGT - Incompletely Imaged</th>\n      <th>NGT - Normal</th>\n      <th>CVC - Abnormal</th>\n      <th>CVC - Borderline</th>\n      <th>CVC - Normal</th>\n      <th>Swan Ganz Catheter Present</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>fold - 0</th>\n      <td>0.954811</td>\n      <td>0.956827</td>\n      <td>0.991526</td>\n      <td>0.91077</td>\n      <td>0.89723</td>\n      <td>0.971671</td>\n      <td>0.966175</td>\n      <td>0.808987</td>\n      <td>0.791711</td>\n      <td>0.833054</td>\n      <td>0.997171</td>\n    </tr>\n    <tr>\n      <th>fold - 1</th>\n      <td>0.92339</td>\n      <td>0.955895</td>\n      <td>0.989013</td>\n      <td>0.887442</td>\n      <td>0.884119</td>\n      <td>0.964834</td>\n      <td>0.96687</td>\n      <td>0.830941</td>\n      <td>0.774067</td>\n      <td>0.839155</td>\n      <td>0.998866</td>\n    </tr>\n    <tr>\n      <th>fold - 2</th>\n      <td>0.967235</td>\n      <td>0.959108</td>\n      <td>0.991139</td>\n      <td>0.90401</td>\n      <td>0.888132</td>\n      <td>0.974249</td>\n      <td>0.967909</td>\n      <td>0.817232</td>\n      <td>0.792888</td>\n      <td>0.829088</td>\n      <td>0.9993</td>\n    </tr>\n    <tr>\n      <th>fold - 3</th>\n      <td>0.953951</td>\n      <td>0.956494</td>\n      <td>0.992525</td>\n      <td>0.872001</td>\n      <td>0.878282</td>\n      <td>0.970925</td>\n      <td>0.964722</td>\n      <td>0.811299</td>\n      <td>0.775994</td>\n      <td>0.83158</td>\n      <td>0.998721</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "fold - 0    0.916358\nfold - 1    0.910418\nfold - 2    0.917299\nfold - 3    0.909681\ndtype: float64"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0.913438849473597"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "num_features=100\n",
    "\n",
    "def get_pred(train,val,col_idx:int):\n",
    "    X_train,y_train=train\n",
    "    X_val,y_val=val\n",
    "    col_name=CFG.target_cols[col_idx]\n",
    "    y_train_col,y_val_col=y_train[col_name],y_val[col_name]\n",
    "\n",
    "    lgb_train=lightgbm.Dataset(X_train,label=y_train_col)\n",
    "    lgb_test=lightgbm.Dataset(X_val,label=y_val_col,reference=lgb_train)\n",
    "\n",
    "    params=pickle.load(open(f\"{CFG.models_dir}lgbm_effnet_tuned/params_{col_idx}.pickle\",\"rb\"))\n",
    "    params[\"early_stopping_rounds\"]=1000\n",
    "\n",
    "    model=lightgbm.train(params,lgb_train,valid_sets=lgb_test,verbose_eval=False)\n",
    "    pred=model.predict(X_val)\n",
    "    auc=roc_auc_score(y_val_col,pred)\n",
    "\n",
    "    return pred,auc\n",
    "\n",
    "\n",
    "results=pd.DataFrame(columns=CFG.target_cols)\n",
    "\n",
    "for n in range(CFG.n_folds):\n",
    "    print(f\"\\nfold - {n}\")\n",
    "    train_idx=(fold[\"folds\"]!=n)\n",
    "    val_idx=(fold[\"folds\"]==n)\n",
    "    X_train,X_val=X[train_idx],X[val_idx]\n",
    "    y_train,y_val=train[train_idx],train[val_idx] \n",
    "\n",
    "    for col_idx,col_name in enumerate(CFG.target_cols):\n",
    "        _,auc=get_pred(train=(X_train,y_train),val=(X_val,y_val),col_idx=col_idx)\n",
    "        results.loc[f\"fold - {n}\",col_name]=auc\n",
    "\n",
    "\n",
    "display(results)\n",
    "display(results.mean(axis=1),results.mean(axis=1).mean())"
   ]
  },
  {
   "source": [
    "## モデルを保存する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Info] Number of positive: 61, number of negative: 22501\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002704 -> initscore=-5.910441\n",
      "[LightGBM] [Info] Start training from score -5.910441\n",
      "[LightGBM] [Info] Number of positive: 875, number of negative: 21687\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038782 -> initscore=-3.210244\n",
      "[LightGBM] [Info] Start training from score -3.210244\n",
      "[LightGBM] [Info] Number of positive: 5351, number of negative: 17211\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.237169 -> initscore=-1.168265\n",
      "[LightGBM] [Info] Start training from score -1.168265\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 211, number of negative: 22351\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.009352 -> initscore=-4.662768\n",
      "[LightGBM] [Info] Start training from score -4.662768\n",
      "[LightGBM] [Info] Number of positive: 382, number of negative: 22180\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.016931 -> initscore=-4.061526\n",
      "[LightGBM] [Info] Start training from score -4.061526\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 2040, number of negative: 20522\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.090418 -> initscore=-2.308548\n",
      "[LightGBM] [Info] Start training from score -2.308548\n",
      "[LightGBM] [Info] Number of positive: 3591, number of negative: 18971\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.159161 -> initscore=-1.664481\n",
      "[LightGBM] [Info] Start training from score -1.664481\n",
      "[LightGBM] [Info] Number of positive: 2423, number of negative: 20139\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007913 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.107393 -> initscore=-2.117652\n",
      "[LightGBM] [Info] Start training from score -2.117652\n",
      "[LightGBM] [Info] Number of positive: 6277, number of negative: 16285\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009975 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278211 -> initscore=-0.953352\n",
      "[LightGBM] [Info] Start training from score -0.953352\n",
      "[LightGBM] [Info] Number of positive: 16024, number of negative: 6538\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.710221 -> initscore=0.896456\n",
      "[LightGBM] [Info] Start training from score 0.896456\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "target_fold=0\n",
    "train_idx=(fold[\"folds\"]!=target_fold)\n",
    "val_idx=(fold[\"folds\"]==target_fold)\n",
    "\n",
    "X_train,X_val=X[train_idx],X[val_idx]\n",
    "y_train,y_val=train[train_idx],train[val_idx]\n",
    "\n",
    "for i,col_name in enumerate(CFG.target_cols):\n",
    "    y_train_col=y_train[col_name]\n",
    "    y_val_col=y_val[col_name]\n",
    "\n",
    "    lgb_train=lightgbm.Dataset(X_train,label=y_train_col)\n",
    "    lgb_val=lightgbm.Dataset(X_val,label=y_val_col,reference=lgb_train)\n",
    "    \n",
    "    params=pickle.load(open(f\"{CFG.models_dir}lgbm_effnet_tuned/params_{i}.pickle\",\"rb\"))    \n",
    "    params[\"early_stopping_rounds\"]=500\n",
    "\n",
    "    model=lightgbm.train(params,lgb_train,valid_sets=lgb_val,verbose_eval=False)\n",
    "    pickle.dump(model,open(f\"{CFG.models_dir}lgbm_effnet_tuned/model_{i}.pickle\",\"wb\"))"
   ]
  }
 ]
}