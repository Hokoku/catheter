{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3da45907e5cd41bdac7b692b424c19725633d98a853f93b11b400b1d8951ac30"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import optuna\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "source": [
    "## CFG"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug=True\n",
    "\n",
    "    input_dir=\"../input/efficientnet_output_straight/\"\n",
    "    dataset_dir=\"../input/ranzcr-clip-catheter-line-classification/\"\n",
    "    models_dir=\"./models/\" if debug else \"../input/efficientnet-lightgbm-models/\"\n",
    "\n",
    "    n_folds=4\n",
    "    num_features=100\n",
    "    epochs=20 if debug else 60\n",
    "    target_cols=['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal', 'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged',           'NGT - Normal', 'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal', 'Swan Ganz Catheter Present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                        StudyInstanceUID  ETT - Abnormal  \\\n0      1.2.826.0.1.3680043.8.498.26697628953273228189...               0   \n1      1.2.826.0.1.3680043.8.498.46302891597398758759...               0   \n2      1.2.826.0.1.3680043.8.498.23819260719748494858...               0   \n3      1.2.826.0.1.3680043.8.498.68286643202323212801...               0   \n4      1.2.826.0.1.3680043.8.498.10050203009225938259...               0   \n...                                                  ...             ...   \n30078  1.2.826.0.1.3680043.8.498.74257566841157531124...               0   \n30079  1.2.826.0.1.3680043.8.498.46510939987173529969...               0   \n30080  1.2.826.0.1.3680043.8.498.43173270582850645437...               0   \n30081  1.2.826.0.1.3680043.8.498.95092491950130838685...               0   \n30082  1.2.826.0.1.3680043.8.498.99518162226171269731...               0   \n\n       ETT - Borderline  ETT - Normal  NGT - Abnormal  NGT - Borderline  \\\n0                     0             0               0                 0   \n1                     0             1               0                 0   \n2                     0             0               0                 0   \n3                     0             0               0                 0   \n4                     0             0               0                 0   \n...                 ...           ...             ...               ...   \n30078                 0             1               0                 0   \n30079                 0             0               0                 0   \n30080                 0             1               0                 0   \n30081                 0             0               0                 0   \n30082                 0             1               0                 0   \n\n       NGT - Incompletely Imaged  NGT - Normal  CVC - Abnormal  \\\n0                              0             1               0   \n1                              1             0               0   \n2                              0             0               0   \n3                              0             0               1   \n4                              0             0               0   \n...                          ...           ...             ...   \n30078                          0             0               0   \n30079                          0             0               0   \n30080                          1             0               1   \n30081                          0             0               0   \n30082                          0             0               0   \n\n       CVC - Borderline  CVC - Normal  Swan Ganz Catheter Present  PatientID  \\\n0                     0             0                           0  ec89415d1   \n1                     0             1                           0  bf4c6da3c   \n2                     1             0                           0  3fc1c97e5   \n3                     0             0                           0  c31019814   \n4                     0             1                           0  207685cd1   \n...                 ...           ...                         ...        ...   \n30078                 1             1                           0  5b5b9ac30   \n30079                 0             1                           0  7192404d8   \n30080                 0             1                           0  d4d1b066d   \n30081                 1             0                           0  01a6602b8   \n30082                 0             1                           0  e692d316c   \n\n       fold  \n0         2  \n1         2  \n2         0  \n3         0  \n4         1  \n...     ...  \n30078     2  \n30079     2  \n30080     3  \n30081     1  \n30082     3  \n\n[30083 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>ETT - Abnormal</th>\n      <th>ETT - Borderline</th>\n      <th>ETT - Normal</th>\n      <th>NGT - Abnormal</th>\n      <th>NGT - Borderline</th>\n      <th>NGT - Incompletely Imaged</th>\n      <th>NGT - Normal</th>\n      <th>CVC - Abnormal</th>\n      <th>CVC - Borderline</th>\n      <th>CVC - Normal</th>\n      <th>Swan Ganz Catheter Present</th>\n      <th>PatientID</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.8.498.26697628953273228189...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ec89415d1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.8.498.46302891597398758759...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>bf4c6da3c</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.8.498.23819260719748494858...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3fc1c97e5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.8.498.68286643202323212801...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>c31019814</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.8.498.10050203009225938259...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>207685cd1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>1.2.826.0.1.3680043.8.498.74257566841157531124...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5b5b9ac30</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>1.2.826.0.1.3680043.8.498.46510939987173529969...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7192404d8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>1.2.826.0.1.3680043.8.498.43173270582850645437...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>d4d1b066d</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>1.2.826.0.1.3680043.8.498.95092491950130838685...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>01a6602b8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>1.2.826.0.1.3680043.8.498.99518162226171269731...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>e692d316c</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>30083 rows × 14 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "train=pd.read_csv(f\"{CFG.dataset_dir}train.csv\")\n",
    "\n",
    "group_kfold=GroupKFold(n_splits=CFG.n_folds)\n",
    "folds=train.copy()\n",
    "\n",
    "for n,(train_idx,val_idx) in enumerate(group_kfold.split(folds,groups=folds[\"PatientID\"].values)):\n",
    "    folds.loc[val_idx,\"fold\"]=n\n",
    "\n",
    "folds[\"fold\"]=folds[\"fold\"].astype(int)\n",
    "display(folds)"
   ]
  },
  {
   "source": [
    "## LightGBMによる予測値の準備"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                        StudyInstanceUID  PatientID         0  \\\n0      1.2.826.0.1.3680043.8.498.26697628953273228189...  ec89415d1  0.169359   \n1      1.2.826.0.1.3680043.8.498.46302891597398758759...  bf4c6da3c  0.365546   \n2      1.2.826.0.1.3680043.8.498.23819260719748494858...  3fc1c97e5  0.211337   \n3      1.2.826.0.1.3680043.8.498.68286643202323212801...  c31019814  0.520994   \n4      1.2.826.0.1.3680043.8.498.10050203009225938259...  207685cd1  0.383775   \n...                                                  ...        ...       ...   \n30078  1.2.826.0.1.3680043.8.498.74257566841157531124...  5b5b9ac30  0.152407   \n30079  1.2.826.0.1.3680043.8.498.46510939987173529969...  7192404d8  0.303968   \n30080  1.2.826.0.1.3680043.8.498.43173270582850645437...  d4d1b066d  0.316939   \n30081  1.2.826.0.1.3680043.8.498.95092491950130838685...  01a6602b8  0.239194   \n30082  1.2.826.0.1.3680043.8.498.99518162226171269731...  e692d316c  0.038298   \n\n              1         2         3         4         5         6         7  \\\n0      0.324016  0.173553  0.095382  0.063119  0.165738  0.181906  0.437814   \n1      0.175262  0.286057  0.205560  0.388747  0.452293  0.021495  0.460131   \n2      0.303350  0.184402  0.248883  0.202594  0.301177  0.208046  0.416033   \n3      0.385111  0.367907  0.247489  0.097945  0.448468  0.377076  0.108094   \n4      0.214603  0.263212  0.429333  0.619226  0.365313  0.264323  0.201577   \n...         ...       ...       ...       ...       ...       ...       ...   \n30078  0.273989  0.429204  0.095627  0.143194  0.151159  0.190642  0.151711   \n30079  0.299640  0.232293  0.115276  0.309395  0.338309  0.164800  0.221858   \n30080  0.351076  0.589629  0.291154  0.185748  0.232997  0.113028  0.353113   \n30081  0.393650  0.438603  0.363903  0.144577  0.259132  0.366278  0.193645   \n30082  0.476334  0.110016  0.089027  0.106914  0.169113  0.072551  0.151309   \n\n       ...      2550      2551      2552      2553      2554      2555  \\\n0      ...  0.302277  0.070963  0.354162  0.284205  0.057408  0.101549   \n1      ...  0.232927  0.223938  0.135535  0.042117  0.169312  0.383767   \n2      ...  0.370746  0.315548  0.384991  0.181793  0.134341  0.147335   \n3      ...  0.355882  0.156018  0.316916  0.141020  0.041345  0.297808   \n4      ...  0.302169  0.318546  0.097627  0.337127  0.124034  0.266738   \n...    ...       ...       ...       ...       ...       ...       ...   \n30078  ...  0.417215  0.330231  0.257251  0.193722  0.070763  0.415085   \n30079  ...  0.509008  0.143263  0.214838  0.167794  0.075009  0.163606   \n30080  ...  0.542412  0.236196  0.170918  0.283692  0.270579  0.275420   \n30081  ...  0.387640  0.075916  0.316283  0.119015  0.101108  0.645644   \n30082  ...  0.394486  0.098980  0.253622  0.046012  0.068931  0.088179   \n\n           2556      2557      2558      2559  \n0      0.200983  0.098618  0.276763  0.473490  \n1      0.095033  0.151809  0.048868  0.117311  \n2      0.313144  0.356484  0.112562  0.404958  \n3      0.273713  0.329452  0.203884  0.219169  \n4      0.595066  0.150074  0.454537  0.071115  \n...         ...       ...       ...       ...  \n30078  0.388353  0.156808  0.239679  0.161568  \n30079  0.415589  0.059742  0.148673  0.164074  \n30080  0.285688  0.203561  0.118533  0.222272  \n30081  0.305596  0.319096  0.330187  0.278469  \n30082  0.348486  0.140222  0.289026  0.741221  \n\n[30083 rows x 2562 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>PatientID</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>2550</th>\n      <th>2551</th>\n      <th>2552</th>\n      <th>2553</th>\n      <th>2554</th>\n      <th>2555</th>\n      <th>2556</th>\n      <th>2557</th>\n      <th>2558</th>\n      <th>2559</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.8.498.26697628953273228189...</td>\n      <td>ec89415d1</td>\n      <td>0.169359</td>\n      <td>0.324016</td>\n      <td>0.173553</td>\n      <td>0.095382</td>\n      <td>0.063119</td>\n      <td>0.165738</td>\n      <td>0.181906</td>\n      <td>0.437814</td>\n      <td>...</td>\n      <td>0.302277</td>\n      <td>0.070963</td>\n      <td>0.354162</td>\n      <td>0.284205</td>\n      <td>0.057408</td>\n      <td>0.101549</td>\n      <td>0.200983</td>\n      <td>0.098618</td>\n      <td>0.276763</td>\n      <td>0.473490</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.8.498.46302891597398758759...</td>\n      <td>bf4c6da3c</td>\n      <td>0.365546</td>\n      <td>0.175262</td>\n      <td>0.286057</td>\n      <td>0.205560</td>\n      <td>0.388747</td>\n      <td>0.452293</td>\n      <td>0.021495</td>\n      <td>0.460131</td>\n      <td>...</td>\n      <td>0.232927</td>\n      <td>0.223938</td>\n      <td>0.135535</td>\n      <td>0.042117</td>\n      <td>0.169312</td>\n      <td>0.383767</td>\n      <td>0.095033</td>\n      <td>0.151809</td>\n      <td>0.048868</td>\n      <td>0.117311</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.8.498.23819260719748494858...</td>\n      <td>3fc1c97e5</td>\n      <td>0.211337</td>\n      <td>0.303350</td>\n      <td>0.184402</td>\n      <td>0.248883</td>\n      <td>0.202594</td>\n      <td>0.301177</td>\n      <td>0.208046</td>\n      <td>0.416033</td>\n      <td>...</td>\n      <td>0.370746</td>\n      <td>0.315548</td>\n      <td>0.384991</td>\n      <td>0.181793</td>\n      <td>0.134341</td>\n      <td>0.147335</td>\n      <td>0.313144</td>\n      <td>0.356484</td>\n      <td>0.112562</td>\n      <td>0.404958</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.8.498.68286643202323212801...</td>\n      <td>c31019814</td>\n      <td>0.520994</td>\n      <td>0.385111</td>\n      <td>0.367907</td>\n      <td>0.247489</td>\n      <td>0.097945</td>\n      <td>0.448468</td>\n      <td>0.377076</td>\n      <td>0.108094</td>\n      <td>...</td>\n      <td>0.355882</td>\n      <td>0.156018</td>\n      <td>0.316916</td>\n      <td>0.141020</td>\n      <td>0.041345</td>\n      <td>0.297808</td>\n      <td>0.273713</td>\n      <td>0.329452</td>\n      <td>0.203884</td>\n      <td>0.219169</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.8.498.10050203009225938259...</td>\n      <td>207685cd1</td>\n      <td>0.383775</td>\n      <td>0.214603</td>\n      <td>0.263212</td>\n      <td>0.429333</td>\n      <td>0.619226</td>\n      <td>0.365313</td>\n      <td>0.264323</td>\n      <td>0.201577</td>\n      <td>...</td>\n      <td>0.302169</td>\n      <td>0.318546</td>\n      <td>0.097627</td>\n      <td>0.337127</td>\n      <td>0.124034</td>\n      <td>0.266738</td>\n      <td>0.595066</td>\n      <td>0.150074</td>\n      <td>0.454537</td>\n      <td>0.071115</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>1.2.826.0.1.3680043.8.498.74257566841157531124...</td>\n      <td>5b5b9ac30</td>\n      <td>0.152407</td>\n      <td>0.273989</td>\n      <td>0.429204</td>\n      <td>0.095627</td>\n      <td>0.143194</td>\n      <td>0.151159</td>\n      <td>0.190642</td>\n      <td>0.151711</td>\n      <td>...</td>\n      <td>0.417215</td>\n      <td>0.330231</td>\n      <td>0.257251</td>\n      <td>0.193722</td>\n      <td>0.070763</td>\n      <td>0.415085</td>\n      <td>0.388353</td>\n      <td>0.156808</td>\n      <td>0.239679</td>\n      <td>0.161568</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>1.2.826.0.1.3680043.8.498.46510939987173529969...</td>\n      <td>7192404d8</td>\n      <td>0.303968</td>\n      <td>0.299640</td>\n      <td>0.232293</td>\n      <td>0.115276</td>\n      <td>0.309395</td>\n      <td>0.338309</td>\n      <td>0.164800</td>\n      <td>0.221858</td>\n      <td>...</td>\n      <td>0.509008</td>\n      <td>0.143263</td>\n      <td>0.214838</td>\n      <td>0.167794</td>\n      <td>0.075009</td>\n      <td>0.163606</td>\n      <td>0.415589</td>\n      <td>0.059742</td>\n      <td>0.148673</td>\n      <td>0.164074</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>1.2.826.0.1.3680043.8.498.43173270582850645437...</td>\n      <td>d4d1b066d</td>\n      <td>0.316939</td>\n      <td>0.351076</td>\n      <td>0.589629</td>\n      <td>0.291154</td>\n      <td>0.185748</td>\n      <td>0.232997</td>\n      <td>0.113028</td>\n      <td>0.353113</td>\n      <td>...</td>\n      <td>0.542412</td>\n      <td>0.236196</td>\n      <td>0.170918</td>\n      <td>0.283692</td>\n      <td>0.270579</td>\n      <td>0.275420</td>\n      <td>0.285688</td>\n      <td>0.203561</td>\n      <td>0.118533</td>\n      <td>0.222272</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>1.2.826.0.1.3680043.8.498.95092491950130838685...</td>\n      <td>01a6602b8</td>\n      <td>0.239194</td>\n      <td>0.393650</td>\n      <td>0.438603</td>\n      <td>0.363903</td>\n      <td>0.144577</td>\n      <td>0.259132</td>\n      <td>0.366278</td>\n      <td>0.193645</td>\n      <td>...</td>\n      <td>0.387640</td>\n      <td>0.075916</td>\n      <td>0.316283</td>\n      <td>0.119015</td>\n      <td>0.101108</td>\n      <td>0.645644</td>\n      <td>0.305596</td>\n      <td>0.319096</td>\n      <td>0.330187</td>\n      <td>0.278469</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>1.2.826.0.1.3680043.8.498.99518162226171269731...</td>\n      <td>e692d316c</td>\n      <td>0.038298</td>\n      <td>0.476334</td>\n      <td>0.110016</td>\n      <td>0.089027</td>\n      <td>0.106914</td>\n      <td>0.169113</td>\n      <td>0.072551</td>\n      <td>0.151309</td>\n      <td>...</td>\n      <td>0.394486</td>\n      <td>0.098980</td>\n      <td>0.253622</td>\n      <td>0.046012</td>\n      <td>0.068931</td>\n      <td>0.088179</td>\n      <td>0.348486</td>\n      <td>0.140222</td>\n      <td>0.289026</td>\n      <td>0.741221</td>\n    </tr>\n  </tbody>\n</table>\n<p>30083 rows × 2562 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "input_df=pd.read_csv(f\"{CFG.models_dir}efficientnet_output_normalized.csv\")\n",
    "display(input_df)"
   ]
  },
  {
   "source": [
    "### AutoEncoderによる次元削減 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder=models.load_model(\"./models/autoencoder_splits10/\")\n",
    "\n",
    "layer_name=\"dense_1\"\n",
    "hidden_layer_model=models.Model(inputs=autoencoder.input,outputs=autoencoder.get_layer(layer_name).output)\n",
    "\n",
    "pred=hidden_layer_model.predict(input_df.iloc[:,2:])\n",
    "features=pd.concat([input_df[\"StudyInstanceUID\"],pd.DataFrame(pred)],axis=1)"
   ]
  },
  {
   "source": [
    "### LightGBMによる推論"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "lgb_pred=folds.copy()\n",
    "\n",
    "for i,col_name in enumerate(CFG.target_cols):\n",
    "    model=pickle.load(open(f\"{CFG.models_dir}autoencoder_smallLR/lgb_model_{i+1}.pickle\",\"rb\"))\n",
    "    pred=model.predict(features.iloc[:,1:])\n",
    "    lgb_pred.loc[:,col_name]=pred\n",
    "\n",
    "display(lgb_pred)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                        StudyInstanceUID  ETT - Abnormal  \\\n0      1.2.826.0.1.3680043.8.498.26697628953273228189...    2.851887e-06   \n1      1.2.826.0.1.3680043.8.498.46302891597398758759...    6.990330e-05   \n2      1.2.826.0.1.3680043.8.498.23819260719748494858...    4.841511e-06   \n3      1.2.826.0.1.3680043.8.498.68286643202323212801...    5.313002e-07   \n4      1.2.826.0.1.3680043.8.498.10050203009225938259...    3.812713e-06   \n...                                                  ...             ...   \n30078  1.2.826.0.1.3680043.8.498.74257566841157531124...    6.560168e-06   \n30079  1.2.826.0.1.3680043.8.498.46510939987173529969...    7.192546e-06   \n30080  1.2.826.0.1.3680043.8.498.43173270582850645437...    2.784264e-06   \n30081  1.2.826.0.1.3680043.8.498.95092491950130838685...    5.331973e-07   \n30082  1.2.826.0.1.3680043.8.498.99518162226171269731...    1.910084e-06   \n\n       ETT - Borderline  ETT - Normal  NGT - Abnormal  NGT - Borderline  \\\n0              0.014421      0.257733        0.007643          0.020949   \n1              0.079135      0.734699        0.016351          0.021119   \n2              0.003495      0.051492        0.003134          0.013448   \n3              0.002256      0.003886        0.002061          0.004724   \n4              0.018148      0.134492        0.015732          0.015907   \n...                 ...           ...             ...               ...   \n30078          0.029976      0.496119        0.009887          0.008641   \n30079          0.022047      0.110283        0.004289          0.007334   \n30080          0.031739      0.556671        0.004088          0.032469   \n30081          0.000567      0.004966        0.001784          0.002526   \n30082          0.032756      0.752125        0.004167          0.007459   \n\n       NGT - Incompletely Imaged  NGT - Normal  CVC - Abnormal  \\\n0                       0.003159      0.854033        0.090729   \n1                       0.960592      0.229085        0.063781   \n2                       0.000845      0.026849        0.070780   \n3                       0.000523      0.003150        0.260812   \n4                       0.007980      0.100083        0.059859   \n...                          ...           ...             ...   \n30078                   0.005662      0.062989        0.059829   \n30079                   0.003377      0.046028        0.060357   \n30080                   0.948585      0.088642        0.544563   \n30081                   0.000255      0.001768        0.046502   \n30082                   0.026528      0.066460        0.090844   \n\n       CVC - Borderline  CVC - Normal  Swan Ganz Catheter Present  PatientID  \\\n0              0.284922      0.473831                    0.000033  ec89415d1   \n1              0.358774      0.788177                    0.000065  bf4c6da3c   \n2              0.357099      0.720060                    0.000022  3fc1c97e5   \n3              0.236471      0.612888                    0.000008  c31019814   \n4              0.283469      0.807261                    0.000075  207685cd1   \n...                 ...           ...                         ...        ...   \n30078          0.570539      0.764016                    0.000237  5b5b9ac30   \n30079          0.350258      0.838486                    0.000081  7192404d8   \n30080          0.287543      0.790466                    0.000191  d4d1b066d   \n30081          0.191689      0.586526                    0.000008  01a6602b8   \n30082          0.356567      0.757640                    0.000062  e692d316c   \n\n       fold  \n0         2  \n1         2  \n2         0  \n3         0  \n4         1  \n...     ...  \n30078     2  \n30079     2  \n30080     3  \n30081     1  \n30082     3  \n\n[30083 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>ETT - Abnormal</th>\n      <th>ETT - Borderline</th>\n      <th>ETT - Normal</th>\n      <th>NGT - Abnormal</th>\n      <th>NGT - Borderline</th>\n      <th>NGT - Incompletely Imaged</th>\n      <th>NGT - Normal</th>\n      <th>CVC - Abnormal</th>\n      <th>CVC - Borderline</th>\n      <th>CVC - Normal</th>\n      <th>Swan Ganz Catheter Present</th>\n      <th>PatientID</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.8.498.26697628953273228189...</td>\n      <td>2.851887e-06</td>\n      <td>0.014421</td>\n      <td>0.257733</td>\n      <td>0.007643</td>\n      <td>0.020949</td>\n      <td>0.003159</td>\n      <td>0.854033</td>\n      <td>0.090729</td>\n      <td>0.284922</td>\n      <td>0.473831</td>\n      <td>0.000033</td>\n      <td>ec89415d1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.8.498.46302891597398758759...</td>\n      <td>6.990330e-05</td>\n      <td>0.079135</td>\n      <td>0.734699</td>\n      <td>0.016351</td>\n      <td>0.021119</td>\n      <td>0.960592</td>\n      <td>0.229085</td>\n      <td>0.063781</td>\n      <td>0.358774</td>\n      <td>0.788177</td>\n      <td>0.000065</td>\n      <td>bf4c6da3c</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.8.498.23819260719748494858...</td>\n      <td>4.841511e-06</td>\n      <td>0.003495</td>\n      <td>0.051492</td>\n      <td>0.003134</td>\n      <td>0.013448</td>\n      <td>0.000845</td>\n      <td>0.026849</td>\n      <td>0.070780</td>\n      <td>0.357099</td>\n      <td>0.720060</td>\n      <td>0.000022</td>\n      <td>3fc1c97e5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.8.498.68286643202323212801...</td>\n      <td>5.313002e-07</td>\n      <td>0.002256</td>\n      <td>0.003886</td>\n      <td>0.002061</td>\n      <td>0.004724</td>\n      <td>0.000523</td>\n      <td>0.003150</td>\n      <td>0.260812</td>\n      <td>0.236471</td>\n      <td>0.612888</td>\n      <td>0.000008</td>\n      <td>c31019814</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.8.498.10050203009225938259...</td>\n      <td>3.812713e-06</td>\n      <td>0.018148</td>\n      <td>0.134492</td>\n      <td>0.015732</td>\n      <td>0.015907</td>\n      <td>0.007980</td>\n      <td>0.100083</td>\n      <td>0.059859</td>\n      <td>0.283469</td>\n      <td>0.807261</td>\n      <td>0.000075</td>\n      <td>207685cd1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>1.2.826.0.1.3680043.8.498.74257566841157531124...</td>\n      <td>6.560168e-06</td>\n      <td>0.029976</td>\n      <td>0.496119</td>\n      <td>0.009887</td>\n      <td>0.008641</td>\n      <td>0.005662</td>\n      <td>0.062989</td>\n      <td>0.059829</td>\n      <td>0.570539</td>\n      <td>0.764016</td>\n      <td>0.000237</td>\n      <td>5b5b9ac30</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>1.2.826.0.1.3680043.8.498.46510939987173529969...</td>\n      <td>7.192546e-06</td>\n      <td>0.022047</td>\n      <td>0.110283</td>\n      <td>0.004289</td>\n      <td>0.007334</td>\n      <td>0.003377</td>\n      <td>0.046028</td>\n      <td>0.060357</td>\n      <td>0.350258</td>\n      <td>0.838486</td>\n      <td>0.000081</td>\n      <td>7192404d8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>1.2.826.0.1.3680043.8.498.43173270582850645437...</td>\n      <td>2.784264e-06</td>\n      <td>0.031739</td>\n      <td>0.556671</td>\n      <td>0.004088</td>\n      <td>0.032469</td>\n      <td>0.948585</td>\n      <td>0.088642</td>\n      <td>0.544563</td>\n      <td>0.287543</td>\n      <td>0.790466</td>\n      <td>0.000191</td>\n      <td>d4d1b066d</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>1.2.826.0.1.3680043.8.498.95092491950130838685...</td>\n      <td>5.331973e-07</td>\n      <td>0.000567</td>\n      <td>0.004966</td>\n      <td>0.001784</td>\n      <td>0.002526</td>\n      <td>0.000255</td>\n      <td>0.001768</td>\n      <td>0.046502</td>\n      <td>0.191689</td>\n      <td>0.586526</td>\n      <td>0.000008</td>\n      <td>01a6602b8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>1.2.826.0.1.3680043.8.498.99518162226171269731...</td>\n      <td>1.910084e-06</td>\n      <td>0.032756</td>\n      <td>0.752125</td>\n      <td>0.004167</td>\n      <td>0.007459</td>\n      <td>0.026528</td>\n      <td>0.066460</td>\n      <td>0.090844</td>\n      <td>0.356567</td>\n      <td>0.757640</td>\n      <td>0.000062</td>\n      <td>e692d316c</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>30083 rows × 14 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "source": [
    "## NNによる予測値の準備"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                        StudyInstanceUID         0         1  \\\n0      1.2.826.0.1.3680043.8.498.26697628953273228189... -0.112231  0.245513   \n1      1.2.826.0.1.3680043.8.498.46302891597398758759...  0.067357  0.011641   \n2      1.2.826.0.1.3680043.8.498.23819260719748494858... -0.073804  0.213021   \n3      1.2.826.0.1.3680043.8.498.68286643202323212801...  0.209653  0.341568   \n4      1.2.826.0.1.3680043.8.498.10050203009225938259...  0.084045  0.073493   \n...                                                  ...       ...       ...   \n30078  1.2.826.0.1.3680043.8.498.74257566841157531124... -0.127748  0.166860   \n30079  1.2.826.0.1.3680043.8.498.46510939987173529969...  0.010990  0.207189   \n30080  1.2.826.0.1.3680043.8.498.43173270582850645437...  0.022863  0.288057   \n30081  1.2.826.0.1.3680043.8.498.95092491950130838685... -0.048304  0.354992   \n30082  1.2.826.0.1.3680043.8.498.99518162226171269731... -0.232202  0.484988   \n\n              2         3         4         5         6         7         8  \\\n0     -0.033396 -0.191451 -0.199094 -0.004325 -0.083669  0.191719 -0.123753   \n1      0.100913 -0.112439  0.140193  0.442245 -0.244318  0.214603  0.095342   \n2     -0.020444 -0.081371 -0.053769  0.206744 -0.057491  0.169386  0.163935   \n3      0.198626 -0.082370 -0.162807  0.436283  0.111789 -0.146370  0.390531   \n4      0.073640  0.048036  0.380339  0.306694 -0.001131 -0.050514  0.304453   \n...         ...       ...       ...       ...       ...       ...       ...   \n30078  0.271803 -0.191275 -0.115660 -0.027046 -0.074921 -0.101646  0.231501   \n30079  0.036729 -0.177184  0.057512  0.264610 -0.100801 -0.029718  0.151412   \n30080  0.463321 -0.051057 -0.071321  0.100492 -0.152650  0.104869  0.151835   \n30081  0.283024  0.001114 -0.114220  0.141220  0.100976 -0.058647  0.230701   \n30082 -0.109247 -0.196009 -0.153462  0.000933 -0.193187 -0.102058  0.223912   \n\n       ...      2550      2551      2552      2553      2554      2555  \\\n0      ...  0.190715 -0.178776  0.320377 -0.041465 -0.231193 -0.159202   \n1      ...  0.089758 -0.004185 -0.029220 -0.230071 -0.154650  0.121530   \n2      ...  0.290390  0.100370  0.369674 -0.121252 -0.178570 -0.113657   \n3      ...  0.268752 -0.081703  0.260819 -0.153018 -0.242180  0.036024   \n4      ...  0.190559  0.103791 -0.089837 -0.000235 -0.185620  0.005118   \n...    ...       ...       ...       ...       ...       ...       ...   \n30078  ...  0.358037  0.117128  0.165411 -0.111959 -0.222058  0.152683   \n30079  ...  0.491667 -0.096260  0.097591 -0.132159 -0.219154 -0.097472   \n30080  ...  0.540294  0.009805  0.027360 -0.041865 -0.085383  0.013754   \n30081  ...  0.314984 -0.173124  0.259807 -0.170161 -0.201301  0.382029   \n30082  ...  0.324950 -0.146800  0.159609 -0.227036 -0.223311 -0.172502   \n\n           2556      2557      2558      2559  \n0      0.118353 -0.183021  0.112336  0.497603  \n1     -0.066817 -0.139586 -0.190869 -0.071105  \n2      0.314377  0.027548 -0.106127  0.388178  \n3      0.245464  0.005475  0.015372  0.091531  \n4      0.807094 -0.141003  0.348858 -0.144865  \n...         ...       ...       ...       ...  \n30078  0.445822 -0.135504  0.062997 -0.000440  \n30079  0.493421 -0.214767 -0.058083  0.003562  \n30080  0.266392 -0.097326 -0.098183  0.096486  \n30081  0.301187 -0.002982  0.183414  0.186215  \n30082  0.376145 -0.149048  0.128651  0.925085  \n\n[30083 rows x 2561 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>...</th>\n      <th>2550</th>\n      <th>2551</th>\n      <th>2552</th>\n      <th>2553</th>\n      <th>2554</th>\n      <th>2555</th>\n      <th>2556</th>\n      <th>2557</th>\n      <th>2558</th>\n      <th>2559</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.8.498.26697628953273228189...</td>\n      <td>-0.112231</td>\n      <td>0.245513</td>\n      <td>-0.033396</td>\n      <td>-0.191451</td>\n      <td>-0.199094</td>\n      <td>-0.004325</td>\n      <td>-0.083669</td>\n      <td>0.191719</td>\n      <td>-0.123753</td>\n      <td>...</td>\n      <td>0.190715</td>\n      <td>-0.178776</td>\n      <td>0.320377</td>\n      <td>-0.041465</td>\n      <td>-0.231193</td>\n      <td>-0.159202</td>\n      <td>0.118353</td>\n      <td>-0.183021</td>\n      <td>0.112336</td>\n      <td>0.497603</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.8.498.46302891597398758759...</td>\n      <td>0.067357</td>\n      <td>0.011641</td>\n      <td>0.100913</td>\n      <td>-0.112439</td>\n      <td>0.140193</td>\n      <td>0.442245</td>\n      <td>-0.244318</td>\n      <td>0.214603</td>\n      <td>0.095342</td>\n      <td>...</td>\n      <td>0.089758</td>\n      <td>-0.004185</td>\n      <td>-0.029220</td>\n      <td>-0.230071</td>\n      <td>-0.154650</td>\n      <td>0.121530</td>\n      <td>-0.066817</td>\n      <td>-0.139586</td>\n      <td>-0.190869</td>\n      <td>-0.071105</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.8.498.23819260719748494858...</td>\n      <td>-0.073804</td>\n      <td>0.213021</td>\n      <td>-0.020444</td>\n      <td>-0.081371</td>\n      <td>-0.053769</td>\n      <td>0.206744</td>\n      <td>-0.057491</td>\n      <td>0.169386</td>\n      <td>0.163935</td>\n      <td>...</td>\n      <td>0.290390</td>\n      <td>0.100370</td>\n      <td>0.369674</td>\n      <td>-0.121252</td>\n      <td>-0.178570</td>\n      <td>-0.113657</td>\n      <td>0.314377</td>\n      <td>0.027548</td>\n      <td>-0.106127</td>\n      <td>0.388178</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.8.498.68286643202323212801...</td>\n      <td>0.209653</td>\n      <td>0.341568</td>\n      <td>0.198626</td>\n      <td>-0.082370</td>\n      <td>-0.162807</td>\n      <td>0.436283</td>\n      <td>0.111789</td>\n      <td>-0.146370</td>\n      <td>0.390531</td>\n      <td>...</td>\n      <td>0.268752</td>\n      <td>-0.081703</td>\n      <td>0.260819</td>\n      <td>-0.153018</td>\n      <td>-0.242180</td>\n      <td>0.036024</td>\n      <td>0.245464</td>\n      <td>0.005475</td>\n      <td>0.015372</td>\n      <td>0.091531</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.8.498.10050203009225938259...</td>\n      <td>0.084045</td>\n      <td>0.073493</td>\n      <td>0.073640</td>\n      <td>0.048036</td>\n      <td>0.380339</td>\n      <td>0.306694</td>\n      <td>-0.001131</td>\n      <td>-0.050514</td>\n      <td>0.304453</td>\n      <td>...</td>\n      <td>0.190559</td>\n      <td>0.103791</td>\n      <td>-0.089837</td>\n      <td>-0.000235</td>\n      <td>-0.185620</td>\n      <td>0.005118</td>\n      <td>0.807094</td>\n      <td>-0.141003</td>\n      <td>0.348858</td>\n      <td>-0.144865</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>1.2.826.0.1.3680043.8.498.74257566841157531124...</td>\n      <td>-0.127748</td>\n      <td>0.166860</td>\n      <td>0.271803</td>\n      <td>-0.191275</td>\n      <td>-0.115660</td>\n      <td>-0.027046</td>\n      <td>-0.074921</td>\n      <td>-0.101646</td>\n      <td>0.231501</td>\n      <td>...</td>\n      <td>0.358037</td>\n      <td>0.117128</td>\n      <td>0.165411</td>\n      <td>-0.111959</td>\n      <td>-0.222058</td>\n      <td>0.152683</td>\n      <td>0.445822</td>\n      <td>-0.135504</td>\n      <td>0.062997</td>\n      <td>-0.000440</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>1.2.826.0.1.3680043.8.498.46510939987173529969...</td>\n      <td>0.010990</td>\n      <td>0.207189</td>\n      <td>0.036729</td>\n      <td>-0.177184</td>\n      <td>0.057512</td>\n      <td>0.264610</td>\n      <td>-0.100801</td>\n      <td>-0.029718</td>\n      <td>0.151412</td>\n      <td>...</td>\n      <td>0.491667</td>\n      <td>-0.096260</td>\n      <td>0.097591</td>\n      <td>-0.132159</td>\n      <td>-0.219154</td>\n      <td>-0.097472</td>\n      <td>0.493421</td>\n      <td>-0.214767</td>\n      <td>-0.058083</td>\n      <td>0.003562</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>1.2.826.0.1.3680043.8.498.43173270582850645437...</td>\n      <td>0.022863</td>\n      <td>0.288057</td>\n      <td>0.463321</td>\n      <td>-0.051057</td>\n      <td>-0.071321</td>\n      <td>0.100492</td>\n      <td>-0.152650</td>\n      <td>0.104869</td>\n      <td>0.151835</td>\n      <td>...</td>\n      <td>0.540294</td>\n      <td>0.009805</td>\n      <td>0.027360</td>\n      <td>-0.041865</td>\n      <td>-0.085383</td>\n      <td>0.013754</td>\n      <td>0.266392</td>\n      <td>-0.097326</td>\n      <td>-0.098183</td>\n      <td>0.096486</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>1.2.826.0.1.3680043.8.498.95092491950130838685...</td>\n      <td>-0.048304</td>\n      <td>0.354992</td>\n      <td>0.283024</td>\n      <td>0.001114</td>\n      <td>-0.114220</td>\n      <td>0.141220</td>\n      <td>0.100976</td>\n      <td>-0.058647</td>\n      <td>0.230701</td>\n      <td>...</td>\n      <td>0.314984</td>\n      <td>-0.173124</td>\n      <td>0.259807</td>\n      <td>-0.170161</td>\n      <td>-0.201301</td>\n      <td>0.382029</td>\n      <td>0.301187</td>\n      <td>-0.002982</td>\n      <td>0.183414</td>\n      <td>0.186215</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>1.2.826.0.1.3680043.8.498.99518162226171269731...</td>\n      <td>-0.232202</td>\n      <td>0.484988</td>\n      <td>-0.109247</td>\n      <td>-0.196009</td>\n      <td>-0.153462</td>\n      <td>0.000933</td>\n      <td>-0.193187</td>\n      <td>-0.102058</td>\n      <td>0.223912</td>\n      <td>...</td>\n      <td>0.324950</td>\n      <td>-0.146800</td>\n      <td>0.159609</td>\n      <td>-0.227036</td>\n      <td>-0.223311</td>\n      <td>-0.172502</td>\n      <td>0.376145</td>\n      <td>-0.149048</td>\n      <td>0.128651</td>\n      <td>0.925085</td>\n    </tr>\n  </tbody>\n</table>\n<p>30083 rows × 2561 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "input_df=pd.read_csv(f\"{CFG.models_dir}efficientnet_output.csv\")\n",
    "display(input_df)"
   ]
  },
  {
   "source": [
    "### Dense層による推論"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndropout (Dropout)            (None, 2560)              0         \n_________________________________________________________________\ndense (Dense)                (None, 11)                28171     \n=================================================================\nTotal params: 28,171\nTrainable params: 28,171\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{CFG.models_dir}eff_dense/model_structure\",\"rt\") as f:\n",
    "    model_json_str=f.read()\n",
    "\n",
    "dense_model=models.model_from_json(model_json_str)\n",
    "dense_model.load_weights(f\"{CFG.models_dir}eff_dense/checkpoint\")\n",
    "dense_model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[keras.metrics.AUC(multi_label=True)])\n",
    "dense_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "       ETT - Abnormal  ETT - Borderline  ETT - Normal  NGT - Abnormal  \\\n0            0.004059          0.021637      0.385945        0.003532   \n1            0.089757          0.301640      0.845686        0.053442   \n2            0.000624          0.006306      0.023352        0.003461   \n3            0.000147          0.000722      0.000731        0.000574   \n4            0.004794          0.050021      0.352987        0.024274   \n...               ...               ...           ...             ...   \n30078        0.015975          0.062557      0.365578        0.002093   \n30079        0.002924          0.032515      0.264873        0.002657   \n30080        0.019645          0.063237      0.545217        0.010459   \n30081        0.000312          0.000754      0.007716        0.000451   \n30082        0.000858          0.028388      0.653046        0.003612   \n\n       NGT - Borderline  NGT - Incompletely Imaged  NGT - Normal  \\\n0              0.025626                   0.043735      0.319992   \n1              0.037303                   0.485664      0.692929   \n2              0.002433                   0.014470      0.012772   \n3              0.000388                   0.003289      0.000765   \n4              0.021685                   0.176899      0.425405   \n...                 ...                        ...           ...   \n30078          0.010424                   0.172909      0.103522   \n30079          0.008054                   0.190094      0.063001   \n30080          0.043144                   0.109558      0.402396   \n30081          0.001699                   0.006645      0.002124   \n30082          0.011948                   0.391288      0.170210   \n\n       CVC - Abnormal  CVC - Borderline  CVC - Normal  \\\n0            0.091947          0.262037      0.622860   \n1            0.144457          0.406314      0.576784   \n2            0.090007          0.371517      0.619742   \n3            0.165605          0.168221      0.626359   \n4            0.052734          0.280874      0.780879   \n...               ...               ...           ...   \n30078        0.082312          0.415560      0.655318   \n30079        0.061255          0.295826      0.710416   \n30080        0.258593          0.379614      0.575094   \n30081        0.063714          0.107356      0.720578   \n30082        0.099786          0.292259      0.666515   \n\n       Swan Ganz Catheter Present  \n0                        0.017982  \n1                        0.176354  \n2                        0.002166  \n3                        0.000445  \n4                        0.005260  \n...                           ...  \n30078                    0.051722  \n30079                    0.060661  \n30080                    0.019723  \n30081                    0.002360  \n30082                    0.005350  \n\n[30083 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ETT - Abnormal</th>\n      <th>ETT - Borderline</th>\n      <th>ETT - Normal</th>\n      <th>NGT - Abnormal</th>\n      <th>NGT - Borderline</th>\n      <th>NGT - Incompletely Imaged</th>\n      <th>NGT - Normal</th>\n      <th>CVC - Abnormal</th>\n      <th>CVC - Borderline</th>\n      <th>CVC - Normal</th>\n      <th>Swan Ganz Catheter Present</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.004059</td>\n      <td>0.021637</td>\n      <td>0.385945</td>\n      <td>0.003532</td>\n      <td>0.025626</td>\n      <td>0.043735</td>\n      <td>0.319992</td>\n      <td>0.091947</td>\n      <td>0.262037</td>\n      <td>0.622860</td>\n      <td>0.017982</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.089757</td>\n      <td>0.301640</td>\n      <td>0.845686</td>\n      <td>0.053442</td>\n      <td>0.037303</td>\n      <td>0.485664</td>\n      <td>0.692929</td>\n      <td>0.144457</td>\n      <td>0.406314</td>\n      <td>0.576784</td>\n      <td>0.176354</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000624</td>\n      <td>0.006306</td>\n      <td>0.023352</td>\n      <td>0.003461</td>\n      <td>0.002433</td>\n      <td>0.014470</td>\n      <td>0.012772</td>\n      <td>0.090007</td>\n      <td>0.371517</td>\n      <td>0.619742</td>\n      <td>0.002166</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000147</td>\n      <td>0.000722</td>\n      <td>0.000731</td>\n      <td>0.000574</td>\n      <td>0.000388</td>\n      <td>0.003289</td>\n      <td>0.000765</td>\n      <td>0.165605</td>\n      <td>0.168221</td>\n      <td>0.626359</td>\n      <td>0.000445</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.004794</td>\n      <td>0.050021</td>\n      <td>0.352987</td>\n      <td>0.024274</td>\n      <td>0.021685</td>\n      <td>0.176899</td>\n      <td>0.425405</td>\n      <td>0.052734</td>\n      <td>0.280874</td>\n      <td>0.780879</td>\n      <td>0.005260</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>0.015975</td>\n      <td>0.062557</td>\n      <td>0.365578</td>\n      <td>0.002093</td>\n      <td>0.010424</td>\n      <td>0.172909</td>\n      <td>0.103522</td>\n      <td>0.082312</td>\n      <td>0.415560</td>\n      <td>0.655318</td>\n      <td>0.051722</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>0.002924</td>\n      <td>0.032515</td>\n      <td>0.264873</td>\n      <td>0.002657</td>\n      <td>0.008054</td>\n      <td>0.190094</td>\n      <td>0.063001</td>\n      <td>0.061255</td>\n      <td>0.295826</td>\n      <td>0.710416</td>\n      <td>0.060661</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>0.019645</td>\n      <td>0.063237</td>\n      <td>0.545217</td>\n      <td>0.010459</td>\n      <td>0.043144</td>\n      <td>0.109558</td>\n      <td>0.402396</td>\n      <td>0.258593</td>\n      <td>0.379614</td>\n      <td>0.575094</td>\n      <td>0.019723</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>0.000312</td>\n      <td>0.000754</td>\n      <td>0.007716</td>\n      <td>0.000451</td>\n      <td>0.001699</td>\n      <td>0.006645</td>\n      <td>0.002124</td>\n      <td>0.063714</td>\n      <td>0.107356</td>\n      <td>0.720578</td>\n      <td>0.002360</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>0.000858</td>\n      <td>0.028388</td>\n      <td>0.653046</td>\n      <td>0.003612</td>\n      <td>0.011948</td>\n      <td>0.391288</td>\n      <td>0.170210</td>\n      <td>0.099786</td>\n      <td>0.292259</td>\n      <td>0.666515</td>\n      <td>0.005350</td>\n    </tr>\n  </tbody>\n</table>\n<p>30083 rows × 11 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "dense_pred=pd.DataFrame(dense_model.predict(input_df.iloc[:,1:]),columns=CFG.target_cols)\n",
    "display(dense_pred)"
   ]
  },
  {
   "source": [
    "## アンサンブルさせる"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "NNではすぐに過学習してauc=1.00となり学習が停滞する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "\n",
    "# pickle.dump(scaler.fit(lgb_pred[CFG.target_cols]),open(f\"{CFG.models_dir}standardscaler_lgb.pickle\",\"wb\"))\n",
    "# pickle.dump(scaler.fit(dense_pred[CFG.target_cols]),open(f\"{CFG.models_dir}standardscaler_dense.pickle\",\"wb\"))\n",
    "\n",
    "\n",
    "lgb_pred_norm=pd.DataFrame(scaler.fit_transform(lgb_pred[CFG.target_cols]),columns=CFG.target_cols)\n",
    "dense_pred_norm=pd.DataFrame(scaler.fit_transform(dense_pred[CFG.target_cols]),columns=CFG.target_cols)\n",
    "\n",
    "lgb_pred_norm=pd.concat([lgb_pred_norm,folds[\"fold\"]],axis=1)\n",
    "dense_pred_norm=pd.concat([dense_pred_norm,folds[\"fold\"]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=0\n",
    "\n",
    "train_lgb=lgb_pred_norm[folds[\"fold\"]!=fold]\n",
    "val_lgb=lgb_pred_norm[folds[\"fold\"]==fold]\n",
    "\n",
    "train_dense=dense_pred_norm[folds[\"fold\"]!=fold]\n",
    "val_dense=dense_pred_norm[folds[\"fold\"]==fold]\n",
    "\n",
    "train_target=train[folds[\"fold\"]!=fold]\n",
    "val_target=train[folds[\"fold\"]==fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "706/706 [==============================] - 1s 2ms/step - loss: 0.6246 - auc: 0.0814 - val_loss: 0.4949 - val_auc: 0.8340\n",
      "Epoch 2/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.3786 - auc: 0.1619 - val_loss: 0.2994 - val_auc: 0.8325\n",
      "Epoch 3/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.2302 - auc: 0.4529 - val_loss: 0.1867 - val_auc: 0.8354\n",
      "Epoch 4/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.1456 - auc: 0.9983 - val_loss: 0.1237 - val_auc: 0.8243\n",
      "Epoch 5/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0978 - auc: 0.9999 - val_loss: 0.0871 - val_auc: 0.8102\n",
      "Epoch 6/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0684 - auc: 1.0000 - val_loss: 0.0641 - val_auc: 0.7801\n",
      "Epoch 7/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0488 - auc: 1.0000 - val_loss: 0.0488 - val_auc: 0.7876\n",
      "Epoch 8/20\n",
      "670/706 [===========================>..] - ETA: 0s - loss: 0.0355 - auc: 1.0000\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0353 - auc: 1.0000 - val_loss: 0.0383 - val_auc: 0.7025\n",
      "Epoch 9/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0276 - auc: 1.0000 - val_loss: 0.0341 - val_auc: 0.8231\n",
      "Epoch 10/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0232 - auc: 1.0000 - val_loss: 0.0302 - val_auc: 0.5810\n",
      "Epoch 11/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0192 - auc: 1.0000 - val_loss: 0.0270 - val_auc: 0.5804\n",
      "Epoch 12/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0157 - auc: 1.0000 - val_loss: 0.0242 - val_auc: 0.5986\n",
      "Epoch 13/20\n",
      "665/706 [===========================>..] - ETA: 0s - loss: 0.0128 - auc: 1.0000\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0128 - auc: 1.0000 - val_loss: 0.0221 - val_auc: 0.4993\n",
      "Epoch 14/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0109 - auc: 1.0000 - val_loss: 0.0211 - val_auc: 0.4995\n",
      "Epoch 15/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0097 - auc: 1.0000 - val_loss: 0.0202 - val_auc: 0.5786\n",
      "Epoch 16/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0085 - auc: 1.0000 - val_loss: 0.0194 - val_auc: 0.5267\n",
      "Epoch 17/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0075 - auc: 1.0000 - val_loss: 0.0188 - val_auc: 0.4995\n",
      "Epoch 18/20\n",
      "684/706 [============================>.] - ETA: 0s - loss: 0.0065 - auc: 1.0000\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0065 - auc: 1.0000 - val_loss: 0.0182 - val_auc: 0.4998\n",
      "Epoch 19/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0059 - auc: 1.0000 - val_loss: 0.0180 - val_auc: 0.4998\n",
      "Epoch 20/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0055 - auc: 1.0000 - val_loss: 0.0177 - val_auc: 0.4999\n",
      "ETT - Abnormal 0.4998667240142822\n",
      "Epoch 1/20\n",
      "706/706 [==============================] - 1s 2ms/step - loss: 0.8286 - auc: 1.4492e-06 - val_loss: 0.4815 - val_auc: 0.6500\n",
      "Epoch 2/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.4844 - auc: 0.0350 - val_loss: 0.3328 - val_auc: 0.7957\n",
      "Epoch 3/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.2882 - auc: 0.6047 - val_loss: 0.2466 - val_auc: 0.8152\n",
      "Epoch 4/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.1758 - auc: 0.9897 - val_loss: 0.1950 - val_auc: 0.8195\n",
      "Epoch 5/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.1129 - auc: 0.9996 - val_loss: 0.1674 - val_auc: 0.8213\n",
      "Epoch 6/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0761 - auc: 1.0000 - val_loss: 0.1532 - val_auc: 0.8175\n",
      "Epoch 7/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0531 - auc: 1.0000 - val_loss: 0.1471 - val_auc: 0.8069\n",
      "Epoch 8/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0379 - auc: 1.0000 - val_loss: 0.1458 - val_auc: 0.7719\n",
      "Epoch 9/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0274 - auc: 1.0000 - val_loss: 0.1477 - val_auc: 0.7807\n",
      "Epoch 10/20\n",
      "681/706 [===========================>..] - ETA: 0s - loss: 0.0201 - auc: 1.0000\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0200 - auc: 1.0000 - val_loss: 0.1518 - val_auc: 0.7611\n",
      "Epoch 11/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0158 - auc: 1.0000 - val_loss: 0.1547 - val_auc: 0.7118\n",
      "Epoch 12/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0133 - auc: 1.0000 - val_loss: 0.1583 - val_auc: 0.7561\n",
      "Epoch 13/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0111 - auc: 1.0000 - val_loss: 0.1626 - val_auc: 0.7131\n",
      "Epoch 14/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0091 - auc: 1.0000 - val_loss: 0.1675 - val_auc: 0.6572\n",
      "Epoch 15/20\n",
      "705/706 [============================>.] - ETA: 0s - loss: 0.0074 - auc: 1.0000\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0074 - auc: 1.0000 - val_loss: 0.1730 - val_auc: 0.6111\n",
      "Epoch 16/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0064 - auc: 1.0000 - val_loss: 0.1759 - val_auc: 0.7406\n",
      "Epoch 17/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0057 - auc: 1.0000 - val_loss: 0.1792 - val_auc: 0.7112\n",
      "Epoch 18/20\n",
      "706/706 [==============================] - 1s 1ms/step - loss: 0.0051 - auc: 1.0000 - val_loss: 0.1827 - val_auc: 0.6969\n",
      "Epoch 19/20\n",
      "706/706 [==============================] - 1s 2ms/step - loss: 0.0045 - auc: 1.0000 - val_loss: 0.1864 - val_auc: 0.6673\n",
      "Epoch 20/20\n",
      "685/706 [============================>.] - ETA: 0s - loss: 0.0040 - auc: 1.0000\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "706/706 [==============================] - 1s 2ms/step - loss: 0.0040 - auc: 1.0000 - val_loss: 0.1901 - val_auc: 0.6426\n",
      "ETT - Borderline 0.6426494717597961\n",
      "Epoch 1/20\n",
      "706/706 [==============================] - 1s 2ms/step - loss: 0.4759 - auc: 0.8890 - val_loss: 0.4201 - val_auc: 0.8932\n",
      "Epoch 2/20\n",
      "123/706 [====>.........................] - ETA: 0s - loss: 0.3659 - auc: 0.9322"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-479feb6de33d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mval_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_lgb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_dense\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     model.fit(x=train_input,y=train_target[col_name],epochs=CFG.epochs,\n\u001b[0m\u001b[0;32m     15\u001b[0m         validation_data=(val_input,val_target[col_name]),verbose=1,callbacks=[lr_reducer])\n\u001b[0;32m     16\u001b[0m     \u001b[0mscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    ensemble_nn=keras.Sequential([keras.layers.Dense(1,input_shape=(2,),activation=\"sigmoid\")])\n",
    "    adam=keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    ensemble_nn.compile(optimizer=adam,loss=\"binary_crossentropy\",metrics=[keras.metrics.AUC(name=\"auc\")])\n",
    "\n",
    "    return ensemble_nn\n",
    "\n",
    "lr_reducer=callbacks.ReduceLROnPlateau(monitor=\"val_auc\",patience=5,verbose=1,mode=\"max\",min_lr=1e-6,factor=0.5)\n",
    "\n",
    "for col_name in CFG.target_cols:\n",
    "    train_input=pd.concat([train_lgb[col_name],train_dense[col_name]],axis=1)\n",
    "    val_input=pd.concat([val_lgb[col_name],val_dense[col_name]],axis=1)\n",
    "    model=create_model()\n",
    "    model.fit(x=train_input,y=train_target[col_name],epochs=CFG.epochs,\n",
    "        validation_data=(val_input,val_target[col_name]),verbose=1,callbacks=[lr_reducer])\n",
    "    score=model.evaluate(x=val_input,y=val_target[col_name],verbose=0)\n",
    "    print(col_name,score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nstudy=optuna.create_study()\\nstudy.optimize(objective,n_trials=200)\\nprint(study.best_params,study.best_value)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    results=pd.DataFrame(index=[\"AUC\"],columns=CFG.target_cols)\n",
    "\n",
    "\n",
    "    logi_c=trial.suggest_int(\"logi_c\",1,1e10,log=True)\n",
    "    clf=LogisticRegression(random_state=0,C=logi_c)\n",
    "\n",
    "\n",
    "    for col_name in CFG.target_cols:\n",
    "\n",
    "        train_input=pd.concat([train_lgb[col_name],train_dense[col_name]],axis=1)\n",
    "        val_input=pd.concat([val_lgb[col_name],val_dense[col_name]],axis=1)\n",
    "\n",
    "        \n",
    "        clf.fit(train_input,train_target[col_name])\n",
    "\n",
    "        ensemble_pred=clf.predict_proba(val_input)[:,1]\n",
    "        score=roc_auc_score(val_target[col_name],ensemble_pred)\n",
    "        \n",
    "        results.loc[\"AUC\",col_name]=score\n",
    "\n",
    "    return results.mean(axis=1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "study=optuna.create_study()\n",
    "study.optimize(objective,n_trials=200)\n",
    "print(study.best_params,study.best_value)\n",
    "\"\"\""
   ]
  },
  {
   "source": [
    "### 重み付け和"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "       ETT - Abnormal ETT - Borderline ETT - Normal NGT - Abnormal  \\\nfold 0       0.832852         0.818918     0.881725       0.760288   \nfold 1            1.0              1.0     0.995519       0.992711   \nfold 2            1.0         0.999985     0.995749       0.999462   \nfold 3            1.0              1.0     0.997561       0.999425   \n\n       NGT - Borderline NGT - Incompletely Imaged NGT - Normal CVC - Abnormal  \\\nfold 0         0.756089                  0.854987     0.870719       0.615146   \nfold 1         0.990708                       1.0     0.999965       0.999809   \nfold 2         0.987794                       1.0     0.999829       0.999773   \nfold 3         0.986358                       1.0     0.999979       0.999493   \n\n       CVC - Borderline CVC - Normal Swan Ganz Catheter Present  \nfold 0         0.594002     0.558873                   0.882527  \nfold 1         0.881025     0.999356                        1.0  \nfold 2         0.888506     0.999477                        1.0  \nfold 3         0.881555     0.999528                        1.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ETT - Abnormal</th>\n      <th>ETT - Borderline</th>\n      <th>ETT - Normal</th>\n      <th>NGT - Abnormal</th>\n      <th>NGT - Borderline</th>\n      <th>NGT - Incompletely Imaged</th>\n      <th>NGT - Normal</th>\n      <th>CVC - Abnormal</th>\n      <th>CVC - Borderline</th>\n      <th>CVC - Normal</th>\n      <th>Swan Ganz Catheter Present</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>fold 0</th>\n      <td>0.832852</td>\n      <td>0.818918</td>\n      <td>0.881725</td>\n      <td>0.760288</td>\n      <td>0.756089</td>\n      <td>0.854987</td>\n      <td>0.870719</td>\n      <td>0.615146</td>\n      <td>0.594002</td>\n      <td>0.558873</td>\n      <td>0.882527</td>\n    </tr>\n    <tr>\n      <th>fold 1</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.995519</td>\n      <td>0.992711</td>\n      <td>0.990708</td>\n      <td>1.0</td>\n      <td>0.999965</td>\n      <td>0.999809</td>\n      <td>0.881025</td>\n      <td>0.999356</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>fold 2</th>\n      <td>1.0</td>\n      <td>0.999985</td>\n      <td>0.995749</td>\n      <td>0.999462</td>\n      <td>0.987794</td>\n      <td>1.0</td>\n      <td>0.999829</td>\n      <td>0.999773</td>\n      <td>0.888506</td>\n      <td>0.999477</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>fold 3</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.997561</td>\n      <td>0.999425</td>\n      <td>0.986358</td>\n      <td>1.0</td>\n      <td>0.999979</td>\n      <td>0.999493</td>\n      <td>0.881555</td>\n      <td>0.999528</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "fold 0    0.766011\nfold 1    0.987190\nfold 2    0.988234\nfold 3    0.987627\ndtype: float64"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "       ETT - Abnormal ETT - Borderline ETT - Normal NGT - Abnormal  \\\nfold 0           0.67              1.0          1.0            1.0   \nfold 1            1.0             0.93          1.0           0.73   \nfold 2            1.0             0.94          1.0           0.73   \nfold 3           0.99             0.95          1.0           0.75   \n\n       NGT - Borderline NGT - Incompletely Imaged NGT - Normal CVC - Abnormal  \\\nfold 0              1.0                      0.65          1.0            1.0   \nfold 1              1.0                       1.0          1.0            1.0   \nfold 2             0.99                       1.0          1.0            1.0   \nfold 3              1.0                       1.0          1.0            1.0   \n\n       CVC - Borderline CVC - Normal Swan Ganz Catheter Present  \nfold 0              1.0          1.0                       0.59  \nfold 1              1.0          1.0                       0.99  \nfold 2              1.0          1.0                       0.99  \nfold 3              1.0          1.0                       0.99  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ETT - Abnormal</th>\n      <th>ETT - Borderline</th>\n      <th>ETT - Normal</th>\n      <th>NGT - Abnormal</th>\n      <th>NGT - Borderline</th>\n      <th>NGT - Incompletely Imaged</th>\n      <th>NGT - Normal</th>\n      <th>CVC - Abnormal</th>\n      <th>CVC - Borderline</th>\n      <th>CVC - Normal</th>\n      <th>Swan Ganz Catheter Present</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>fold 0</th>\n      <td>0.67</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.65</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.59</td>\n    </tr>\n    <tr>\n      <th>fold 1</th>\n      <td>1.0</td>\n      <td>0.93</td>\n      <td>1.0</td>\n      <td>0.73</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.99</td>\n    </tr>\n    <tr>\n      <th>fold 2</th>\n      <td>1.0</td>\n      <td>0.94</td>\n      <td>1.0</td>\n      <td>0.73</td>\n      <td>0.99</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.99</td>\n    </tr>\n    <tr>\n      <th>fold 3</th>\n      <td>0.99</td>\n      <td>0.95</td>\n      <td>1.0</td>\n      <td>0.75</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.99</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "ETT - Abnormal                0.9150\nETT - Borderline              0.9550\nETT - Normal                  1.0000\nNGT - Abnormal                0.8025\nNGT - Borderline              0.9975\nNGT - Incompletely Imaged     0.9125\nNGT - Normal                  1.0000\nCVC - Abnormal                1.0000\nCVC - Borderline              1.0000\nCVC - Normal                  1.0000\nSwan Ganz Catheter Present    0.8900\ndtype: float64"
     },
     "metadata": {}
    }
   ],
   "source": [
    "results=pd.DataFrame(columns=CFG.target_cols)\n",
    "\n",
    "def get_best_params(train_lgb,train_dense,train_target):\n",
    "    best_params={}\n",
    "    for col_name in CFG.target_cols:\n",
    "\n",
    "        best_value=(0,0)\n",
    "\n",
    "        for p in range(101):\n",
    "            p*=0.01\n",
    "\n",
    "            ensemble_pred=train_lgb[col_name]*p+train_dense[col_name]*(1-p)\n",
    "            score=roc_auc_score(train_target[col_name],ensemble_pred)\n",
    "\n",
    "            if score>best_value[1]:\n",
    "                best_value=(p,score)\n",
    "\n",
    "        best_params[col_name]=best_value[0]\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "oof_best_params=pd.DataFrame(columns=CFG.target_cols)\n",
    "\n",
    "for n in range(CFG.n_folds):\n",
    "    train_lgb=lgb_pred_norm[folds[\"fold\"]!=n]\n",
    "    train_dense=dense_pred_norm[folds[\"fold\"]!=n]\n",
    "    train_target=train[folds[\"fold\"]!=n]\n",
    "\n",
    "    best_params=get_best_params(train_lgb,train_dense,train_target)\n",
    "\n",
    "    for col_name,p in best_params.items():\n",
    "        oof_best_params.loc[f\"fold {n}\",col_name]=p\n",
    "\n",
    "    val_lgb=lgb_pred_norm[folds[\"fold\"]==n]\n",
    "    val_dense=dense_pred_norm[folds[\"fold\"]==n]\n",
    "    val_target=train[folds[\"fold\"]==n]\n",
    "\n",
    "    for col_name in CFG.target_cols:\n",
    "        p=best_params[col_name]\n",
    "        ensemble_pred=val_lgb[col_name]*p+val_dense[col_name]*(1-p)\n",
    "\n",
    "        score=roc_auc_score(val_target[col_name],ensemble_pred)\n",
    "        results.loc[f\"fold {n}\",col_name]=score\n",
    "\n",
    "display(results,results.mean(axis=1),oof_best_params,oof_best_params.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_oof_ave={\n",
    "    \"ETT - Abnormal\"                :0.9150,\n",
    "    \"ETT - Borderline\"              :0.9550,\n",
    "    \"ETT - Normal\"                  :1.0000,\n",
    "    \"NGT - Abnormal\"                :0.8025,\n",
    "    \"NGT - Borderline\"              :0.9975,\n",
    "    \"NGT - Incompletely Imaged\"     :0.9125,\n",
    "    \"NGT - Normal\"                  :1.0000,\n",
    "    \"CVC - Abnormal\"                :1.0000,\n",
    "    \"CVC - Borderline\"              :1.0000,\n",
    "    \"CVC - Normal\"                  :1.0000,\n",
    "    \"Swan Ganz Catheter Present\"    :0.8900\n",
    "}\n",
    "\n",
    "pickle.dump(best_params_oof_ave,open(f\"{CFG.models_dir}weightedsum_params_oof_ave.pickle\",\"wb\"))"
   ]
  },
  {
   "source": [
    "### パラメータの最適化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "th positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "min_data_in_leaf, val_score: 0.850004: 100%|##########| 5/5 [00:03<00:00,  1.32it/s]\u001b[32m[I 2021-02-27 19:28:10,471]\u001b[0m Trial 67 finished with value: 0.8499879866015273 and parameters: {'min_child_samples': 25}. Best is trial 64 with value: 0.8500035922417002.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.850004: 100%|##########| 5/5 [00:03<00:00,  1.31it/s]Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Wall time: 10min 54s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from optuna.integration import lightgbm as lgb\n",
    "\n",
    "def optimize_params():\n",
    "\n",
    "    for i,col_name in enumerate(CFG.target_cols):\n",
    "        train_input=pd.concat([train_lgb[col_name],train_dense[col_name]],axis=1)\n",
    "        val_input=pd.concat([val_lgb[col_name],val_dense[col_name]],axis=1)\n",
    "\n",
    "        train_input.columns=range(train_input.shape[1])\n",
    "        val_input.columns=range(val_input.shape[1])\n",
    "\n",
    "        lgb_train=lgb.Dataset(train_input,label=train_target[col_name])\n",
    "        lgb_test=lgb.Dataset(val_input,label=val_target[col_name],reference=lgb_train)\n",
    "        \n",
    "        params={\n",
    "            \"task\":\"train\",\n",
    "            \"boosting_type\":\"gbdt\",\n",
    "            \"objective\":\"binary\",\n",
    "            \"metric\":\"auc\",\n",
    "            \"learning_rate\":1e-1,\n",
    "            \"num_iterations\":500\n",
    "            # \"early_stopping_rounds\":200, #early_stopping_roundsを指定しないとbest_iterationは保存されない\n",
    "        }\n",
    "\n",
    "        opt=lgb.train(params,lgb_train,valid_sets=lgb_test, verbose_eval=False)\n",
    "        pickle.dump(opt.params,open(f\"{CFG.models_dir}ensemble_lgb/lgb_params_{i}.pickle\",\"wb\"))\n",
    "\n",
    "optimize_params()"
   ]
  },
  {
   "source": [
    "### パラメータからモデルの構築"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "in: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Wall time: 8.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import lightgbm\n",
    "\n",
    "\n",
    "for i,col_name in enumerate(CFG.target_cols):\n",
    "    train_input=pd.concat([train_lgb[col_name],train_dense[col_name]],axis=1)\n",
    "    val_input=pd.concat([val_lgb[col_name],val_dense[col_name]],axis=1)\n",
    "\n",
    "    train_input.columns=range(train_input.shape[1])\n",
    "    val_input.columns=range(val_input.shape[1])\n",
    "\n",
    "    lgb_train=lgb.Dataset(train_input,label=train_target[col_name])\n",
    "    lgb_test=lgb.Dataset(val_input,label=val_target[col_name],reference=lgb_train)\n",
    "\n",
    "    target_model_dir=f\"{CFG.models_dir}ensemble_lgb\"\n",
    "    \n",
    "    params=pickle.load(open(f\"{target_model_dir}/lgb_params_{i}.pickle\",\"rb\"))\n",
    "    params[\"early_stopping_rounds\"]=500\n",
    "\n",
    "    model=lightgbm.train(params,lgb_train,valid_sets=lgb_test,verbose_eval=False)\n",
    "    pickle.dump(model,open(f\"{target_model_dir}/lgb_model_{i}.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "         ETT - Abnormal ETT - Borderline ETT - Normal NGT - Abnormal  \\\nfold - 0       0.880355         0.816767     0.878870       0.655587   \nfold - 1       1.000000         1.000000     0.995628       0.997398   \nfold - 2       1.000000         0.999973     0.995906       0.999814   \nfold - 3       1.000000         0.999973     0.997586       0.999788   \n\n         NGT - Borderline NGT - Incompletely Imaged NGT - Normal  \\\nfold - 0         0.743923                  0.834016     0.808072   \nfold - 1         0.992063                  1.000000     0.999962   \nfold - 2         0.989540                  1.000000     0.999843   \nfold - 3         0.990172                  1.000000     0.999975   \n\n         CVC - Abnormal CVC - Borderline CVC - Normal  \\\nfold - 0       0.615765         0.594139     0.553794   \nfold - 1       0.999775         0.882005     0.999421   \nfold - 2       0.999826         0.889521     0.999494   \nfold - 3       0.999769         0.883052     0.999556   \n\n         Swan Ganz Catheter Present  \nfold - 0                   0.854552  \nfold - 1                   1.000000  \nfold - 2                   1.000000  \nfold - 3                   1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ETT - Abnormal</th>\n      <th>ETT - Borderline</th>\n      <th>ETT - Normal</th>\n      <th>NGT - Abnormal</th>\n      <th>NGT - Borderline</th>\n      <th>NGT - Incompletely Imaged</th>\n      <th>NGT - Normal</th>\n      <th>CVC - Abnormal</th>\n      <th>CVC - Borderline</th>\n      <th>CVC - Normal</th>\n      <th>Swan Ganz Catheter Present</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>fold - 0</th>\n      <td>0.880355</td>\n      <td>0.816767</td>\n      <td>0.878870</td>\n      <td>0.655587</td>\n      <td>0.743923</td>\n      <td>0.834016</td>\n      <td>0.808072</td>\n      <td>0.615765</td>\n      <td>0.594139</td>\n      <td>0.553794</td>\n      <td>0.854552</td>\n    </tr>\n    <tr>\n      <th>fold - 1</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.995628</td>\n      <td>0.997398</td>\n      <td>0.992063</td>\n      <td>1.000000</td>\n      <td>0.999962</td>\n      <td>0.999775</td>\n      <td>0.882005</td>\n      <td>0.999421</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>fold - 2</th>\n      <td>1.000000</td>\n      <td>0.999973</td>\n      <td>0.995906</td>\n      <td>0.999814</td>\n      <td>0.989540</td>\n      <td>1.000000</td>\n      <td>0.999843</td>\n      <td>0.999826</td>\n      <td>0.889521</td>\n      <td>0.999494</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>fold - 3</th>\n      <td>1.000000</td>\n      <td>0.999973</td>\n      <td>0.997586</td>\n      <td>0.999788</td>\n      <td>0.990172</td>\n      <td>1.000000</td>\n      <td>0.999975</td>\n      <td>0.999769</td>\n      <td>0.883052</td>\n      <td>0.999556</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "fold - 0    0.748713\nfold - 1    0.987841\nfold - 2    0.988538\nfold - 3    0.988170\ndtype: float64"
     },
     "metadata": {}
    }
   ],
   "source": [
    "results=pd.DataFrame(columns=CFG.target_cols)\n",
    "\n",
    "for n in range(CFG.n_folds):\n",
    "    val_lgb=lgb_pred_norm[folds[\"fold\"]==n]\n",
    "    val_dense=dense_pred_norm[folds[\"fold\"]==n]\n",
    "    val_target=train[folds[\"fold\"]==n]\n",
    "\n",
    "\n",
    "    for i,col_name in enumerate(CFG.target_cols):\n",
    "        val_input=pd.concat([val_lgb[col_name],val_dense[col_name]],axis=1)\n",
    "        \n",
    "        model=pickle.load(open(f\"{CFG.models_dir}ensemble_lgb/lgb_model_{i}.pickle\",\"rb\"))\n",
    "        score=roc_auc_score(val_target[col_name],model.predict(val_input))\n",
    "        results.loc[f\"fold - {n}\",col_name]=score\n",
    "\n",
    "display(results,results.mean(axis=1))"
   ]
  },
  {
   "source": [
    "### RandomForest\n",
    "{'rf_max_depth': 4, 'rf_n_estimators': 300} 0.7601442329875496\n",
    "\n",
    "### LightGBM\n",
    "lr 0.001, itr 1000: 0.752988 (23min 8s)  \n",
    "lr 0.1, itr 200: 0.756288 (6min 58s)  \n",
    "lr 1e-2, itr 1e3: 0.754762 (24min 22s)  \n",
    "lr 1e-1, itr 500: 0.748713 (10min 54s)  \n",
    "\n",
    "### LogisticRegression\n",
    "{'logi_c': 50} 0.6739926270642816\n",
    "\n",
    "### Weighted Sum\n",
    "learning_rate 0.01: 0.766011"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}