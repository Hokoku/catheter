{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3da45907e5cd41bdac7b692b424c19725633d98a853f93b11b400b1d8951ac30"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    dataset_dir=\"../input/ranzcr-clip-catheter-line-classification/\"\n",
    "    n_folds=4\n",
    "    target_cols=['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal', 'NGT - Abnormal', 'NGT - Borderline',\n",
    "       'NGT - Incompletely Imaged', 'NGT - Normal', 'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal', 'Swan Ganz Catheter Present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Index(['StudyInstanceUID', 'ETT - Abnormal', 'ETT - Borderline',\n       'ETT - Normal', 'NGT - Abnormal', 'NGT - Borderline',\n       'NGT - Incompletely Imaged', 'NGT - Normal', 'CVC - Abnormal',\n       'CVC - Borderline', 'CVC - Normal', 'Swan Ganz Catheter Present',\n       'PatientID'],\n      dtype='object')"
     },
     "metadata": {}
    }
   ],
   "source": [
    "train=pd.read_csv(f\"{CFG.dataset_dir}train.csv\")\n",
    "display(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "1    29333\n0      750\nName: CVC Present, dtype: int64"
     },
     "metadata": {}
    }
   ],
   "source": [
    "cvc_cols=[col_name for col_name in CFG.target_cols if (\"CVC\" in col_name)]\n",
    "\n",
    "train[\"CVC Present\"]=train[cvc_cols].sum(axis=1)\n",
    "false_idx=(train[\"CVC Present\"]==0)\n",
    "train[\"CVC Present\"]=1\n",
    "train.loc[false_idx,\"CVC Present\"]=0\n",
    "display(train[\"CVC Present\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                        StudyInstanceUID  ETT - Abnormal  \\\n0      1.2.826.0.1.3680043.8.498.26697628953273228189...               0   \n1      1.2.826.0.1.3680043.8.498.46302891597398758759...               0   \n2      1.2.826.0.1.3680043.8.498.23819260719748494858...               0   \n3      1.2.826.0.1.3680043.8.498.68286643202323212801...               0   \n4      1.2.826.0.1.3680043.8.498.10050203009225938259...               0   \n...                                                  ...             ...   \n30078  1.2.826.0.1.3680043.8.498.74257566841157531124...               0   \n30079  1.2.826.0.1.3680043.8.498.46510939987173529969...               0   \n30080  1.2.826.0.1.3680043.8.498.43173270582850645437...               0   \n30081  1.2.826.0.1.3680043.8.498.95092491950130838685...               0   \n30082  1.2.826.0.1.3680043.8.498.99518162226171269731...               0   \n\n       ETT - Borderline  ETT - Normal  NGT - Abnormal  NGT - Borderline  \\\n0                     0             0               0                 0   \n1                     0             1               0                 0   \n2                     0             0               0                 0   \n3                     0             0               0                 0   \n4                     0             0               0                 0   \n...                 ...           ...             ...               ...   \n30078                 0             1               0                 0   \n30079                 0             0               0                 0   \n30080                 0             1               0                 0   \n30081                 0             0               0                 0   \n30082                 0             1               0                 0   \n\n       NGT - Incompletely Imaged  NGT - Normal  CVC - Abnormal  \\\n0                              0             1               0   \n1                              1             0               0   \n2                              0             0               0   \n3                              0             0               1   \n4                              0             0               0   \n...                          ...           ...             ...   \n30078                          0             0               0   \n30079                          0             0               0   \n30080                          1             0               1   \n30081                          0             0               0   \n30082                          0             0               0   \n\n       CVC - Borderline  CVC - Normal  Swan Ganz Catheter Present  PatientID  \\\n0                     0             0                           0  ec89415d1   \n1                     0             1                           0  bf4c6da3c   \n2                     1             0                           0  3fc1c97e5   \n3                     0             0                           0  c31019814   \n4                     0             1                           0  207685cd1   \n...                 ...           ...                         ...        ...   \n30078                 1             1                           0  5b5b9ac30   \n30079                 0             1                           0  7192404d8   \n30080                 0             1                           0  d4d1b066d   \n30081                 1             0                           0  01a6602b8   \n30082                 0             1                           0  e692d316c   \n\n       CVC Present  folds  \n0                0      0  \n1                1      0  \n2                1      0  \n3                1      0  \n4                1      0  \n...            ...    ...  \n30078            1      3  \n30079            1      3  \n30080            1      3  \n30081            1      3  \n30082            1      3  \n\n[30083 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>ETT - Abnormal</th>\n      <th>ETT - Borderline</th>\n      <th>ETT - Normal</th>\n      <th>NGT - Abnormal</th>\n      <th>NGT - Borderline</th>\n      <th>NGT - Incompletely Imaged</th>\n      <th>NGT - Normal</th>\n      <th>CVC - Abnormal</th>\n      <th>CVC - Borderline</th>\n      <th>CVC - Normal</th>\n      <th>Swan Ganz Catheter Present</th>\n      <th>PatientID</th>\n      <th>CVC Present</th>\n      <th>folds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.8.498.26697628953273228189...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ec89415d1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.8.498.46302891597398758759...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>bf4c6da3c</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.8.498.23819260719748494858...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3fc1c97e5</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.8.498.68286643202323212801...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>c31019814</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.8.498.10050203009225938259...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>207685cd1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>1.2.826.0.1.3680043.8.498.74257566841157531124...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5b5b9ac30</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>1.2.826.0.1.3680043.8.498.46510939987173529969...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7192404d8</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>1.2.826.0.1.3680043.8.498.43173270582850645437...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>d4d1b066d</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>1.2.826.0.1.3680043.8.498.95092491950130838685...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>01a6602b8</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>1.2.826.0.1.3680043.8.498.99518162226171269731...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>e692d316c</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>30083 rows × 15 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "fold=train.copy()\n",
    "\n",
    "skf=StratifiedKFold(n_splits=CFG.n_folds)\n",
    "for n,(train_idx,val_idx) in enumerate(skf.split(train,train[\"CVC Present\"])):\n",
    "    fold.loc[val_idx,\"folds\"]=n\n",
    "fold[\"folds\"]=fold[\"folds\"].astype(int)\n",
    "\n",
    "display(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                        StudyInstanceUID  PatientID         0  \\\n0      1.2.826.0.1.3680043.8.498.26697628953273228189...  ec89415d1  0.169359   \n1      1.2.826.0.1.3680043.8.498.46302891597398758759...  bf4c6da3c  0.365546   \n2      1.2.826.0.1.3680043.8.498.23819260719748494858...  3fc1c97e5  0.211337   \n3      1.2.826.0.1.3680043.8.498.68286643202323212801...  c31019814  0.520994   \n4      1.2.826.0.1.3680043.8.498.10050203009225938259...  207685cd1  0.383775   \n...                                                  ...        ...       ...   \n30078  1.2.826.0.1.3680043.8.498.74257566841157531124...  5b5b9ac30  0.152407   \n30079  1.2.826.0.1.3680043.8.498.46510939987173529969...  7192404d8  0.303968   \n30080  1.2.826.0.1.3680043.8.498.43173270582850645437...  d4d1b066d  0.316939   \n30081  1.2.826.0.1.3680043.8.498.95092491950130838685...  01a6602b8  0.239194   \n30082  1.2.826.0.1.3680043.8.498.99518162226171269731...  e692d316c  0.038298   \n\n              1         2         3         4         5         6         7  \\\n0      0.324016  0.173553  0.095382  0.063119  0.165738  0.181906  0.437814   \n1      0.175262  0.286057  0.205560  0.388747  0.452293  0.021495  0.460131   \n2      0.303350  0.184402  0.248883  0.202594  0.301177  0.208046  0.416033   \n3      0.385111  0.367907  0.247489  0.097945  0.448468  0.377076  0.108094   \n4      0.214603  0.263212  0.429333  0.619226  0.365313  0.264323  0.201577   \n...         ...       ...       ...       ...       ...       ...       ...   \n30078  0.273989  0.429204  0.095627  0.143194  0.151159  0.190642  0.151711   \n30079  0.299640  0.232293  0.115276  0.309395  0.338309  0.164800  0.221858   \n30080  0.351076  0.589629  0.291154  0.185748  0.232997  0.113028  0.353113   \n30081  0.393650  0.438603  0.363903  0.144577  0.259132  0.366278  0.193645   \n30082  0.476334  0.110016  0.089027  0.106914  0.169113  0.072551  0.151309   \n\n       ...      2550      2551      2552      2553      2554      2555  \\\n0      ...  0.302277  0.070963  0.354162  0.284205  0.057408  0.101549   \n1      ...  0.232927  0.223938  0.135535  0.042117  0.169312  0.383767   \n2      ...  0.370746  0.315548  0.384991  0.181793  0.134341  0.147335   \n3      ...  0.355882  0.156018  0.316916  0.141020  0.041345  0.297808   \n4      ...  0.302169  0.318546  0.097627  0.337127  0.124034  0.266738   \n...    ...       ...       ...       ...       ...       ...       ...   \n30078  ...  0.417215  0.330231  0.257251  0.193722  0.070763  0.415085   \n30079  ...  0.509008  0.143263  0.214838  0.167794  0.075009  0.163606   \n30080  ...  0.542412  0.236196  0.170918  0.283692  0.270579  0.275420   \n30081  ...  0.387640  0.075916  0.316283  0.119015  0.101108  0.645644   \n30082  ...  0.394486  0.098980  0.253622  0.046012  0.068931  0.088179   \n\n           2556      2557      2558      2559  \n0      0.200983  0.098618  0.276763  0.473490  \n1      0.095033  0.151809  0.048868  0.117311  \n2      0.313144  0.356484  0.112562  0.404958  \n3      0.273713  0.329452  0.203884  0.219169  \n4      0.595066  0.150074  0.454537  0.071115  \n...         ...       ...       ...       ...  \n30078  0.388353  0.156808  0.239679  0.161568  \n30079  0.415589  0.059742  0.148673  0.164074  \n30080  0.285688  0.203561  0.118533  0.222272  \n30081  0.305596  0.319096  0.330187  0.278469  \n30082  0.348486  0.140222  0.289026  0.741221  \n\n[30083 rows x 2562 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>PatientID</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>2550</th>\n      <th>2551</th>\n      <th>2552</th>\n      <th>2553</th>\n      <th>2554</th>\n      <th>2555</th>\n      <th>2556</th>\n      <th>2557</th>\n      <th>2558</th>\n      <th>2559</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.8.498.26697628953273228189...</td>\n      <td>ec89415d1</td>\n      <td>0.169359</td>\n      <td>0.324016</td>\n      <td>0.173553</td>\n      <td>0.095382</td>\n      <td>0.063119</td>\n      <td>0.165738</td>\n      <td>0.181906</td>\n      <td>0.437814</td>\n      <td>...</td>\n      <td>0.302277</td>\n      <td>0.070963</td>\n      <td>0.354162</td>\n      <td>0.284205</td>\n      <td>0.057408</td>\n      <td>0.101549</td>\n      <td>0.200983</td>\n      <td>0.098618</td>\n      <td>0.276763</td>\n      <td>0.473490</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.8.498.46302891597398758759...</td>\n      <td>bf4c6da3c</td>\n      <td>0.365546</td>\n      <td>0.175262</td>\n      <td>0.286057</td>\n      <td>0.205560</td>\n      <td>0.388747</td>\n      <td>0.452293</td>\n      <td>0.021495</td>\n      <td>0.460131</td>\n      <td>...</td>\n      <td>0.232927</td>\n      <td>0.223938</td>\n      <td>0.135535</td>\n      <td>0.042117</td>\n      <td>0.169312</td>\n      <td>0.383767</td>\n      <td>0.095033</td>\n      <td>0.151809</td>\n      <td>0.048868</td>\n      <td>0.117311</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.8.498.23819260719748494858...</td>\n      <td>3fc1c97e5</td>\n      <td>0.211337</td>\n      <td>0.303350</td>\n      <td>0.184402</td>\n      <td>0.248883</td>\n      <td>0.202594</td>\n      <td>0.301177</td>\n      <td>0.208046</td>\n      <td>0.416033</td>\n      <td>...</td>\n      <td>0.370746</td>\n      <td>0.315548</td>\n      <td>0.384991</td>\n      <td>0.181793</td>\n      <td>0.134341</td>\n      <td>0.147335</td>\n      <td>0.313144</td>\n      <td>0.356484</td>\n      <td>0.112562</td>\n      <td>0.404958</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.8.498.68286643202323212801...</td>\n      <td>c31019814</td>\n      <td>0.520994</td>\n      <td>0.385111</td>\n      <td>0.367907</td>\n      <td>0.247489</td>\n      <td>0.097945</td>\n      <td>0.448468</td>\n      <td>0.377076</td>\n      <td>0.108094</td>\n      <td>...</td>\n      <td>0.355882</td>\n      <td>0.156018</td>\n      <td>0.316916</td>\n      <td>0.141020</td>\n      <td>0.041345</td>\n      <td>0.297808</td>\n      <td>0.273713</td>\n      <td>0.329452</td>\n      <td>0.203884</td>\n      <td>0.219169</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.8.498.10050203009225938259...</td>\n      <td>207685cd1</td>\n      <td>0.383775</td>\n      <td>0.214603</td>\n      <td>0.263212</td>\n      <td>0.429333</td>\n      <td>0.619226</td>\n      <td>0.365313</td>\n      <td>0.264323</td>\n      <td>0.201577</td>\n      <td>...</td>\n      <td>0.302169</td>\n      <td>0.318546</td>\n      <td>0.097627</td>\n      <td>0.337127</td>\n      <td>0.124034</td>\n      <td>0.266738</td>\n      <td>0.595066</td>\n      <td>0.150074</td>\n      <td>0.454537</td>\n      <td>0.071115</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>1.2.826.0.1.3680043.8.498.74257566841157531124...</td>\n      <td>5b5b9ac30</td>\n      <td>0.152407</td>\n      <td>0.273989</td>\n      <td>0.429204</td>\n      <td>0.095627</td>\n      <td>0.143194</td>\n      <td>0.151159</td>\n      <td>0.190642</td>\n      <td>0.151711</td>\n      <td>...</td>\n      <td>0.417215</td>\n      <td>0.330231</td>\n      <td>0.257251</td>\n      <td>0.193722</td>\n      <td>0.070763</td>\n      <td>0.415085</td>\n      <td>0.388353</td>\n      <td>0.156808</td>\n      <td>0.239679</td>\n      <td>0.161568</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>1.2.826.0.1.3680043.8.498.46510939987173529969...</td>\n      <td>7192404d8</td>\n      <td>0.303968</td>\n      <td>0.299640</td>\n      <td>0.232293</td>\n      <td>0.115276</td>\n      <td>0.309395</td>\n      <td>0.338309</td>\n      <td>0.164800</td>\n      <td>0.221858</td>\n      <td>...</td>\n      <td>0.509008</td>\n      <td>0.143263</td>\n      <td>0.214838</td>\n      <td>0.167794</td>\n      <td>0.075009</td>\n      <td>0.163606</td>\n      <td>0.415589</td>\n      <td>0.059742</td>\n      <td>0.148673</td>\n      <td>0.164074</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>1.2.826.0.1.3680043.8.498.43173270582850645437...</td>\n      <td>d4d1b066d</td>\n      <td>0.316939</td>\n      <td>0.351076</td>\n      <td>0.589629</td>\n      <td>0.291154</td>\n      <td>0.185748</td>\n      <td>0.232997</td>\n      <td>0.113028</td>\n      <td>0.353113</td>\n      <td>...</td>\n      <td>0.542412</td>\n      <td>0.236196</td>\n      <td>0.170918</td>\n      <td>0.283692</td>\n      <td>0.270579</td>\n      <td>0.275420</td>\n      <td>0.285688</td>\n      <td>0.203561</td>\n      <td>0.118533</td>\n      <td>0.222272</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>1.2.826.0.1.3680043.8.498.95092491950130838685...</td>\n      <td>01a6602b8</td>\n      <td>0.239194</td>\n      <td>0.393650</td>\n      <td>0.438603</td>\n      <td>0.363903</td>\n      <td>0.144577</td>\n      <td>0.259132</td>\n      <td>0.366278</td>\n      <td>0.193645</td>\n      <td>...</td>\n      <td>0.387640</td>\n      <td>0.075916</td>\n      <td>0.316283</td>\n      <td>0.119015</td>\n      <td>0.101108</td>\n      <td>0.645644</td>\n      <td>0.305596</td>\n      <td>0.319096</td>\n      <td>0.330187</td>\n      <td>0.278469</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>1.2.826.0.1.3680043.8.498.99518162226171269731...</td>\n      <td>e692d316c</td>\n      <td>0.038298</td>\n      <td>0.476334</td>\n      <td>0.110016</td>\n      <td>0.089027</td>\n      <td>0.106914</td>\n      <td>0.169113</td>\n      <td>0.072551</td>\n      <td>0.151309</td>\n      <td>...</td>\n      <td>0.394486</td>\n      <td>0.098980</td>\n      <td>0.253622</td>\n      <td>0.046012</td>\n      <td>0.068931</td>\n      <td>0.088179</td>\n      <td>0.348486</td>\n      <td>0.140222</td>\n      <td>0.289026</td>\n      <td>0.741221</td>\n    </tr>\n  </tbody>\n</table>\n<p>30083 rows × 2562 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "features=pd.read_csv(\"./models/efficientnet_output_normalized.csv\")\n",
    "display(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        0         1         2         3         4         5         6    7   \\\n0      0.0  3.523285  2.151111  3.073467  3.363639  3.601960  1.177225  0.0   \n1      0.0  2.752369  2.418850  2.875795  3.922628  5.291559  4.010684  0.0   \n2      0.0  5.127551  4.384488  8.046625  5.567223  6.195307  7.095205  0.0   \n3      0.0  2.767875  1.731207  5.098400  2.484329  3.534876  4.957430  0.0   \n4      0.0  5.906751  2.528070  5.296381  2.448073  2.876235  8.141816  0.0   \n...    ...       ...       ...       ...       ...       ...       ...  ...   \n30078  0.0  2.509894  2.170190  3.057890  5.210413  4.008926  4.459599  0.0   \n30079  0.0  3.314908  2.372607  1.440810  2.099609  3.282451  4.099072  0.0   \n30080  0.0  5.604525  2.085924  4.015664  4.236988  6.777108  4.505717  0.0   \n30081  0.0  5.239021  1.398003  3.633277  4.615515  3.244748  3.063015  0.0   \n30082  0.0  2.697263  2.477276  3.014830  3.863428  2.657923  2.588749  0.0   \n\n             8         9   ...   90        91        92        93        94  \\\n0      3.399705  4.387469  ...  0.0  1.328145  1.331692  1.144717  2.307059   \n1      4.176135  4.387034  ...  0.0  1.365769  3.142598  6.856853  2.623820   \n2      4.194753  1.949514  ...  0.0  1.316996  5.129144  5.434055  5.875688   \n3      6.703936  4.863359  ...  0.0  0.875055  3.810669  2.164724  1.225728   \n4      4.746807  5.582016  ...  0.0  1.980754  6.254673  1.629905  3.840276   \n...         ...       ...  ...  ...       ...       ...       ...       ...   \n30078  4.928145  5.961706  ...  0.0  2.131990  4.075906  4.149724  2.756427   \n30079  3.791726  5.290777  ...  0.0  1.977184  6.820914  5.900064  3.299438   \n30080  5.942400  6.121769  ...  0.0  1.665230  5.557427  4.285787  2.033923   \n30081  6.512488  2.196148  ...  0.0  1.476729  3.201416  1.321704  2.968237   \n30082  2.194187  6.879325  ...  0.0  1.923943  2.995123  2.970642  3.018536   \n\n             95        96         97        98        99  \n0      2.733480  2.879669   4.131839  3.130108  2.915718  \n1      4.440138  6.352470   7.112836  8.021735  6.673033  \n2      4.957835  5.093932   4.814246  1.765945  7.694550  \n3      1.915580  2.081527   1.964060  2.458131  4.171708  \n4      2.863812  4.631470   4.077279  5.729913  6.588160  \n...         ...       ...        ...       ...       ...  \n30078  2.651626  5.739603   5.785378  3.671273  3.416882  \n30079  3.492918  4.771842   4.394142  3.231114  3.616853  \n30080  4.248517  4.583074  10.027946  7.137775  5.121286  \n30081  3.272295  2.854025   0.896478  3.066891  2.736391  \n30082  6.120244  4.012001   7.346872  2.355797  3.723385  \n\n[30083 rows x 100 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>3.523285</td>\n      <td>2.151111</td>\n      <td>3.073467</td>\n      <td>3.363639</td>\n      <td>3.601960</td>\n      <td>1.177225</td>\n      <td>0.0</td>\n      <td>3.399705</td>\n      <td>4.387469</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.328145</td>\n      <td>1.331692</td>\n      <td>1.144717</td>\n      <td>2.307059</td>\n      <td>2.733480</td>\n      <td>2.879669</td>\n      <td>4.131839</td>\n      <td>3.130108</td>\n      <td>2.915718</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>2.752369</td>\n      <td>2.418850</td>\n      <td>2.875795</td>\n      <td>3.922628</td>\n      <td>5.291559</td>\n      <td>4.010684</td>\n      <td>0.0</td>\n      <td>4.176135</td>\n      <td>4.387034</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.365769</td>\n      <td>3.142598</td>\n      <td>6.856853</td>\n      <td>2.623820</td>\n      <td>4.440138</td>\n      <td>6.352470</td>\n      <td>7.112836</td>\n      <td>8.021735</td>\n      <td>6.673033</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>5.127551</td>\n      <td>4.384488</td>\n      <td>8.046625</td>\n      <td>5.567223</td>\n      <td>6.195307</td>\n      <td>7.095205</td>\n      <td>0.0</td>\n      <td>4.194753</td>\n      <td>1.949514</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.316996</td>\n      <td>5.129144</td>\n      <td>5.434055</td>\n      <td>5.875688</td>\n      <td>4.957835</td>\n      <td>5.093932</td>\n      <td>4.814246</td>\n      <td>1.765945</td>\n      <td>7.694550</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>2.767875</td>\n      <td>1.731207</td>\n      <td>5.098400</td>\n      <td>2.484329</td>\n      <td>3.534876</td>\n      <td>4.957430</td>\n      <td>0.0</td>\n      <td>6.703936</td>\n      <td>4.863359</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.875055</td>\n      <td>3.810669</td>\n      <td>2.164724</td>\n      <td>1.225728</td>\n      <td>1.915580</td>\n      <td>2.081527</td>\n      <td>1.964060</td>\n      <td>2.458131</td>\n      <td>4.171708</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>5.906751</td>\n      <td>2.528070</td>\n      <td>5.296381</td>\n      <td>2.448073</td>\n      <td>2.876235</td>\n      <td>8.141816</td>\n      <td>0.0</td>\n      <td>4.746807</td>\n      <td>5.582016</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.980754</td>\n      <td>6.254673</td>\n      <td>1.629905</td>\n      <td>3.840276</td>\n      <td>2.863812</td>\n      <td>4.631470</td>\n      <td>4.077279</td>\n      <td>5.729913</td>\n      <td>6.588160</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>0.0</td>\n      <td>2.509894</td>\n      <td>2.170190</td>\n      <td>3.057890</td>\n      <td>5.210413</td>\n      <td>4.008926</td>\n      <td>4.459599</td>\n      <td>0.0</td>\n      <td>4.928145</td>\n      <td>5.961706</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2.131990</td>\n      <td>4.075906</td>\n      <td>4.149724</td>\n      <td>2.756427</td>\n      <td>2.651626</td>\n      <td>5.739603</td>\n      <td>5.785378</td>\n      <td>3.671273</td>\n      <td>3.416882</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>0.0</td>\n      <td>3.314908</td>\n      <td>2.372607</td>\n      <td>1.440810</td>\n      <td>2.099609</td>\n      <td>3.282451</td>\n      <td>4.099072</td>\n      <td>0.0</td>\n      <td>3.791726</td>\n      <td>5.290777</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.977184</td>\n      <td>6.820914</td>\n      <td>5.900064</td>\n      <td>3.299438</td>\n      <td>3.492918</td>\n      <td>4.771842</td>\n      <td>4.394142</td>\n      <td>3.231114</td>\n      <td>3.616853</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>0.0</td>\n      <td>5.604525</td>\n      <td>2.085924</td>\n      <td>4.015664</td>\n      <td>4.236988</td>\n      <td>6.777108</td>\n      <td>4.505717</td>\n      <td>0.0</td>\n      <td>5.942400</td>\n      <td>6.121769</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.665230</td>\n      <td>5.557427</td>\n      <td>4.285787</td>\n      <td>2.033923</td>\n      <td>4.248517</td>\n      <td>4.583074</td>\n      <td>10.027946</td>\n      <td>7.137775</td>\n      <td>5.121286</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>0.0</td>\n      <td>5.239021</td>\n      <td>1.398003</td>\n      <td>3.633277</td>\n      <td>4.615515</td>\n      <td>3.244748</td>\n      <td>3.063015</td>\n      <td>0.0</td>\n      <td>6.512488</td>\n      <td>2.196148</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.476729</td>\n      <td>3.201416</td>\n      <td>1.321704</td>\n      <td>2.968237</td>\n      <td>3.272295</td>\n      <td>2.854025</td>\n      <td>0.896478</td>\n      <td>3.066891</td>\n      <td>2.736391</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>0.0</td>\n      <td>2.697263</td>\n      <td>2.477276</td>\n      <td>3.014830</td>\n      <td>3.863428</td>\n      <td>2.657923</td>\n      <td>2.588749</td>\n      <td>0.0</td>\n      <td>2.194187</td>\n      <td>6.879325</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.923943</td>\n      <td>2.995123</td>\n      <td>2.970642</td>\n      <td>3.018536</td>\n      <td>6.120244</td>\n      <td>4.012001</td>\n      <td>7.346872</td>\n      <td>2.355797</td>\n      <td>3.723385</td>\n    </tr>\n  </tbody>\n</table>\n<p>30083 rows × 100 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "autoencoder=models.load_model(\"./models/autoencoder_splits10/\")\n",
    "\n",
    "layer_name=\"dense_1\"\n",
    "hidden_layer_model=models.Model(inputs=autoencoder.input,outputs=autoencoder.get_layer(layer_name).output)\n",
    "\n",
    "pred=hidden_layer_model.predict(features.iloc[:,2:])\n",
    "X=pd.DataFrame(pred)\n",
    "display(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "e: 0.7385304264313755 and parameters: {'lambda_l1': 3.2400178476066615e-06, 'lambda_l2': 0.00013353527786452612}. Best is trial 43 with value: 0.7453032197788487.\u001b[0m\n",
      "regularization_factors, val_score: 0.745303:  30%|###       | 6/20 [00:20<00:48,  3.43s/it][100]\tvalid_0's auc: 0.73853\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.733921\n",
      "regularization_factors, val_score: 0.745303:  35%|###5      | 7/20 [00:23<00:45,  3.52s/it]\u001b[32m[I 2021-03-07 10:38:41,319]\u001b[0m Trial 49 finished with value: 0.7362948315832538 and parameters: {'lambda_l1': 3.5596063081489196e-06, 'lambda_l2': 1.937241259959886e-05}. Best is trial 43 with value: 0.7453032197788487.\u001b[0m\n",
      "regularization_factors, val_score: 0.745303:  35%|###5      | 7/20 [00:23<00:45,  3.52s/it][100]\tvalid_0's auc: 0.736295\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.719162\n",
      "regularization_factors, val_score: 0.745303:  40%|####      | 8/20 [00:27<00:42,  3.56s/it]\u001b[32m[I 2021-03-07 10:38:44,968]\u001b[0m Trial 50 finished with value: 0.7330357376012255 and parameters: {'lambda_l1': 0.04176351723767753, 'lambda_l2': 0.0224243011552411}. Best is trial 43 with value: 0.7453032197788487.\u001b[0m\n",
      "regularization_factors, val_score: 0.745303:  40%|####      | 8/20 [00:27<00:42,  3.56s/it][100]\tvalid_0's auc: 0.733036\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.711718\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "regularization_factors, val_score: 0.745303:  45%|####5     | 9/20 [00:30<00:36,  3.27s/it]\u001b[32m[I 2021-03-07 10:38:47,606]\u001b[0m Trial 51 finished with value: 0.7181141212414878 and parameters: {'lambda_l1': 1.2595898653517137, 'lambda_l2': 1.2294056631068138e-08}. Best is trial 43 with value: 0.7453032197788487.\u001b[0m\n",
      "No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's auc: 0.718114\n",
      "regularization_factors, val_score: 0.745303:  45%|####5     | 9/20 [00:30<00:36,  3.27s/it][LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012782 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.726374\n",
      "regularization_factors, val_score: 0.745303:  50%|#####     | 10/20 [00:33<00:34,  3.41s/it]\u001b[32m[I 2021-03-07 10:38:51,319]\u001b[0m Trial 52 finished with value: 0.7324155450005948 and parameters: {'lambda_l1': 1.6530019135321105e-06, 'lambda_l2': 0.018085810992402175}. Best is trial 43 with value: 0.7453032197788487.\u001b[0m\n",
      "regularization_factors, val_score: 0.745303:  50%|#####     | 10/20 [00:33<00:34,  3.41s/it][100]\tvalid_0's auc: 0.732416\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.723478\n",
      "regularization_factors, val_score: 0.745303:  55%|#####5    | 11/20 [00:36<00:29,  3.31s/it]\u001b[32m[I 2021-03-07 10:38:54,419]\u001b[0m Trial 53 finished with value: 0.7333853666462595 and parameters: {'lambda_l1': 5.323132201295658e-08, 'lambda_l2': 9.33999773085335}. Best is trial 43 with value: 0.7453032197788487.\u001b[0m\n",
      "regularization_factors, val_score: 0.745303:  55%|#####5    | 11/20 [00:36<00:29,  3.31s/it][100]\tvalid_0's auc: 0.733385\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.72271\n",
      "regularization_factors, val_score: 0.745303:  60%|######    | 12/20 [00:40<00:27,  3.48s/it]\u001b[32m[I 2021-03-07 10:38:58,269]\u001b[0m Trial 54 finished with value: 0.7318649880603857 and parameters: {'lambda_l1': 2.4446581067831506e-08, 'lambda_l2': 0.01825672826978084}. Best is trial 43 with value: 0.7453032197788487.\u001b[0m\n",
      "regularization_factors, val_score: 0.745303:  60%|######    | 12/20 [00:40<00:27,  3.48s/it][100]\tvalid_0's auc: 0.731865\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013618 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.739127\n",
      "regularization_factors, val_score: 0.751392:  65%|######5   | 13/20 [00:44<00:24,  3.57s/it]\u001b[32m[I 2021-03-07 10:39:02,044]\u001b[0m Trial 55 finished with value: 0.7513919878369714 and parameters: {'lambda_l1': 0.0004065547184871786, 'lambda_l2': 7.15314746964025e-06}. Best is trial 55 with value: 0.7513919878369714.\u001b[0m\n",
      "regularization_factors, val_score: 0.751392:  65%|######5   | 13/20 [00:44<00:24,  3.57s/it][100]\tvalid_0's auc: 0.751392\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.722267\n",
      "regularization_factors, val_score: 0.751392:  70%|#######   | 14/20 [00:48<00:21,  3.63s/it]\u001b[32m[I 2021-03-07 10:39:05,816]\u001b[0m Trial 56 finished with value: 0.7313129803772512 and parameters: {'lambda_l1': 0.0008240787374240012, 'lambda_l2': 0.001056800132717259}. Best is trial 55 with value: 0.7513919878369714.\u001b[0m\n",
      "regularization_factors, val_score: 0.751392:  70%|#######   | 14/20 [00:48<00:21,  3.63s/it][100]\tvalid_0's auc: 0.731313\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.742174\n",
      "regularization_factors, val_score: 0.751392:  75%|#######5  | 15/20 [00:51<00:17,  3.58s/it]\u001b[32m[I 2021-03-07 10:39:09,296]\u001b[0m Trial 57 finished with value: 0.7439605572013428 and parameters: {'lambda_l1': 0.00016061990885536223, 'lambda_l2': 0.7896119154314718}. Best is trial 55 with value: 0.7513919878369714.\u001b[0m\n",
      "regularization_factors, val_score: 0.751392:  75%|#######5  | 15/20 [00:51<00:17,  3.58s/it][100]\tvalid_0's auc: 0.743961\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.745918\n",
      "regularization_factors, val_score: 0.751392:  80%|########  | 16/20 [00:55<00:14,  3.67s/it]\u001b[32m[I 2021-03-07 10:39:13,152]\u001b[0m Trial 58 finished with value: 0.7513695013216268 and parameters: {'lambda_l1': 0.004514321064740586, 'lambda_l2': 0.0007694788605182103}. Best is trial 55 with value: 0.7513919878369714.\u001b[0m\n",
      "regularization_factors, val_score: 0.751392:  80%|########  | 16/20 [00:55<00:14,  3.67s/it][100]\tvalid_0's auc: 0.75137\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.717976\n",
      "regularization_factors, val_score: 0.751392:  85%|########5 | 17/20 [00:59<00:10,  3.67s/it]\u001b[32m[I 2021-03-07 10:39:16,817]\u001b[0m Trial 59 finished with value: 0.7280154417076985 and parameters: {'lambda_l1': 0.05567295320664314, 'lambda_l2': 0.00031722264427065563}. Best is trial 55 with value: 0.7513919878369714.\u001b[0m\n",
      "regularization_factors, val_score: 0.751392:  85%|########5 | 17/20 [00:59<00:10,  3.67s/it][100]\tvalid_0's auc: 0.728015\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.737743\n",
      "regularization_factors, val_score: 0.751392:  90%|######### | 18/20 [01:03<00:07,  3.82s/it]\u001b[32m[I 2021-03-07 10:39:20,989]\u001b[0m Trial 60 finished with value: 0.7343515614346107 and parameters: {'lambda_l1': 0.0012386328515833185, 'lambda_l2': 2.0784965632224236e-05}. Best is trial 55 with value: 0.7513919878369714.\u001b[0m\n",
      "regularization_factors, val_score: 0.751392:  90%|######### | 18/20 [01:03<00:07,  3.82s/it][100]\tvalid_0's auc: 0.734352\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.737161\n",
      "regularization_factors, val_score: 0.751392:  95%|#########5| 19/20 [01:07<00:03,  3.89s/it]\u001b[32m[I 2021-03-07 10:39:25,057]\u001b[0m Trial 61 finished with value: 0.7381655645856243 and parameters: {'lambda_l1': 0.18504150697806587, 'lambda_l2': 0.0016320383356252298}. Best is trial 55 with value: 0.7513919878369714.\u001b[0m\n",
      "regularization_factors, val_score: 0.751392:  95%|#########5| 19/20 [01:07<00:03,  3.89s/it][100]\tvalid_0's auc: 0.738166\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010576 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.741067\n",
      "regularization_factors, val_score: 0.751392: 100%|##########| 20/20 [01:11<00:00,  3.91s/it]\u001b[32m[I 2021-03-07 10:39:28,999]\u001b[0m Trial 62 finished with value: 0.7384970593440902 and parameters: {'lambda_l1': 0.002707112619818617, 'lambda_l2': 2.4140098323450544e-08}. Best is trial 55 with value: 0.7513919878369714.\u001b[0m\n",
      "regularization_factors, val_score: 0.751392: 100%|##########| 20/20 [01:11<00:00,  3.58s/it]\n",
      "min_data_in_leaf, val_score: 0.751392:   0%|          | 0/5 [00:00<?, ?it/s][100]\tvalid_0's auc: 0.738497\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.721057\n",
      "min_data_in_leaf, val_score: 0.751392:  20%|##        | 1/5 [00:04<00:17,  4.25s/it]\u001b[32m[I 2021-03-07 10:39:33,268]\u001b[0m Trial 63 finished with value: 0.7369520181284835 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 0.7369520181284835.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.751392:  20%|##        | 1/5 [00:04<00:17,  4.25s/it][100]\tvalid_0's auc: 0.736952\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.725932\n",
      "min_data_in_leaf, val_score: 0.751392:  40%|####      | 2/5 [00:08<00:12,  4.21s/it]\u001b[32m[I 2021-03-07 10:39:37,445]\u001b[0m Trial 64 finished with value: 0.7399398231834522 and parameters: {'min_child_samples': 25}. Best is trial 64 with value: 0.7399398231834522.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.751392:  40%|####      | 2/5 [00:08<00:12,  4.21s/it][100]\tvalid_0's auc: 0.73994\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tvalid_0's auc: 0.735435\n",
      "min_data_in_leaf, val_score: 0.751392:  60%|######    | 3/5 [00:11<00:07,  3.73s/it]\u001b[32m[I 2021-03-07 10:39:40,603]\u001b[0m Trial 65 finished with value: 0.7398498771220742 and parameters: {'min_child_samples': 100}. Best is trial 64 with value: 0.7399398231834522.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.751392:  60%|######    | 3/5 [00:11<00:07,  3.73s/it][100]\tvalid_0's auc: 0.73985\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.726659\n",
      "min_data_in_leaf, val_score: 0.751392:  80%|########  | 4/5 [00:15<00:03,  3.61s/it]\u001b[32m[I 2021-03-07 10:39:44,027]\u001b[0m Trial 66 finished with value: 0.7331002956614082 and parameters: {'min_child_samples': 50}. Best is trial 64 with value: 0.7399398231834522.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.751392:  80%|########  | 4/5 [00:15<00:03,  3.61s/it][100]\tvalid_0's auc: 0.7331\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20151\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.724736\n",
      "min_data_in_leaf, val_score: 0.751392: 100%|##########| 5/5 [00:19<00:00,  3.80s/it]\u001b[32m[I 2021-03-07 10:39:48,177]\u001b[0m Trial 67 finished with value: 0.740912546314968 and parameters: {'min_child_samples': 5}. Best is trial 67 with value: 0.740912546314968.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.751392: 100%|##########| 5/5 [00:19<00:00,  3.83s/it][100]\tvalid_0's auc: 0.740913\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36moptimize_params\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from optuna.integration import lightgbm as lgb\n",
    "\n",
    "n=0\n",
    "train_idx=(fold[\"folds\"]!=n)\n",
    "val_idx=(fold[\"folds\"]==n)\n",
    "\n",
    "def optimize_params():\n",
    "    X_train=X[train_idx]\n",
    "    X_val=X[val_idx]\n",
    "    y_train=train[\"CVC Present\"][train_idx]\n",
    "    y_val=train[\"CVC Present\"][val_idx]\n",
    "\n",
    "    lgb_train=lgb.Dataset(X_train,label=y_train)\n",
    "    lgb_test=lgb.Dataset(X_val,label=y_val,reference=lgb_train)\n",
    "    \n",
    "    params={\n",
    "        \"task\":\"train\",\n",
    "        \"boosting_type\":\"gbdt\",\n",
    "        \"objective\":\"binary\",\n",
    "        \"metric\":\"auc\",\n",
    "        \"learning_rate\":0.1,\n",
    "        \"num_iterations\":100\n",
    "        # \"early_stopping_rounds\":200, #early_stopping_roundsを指定しないとbest_iterationは保存されない\n",
    "    }\n",
    "\n",
    "    opt=lgb.train(params,lgb_train,valid_sets=lgb_test, verbose_eval=50)\n",
    "    pickle.dump(opt.params,open(f\"{models_dir}lgbm_cvc/lgb_params_{n}.pickle\",\"wb\"))\n",
    "\n",
    "optimize_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "fold - 0\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20145\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\tvalid_0's auc: 0.703537\n",
      "[20]\tvalid_0's auc: 0.703236\n",
      "[30]\tvalid_0's auc: 0.71102\n",
      "[40]\tvalid_0's auc: 0.716908\n",
      "[50]\tvalid_0's auc: 0.721972\n",
      "[60]\tvalid_0's auc: 0.721623\n",
      "[70]\tvalid_0's auc: 0.724404\n",
      "[80]\tvalid_0's auc: 0.73053\n",
      "[90]\tvalid_0's auc: 0.735704\n",
      "[100]\tvalid_0's auc: 0.73594\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[92]\tvalid_0's auc: 0.738053\n",
      "\n",
      "fold - 1\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017441 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20145\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\tvalid_0's auc: 0.724946\n",
      "[20]\tvalid_0's auc: 0.737368\n",
      "[30]\tvalid_0's auc: 0.744738\n",
      "[40]\tvalid_0's auc: 0.751392\n",
      "[50]\tvalid_0's auc: 0.754282\n",
      "[60]\tvalid_0's auc: 0.751097\n",
      "[70]\tvalid_0's auc: 0.751832\n",
      "[80]\tvalid_0's auc: 0.755047\n",
      "[90]\tvalid_0's auc: 0.754375\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's auc: 0.757254\n",
      "\n",
      "fold - 2\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 21999, number of negative: 563\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20145\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975047 -> initscore=3.665473\n",
      "[LightGBM] [Info] Start training from score 3.665473\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\tvalid_0's auc: 0.712095\n",
      "[20]\tvalid_0's auc: 0.718398\n",
      "[30]\tvalid_0's auc: 0.73606\n",
      "[40]\tvalid_0's auc: 0.745442\n",
      "[50]\tvalid_0's auc: 0.746945\n",
      "[60]\tvalid_0's auc: 0.745795\n",
      "[70]\tvalid_0's auc: 0.749667\n",
      "[80]\tvalid_0's auc: 0.751389\n",
      "[90]\tvalid_0's auc: 0.754215\n",
      "[100]\tvalid_0's auc: 0.754054\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[93]\tvalid_0's auc: 0.756679\n",
      "\n",
      "fold - 3\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 563\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20145\n",
      "[LightGBM] [Info] Number of data points in the train set: 22563, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975048 -> initscore=3.665518\n",
      "[LightGBM] [Info] Start training from score 3.665518\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\tvalid_0's auc: 0.716614\n",
      "[20]\tvalid_0's auc: 0.721757\n",
      "[30]\tvalid_0's auc: 0.72716\n",
      "[40]\tvalid_0's auc: 0.7328\n",
      "[50]\tvalid_0's auc: 0.734069\n",
      "[60]\tvalid_0's auc: 0.741918\n",
      "[70]\tvalid_0's auc: 0.745761\n",
      "[80]\tvalid_0's auc: 0.748136\n",
      "[90]\tvalid_0's auc: 0.746922\n",
      "[100]\tvalid_0's auc: 0.749063\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.749063\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "         CVC Present\nfold - 0    0.738053\nfold - 1    0.757254\nfold - 2    0.756679\nfold - 3    0.749063",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CVC Present</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>fold - 0</th>\n      <td>0.738053</td>\n    </tr>\n    <tr>\n      <th>fold - 1</th>\n      <td>0.757254</td>\n    </tr>\n    <tr>\n      <th>fold - 2</th>\n      <td>0.756679</td>\n    </tr>\n    <tr>\n      <th>fold - 3</th>\n      <td>0.749063</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "CVC Present    0.750262\ndtype: float64"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "\n",
    "def get_pred(train,val):\n",
    "\n",
    "    X_train,y_train=train\n",
    "    X_val,y_val=val\n",
    "\n",
    "    lgb_train=lightgbm.Dataset(X_train,label=y_train)\n",
    "    lgb_test=lightgbm.Dataset(X_val,label=y_val,reference=lgb_train)\n",
    "\n",
    "    # params=pickle.load(open(f\"{CFG.models_dir}autoencoder_smallLR/lgb_params_{col_index}.pickle\",\"rb\"))\n",
    "    # params[\"early_stopping_rounds\"]=500\n",
    "    params={\n",
    "        \"task\":\"train\",\n",
    "        \"boosting_type\":\"gbdt\",\n",
    "        \"objective\":\"binary\",\n",
    "        \"metric\":\"auc\",\n",
    "        \"early_stopping_rounds\":50, #early_stopping_roundsを指定しないとbest_iterationは保存されない\n",
    "    }\n",
    "\n",
    "    lgbm_model=lightgbm.train(params,lgb_train,valid_sets=lgb_test,verbose_eval=10)\n",
    "    pred=lgbm_model.predict(X_val)\n",
    "    auc=roc_auc_score(y_val,pred)\n",
    "\n",
    "    return pred,auc\n",
    "\n",
    "\n",
    "results=pd.DataFrame(columns=[\"CVC Present\"])\n",
    "\n",
    "for n in range(CFG.n_folds):\n",
    "    print(f\"\\nfold - {n}\")\n",
    "    train_idx=(fold[\"folds\"]!=n)\n",
    "    val_idx=(fold[\"folds\"]==n)\n",
    "\n",
    "    train_data=X[train_idx].iloc[:,1:],train[\"CVC Present\"][train_idx]\n",
    "    val_data=X[val_idx].iloc[:,1:],train[\"CVC Present\"][val_idx]\n",
    "\n",
    "    _,auc=get_pred(train=train_data,val=val_data)\n",
    "        \n",
    "    results.loc[f\"fold - {n}\",\"CVC Present\"]=auc\n",
    "\n",
    "\n",
    "display(results,results.mean(axis=0))"
   ]
  }
 ]
}