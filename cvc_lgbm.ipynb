{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3da45907e5cd41bdac7b692b424c19725633d98a853f93b11b400b1d8951ac30"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    dataset_dir=\"../input/ranzcr-clip-catheter-line-classification/\"\n",
    "    models_dir=\"./models/\"\n",
    "    n_folds=4\n",
    "    target_cols=['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal', 'NGT - Abnormal', 'NGT - Borderline',\n",
    "       'NGT - Incompletely Imaged', 'NGT - Normal', 'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal', 'Swan Ganz Catheter Present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(f\"{CFG.dataset_dir}train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "1    29333\n0      750\nName: CVC Present, dtype: int64"
     },
     "metadata": {}
    }
   ],
   "source": [
    "cvc_cols=[col_name for col_name in CFG.target_cols if (\"CVC\" in col_name)]\n",
    "\n",
    "train[\"CVC Present\"]=train[cvc_cols].sum(axis=1)\n",
    "false_idx=(train[\"CVC Present\"]==0)\n",
    "train[\"CVC Present\"]=1\n",
    "train.loc[false_idx,\"CVC Present\"]=0\n",
    "display(train[\"CVC Present\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold(train):\n",
    "    fold=train.copy()\n",
    "    splitter=StratifiedKFold(n_splits=CFG.n_folds)\n",
    "    for n,(train_idx,val_idx) in enumerate(splitter.split(train,train[\"CVC Present\"])):\n",
    "        fold.loc[val_idx,\"folds\"]=n\n",
    "    fold[\"folds\"]=fold[\"folds\"].astype(int)\n",
    "    return fold\n",
    "\n",
    "fold=get_fold(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 30083/30083 [01:01<00:00, 492.60it/s]\n"
     ]
    }
   ],
   "source": [
    "npz=np.load(\"../input/effnet_tuned_output.npz\")\n",
    "features_list=[npz[uid] for uid in tqdm(train[\"StudyInstanceUID\"])]\n",
    "features=np.array(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "             0         1         2         3         5         6         8   \\\n0      1.843573  0.762214  0.869982  1.386810  1.369750  0.943624  0.868235   \n1      1.316493  1.157231  3.745368  2.056508  1.526756  1.638721  2.746008   \n2      0.846063  1.605237  0.805049  0.902661  1.622584  1.147530  1.275458   \n3      1.093408  1.223410  1.874324  2.219434  0.879066  2.298780  1.477697   \n4      1.628255  1.978409  1.783667  0.579625  1.048298  1.604363  1.627460   \n...         ...       ...       ...       ...       ...       ...       ...   \n30078  1.228411  1.137532  1.340785  1.927307  2.104715  0.949438  2.384785   \n30079  1.395945  1.438516  2.053834  1.840734  1.080493  2.201542  1.104149   \n30080  0.937448  1.242203  2.101459  1.849006  1.106512  1.358821  2.112649   \n30081  1.151241  1.584572  0.763552  0.840122  1.344241  0.730238  1.048365   \n30082  1.531249  2.036184  3.165080  1.142034  1.383862  2.330548  3.722769   \n\n        10        11        14  ...        87        88        90        91  \\\n0      0.0  2.362543  1.963967  ...  0.908345  1.251791  1.660741  0.820461   \n1      0.0  0.728871  0.248490  ...  1.174902  2.501284  1.452303  1.189284   \n2      0.0  2.153320  0.926055  ...  1.184017  0.880394  1.806954  1.442235   \n3      0.0  2.164646  2.043216  ...  1.126441  1.279361  2.040087  1.496110   \n4      0.0  1.761667  0.639846  ...  1.444271  1.143819  1.492713  1.309319   \n...    ...       ...       ...  ...       ...       ...       ...       ...   \n30078  0.0  1.818742  1.342715  ...  1.269412  2.555556  2.009609  1.011214   \n30079  0.0  2.985705  1.766386  ...  1.134073  1.053623  1.217309  1.292022   \n30080  0.0  1.165469  0.969044  ...  2.323654  3.102413  2.430577  1.500344   \n30081  0.0  1.512467  1.206768  ...  1.418048  1.720290  1.908568  1.478042   \n30082  0.0  0.849463  0.799035  ...  1.831066  3.691451  1.310009  1.665144   \n\n             93        94        95        96        98        99  \n0      1.360656  2.405944  1.016294  0.000000  1.393855  0.974065  \n1      2.883156  1.588075  0.725331  0.163878  1.702431  2.194259  \n2      0.801363  1.626994  0.568042  0.000000  1.896320  0.976791  \n3      1.981401  1.202085  0.585374  0.113964  1.038824  0.465566  \n4      0.426623  2.758231  0.724342  0.000000  1.823411  1.396208  \n...         ...       ...       ...       ...       ...       ...  \n30078  1.135506  1.322723  0.672355  0.000000  1.523899  0.802592  \n30079  1.524583  0.577702  0.827970  0.000000  1.983905  1.054130  \n30080  2.230500  2.033732  0.599059  0.000000  1.299392  1.479758  \n30081  0.804574  1.925837  0.350558  0.270158  2.035101  1.448286  \n30082  1.621052  0.964893  0.446503  0.000000  1.106691  2.237899  \n\n[30083 rows x 75 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>5</th>\n      <th>6</th>\n      <th>8</th>\n      <th>10</th>\n      <th>11</th>\n      <th>14</th>\n      <th>...</th>\n      <th>87</th>\n      <th>88</th>\n      <th>90</th>\n      <th>91</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.843573</td>\n      <td>0.762214</td>\n      <td>0.869982</td>\n      <td>1.386810</td>\n      <td>1.369750</td>\n      <td>0.943624</td>\n      <td>0.868235</td>\n      <td>0.0</td>\n      <td>2.362543</td>\n      <td>1.963967</td>\n      <td>...</td>\n      <td>0.908345</td>\n      <td>1.251791</td>\n      <td>1.660741</td>\n      <td>0.820461</td>\n      <td>1.360656</td>\n      <td>2.405944</td>\n      <td>1.016294</td>\n      <td>0.000000</td>\n      <td>1.393855</td>\n      <td>0.974065</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.316493</td>\n      <td>1.157231</td>\n      <td>3.745368</td>\n      <td>2.056508</td>\n      <td>1.526756</td>\n      <td>1.638721</td>\n      <td>2.746008</td>\n      <td>0.0</td>\n      <td>0.728871</td>\n      <td>0.248490</td>\n      <td>...</td>\n      <td>1.174902</td>\n      <td>2.501284</td>\n      <td>1.452303</td>\n      <td>1.189284</td>\n      <td>2.883156</td>\n      <td>1.588075</td>\n      <td>0.725331</td>\n      <td>0.163878</td>\n      <td>1.702431</td>\n      <td>2.194259</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.846063</td>\n      <td>1.605237</td>\n      <td>0.805049</td>\n      <td>0.902661</td>\n      <td>1.622584</td>\n      <td>1.147530</td>\n      <td>1.275458</td>\n      <td>0.0</td>\n      <td>2.153320</td>\n      <td>0.926055</td>\n      <td>...</td>\n      <td>1.184017</td>\n      <td>0.880394</td>\n      <td>1.806954</td>\n      <td>1.442235</td>\n      <td>0.801363</td>\n      <td>1.626994</td>\n      <td>0.568042</td>\n      <td>0.000000</td>\n      <td>1.896320</td>\n      <td>0.976791</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.093408</td>\n      <td>1.223410</td>\n      <td>1.874324</td>\n      <td>2.219434</td>\n      <td>0.879066</td>\n      <td>2.298780</td>\n      <td>1.477697</td>\n      <td>0.0</td>\n      <td>2.164646</td>\n      <td>2.043216</td>\n      <td>...</td>\n      <td>1.126441</td>\n      <td>1.279361</td>\n      <td>2.040087</td>\n      <td>1.496110</td>\n      <td>1.981401</td>\n      <td>1.202085</td>\n      <td>0.585374</td>\n      <td>0.113964</td>\n      <td>1.038824</td>\n      <td>0.465566</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.628255</td>\n      <td>1.978409</td>\n      <td>1.783667</td>\n      <td>0.579625</td>\n      <td>1.048298</td>\n      <td>1.604363</td>\n      <td>1.627460</td>\n      <td>0.0</td>\n      <td>1.761667</td>\n      <td>0.639846</td>\n      <td>...</td>\n      <td>1.444271</td>\n      <td>1.143819</td>\n      <td>1.492713</td>\n      <td>1.309319</td>\n      <td>0.426623</td>\n      <td>2.758231</td>\n      <td>0.724342</td>\n      <td>0.000000</td>\n      <td>1.823411</td>\n      <td>1.396208</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>1.228411</td>\n      <td>1.137532</td>\n      <td>1.340785</td>\n      <td>1.927307</td>\n      <td>2.104715</td>\n      <td>0.949438</td>\n      <td>2.384785</td>\n      <td>0.0</td>\n      <td>1.818742</td>\n      <td>1.342715</td>\n      <td>...</td>\n      <td>1.269412</td>\n      <td>2.555556</td>\n      <td>2.009609</td>\n      <td>1.011214</td>\n      <td>1.135506</td>\n      <td>1.322723</td>\n      <td>0.672355</td>\n      <td>0.000000</td>\n      <td>1.523899</td>\n      <td>0.802592</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>1.395945</td>\n      <td>1.438516</td>\n      <td>2.053834</td>\n      <td>1.840734</td>\n      <td>1.080493</td>\n      <td>2.201542</td>\n      <td>1.104149</td>\n      <td>0.0</td>\n      <td>2.985705</td>\n      <td>1.766386</td>\n      <td>...</td>\n      <td>1.134073</td>\n      <td>1.053623</td>\n      <td>1.217309</td>\n      <td>1.292022</td>\n      <td>1.524583</td>\n      <td>0.577702</td>\n      <td>0.827970</td>\n      <td>0.000000</td>\n      <td>1.983905</td>\n      <td>1.054130</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>0.937448</td>\n      <td>1.242203</td>\n      <td>2.101459</td>\n      <td>1.849006</td>\n      <td>1.106512</td>\n      <td>1.358821</td>\n      <td>2.112649</td>\n      <td>0.0</td>\n      <td>1.165469</td>\n      <td>0.969044</td>\n      <td>...</td>\n      <td>2.323654</td>\n      <td>3.102413</td>\n      <td>2.430577</td>\n      <td>1.500344</td>\n      <td>2.230500</td>\n      <td>2.033732</td>\n      <td>0.599059</td>\n      <td>0.000000</td>\n      <td>1.299392</td>\n      <td>1.479758</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>1.151241</td>\n      <td>1.584572</td>\n      <td>0.763552</td>\n      <td>0.840122</td>\n      <td>1.344241</td>\n      <td>0.730238</td>\n      <td>1.048365</td>\n      <td>0.0</td>\n      <td>1.512467</td>\n      <td>1.206768</td>\n      <td>...</td>\n      <td>1.418048</td>\n      <td>1.720290</td>\n      <td>1.908568</td>\n      <td>1.478042</td>\n      <td>0.804574</td>\n      <td>1.925837</td>\n      <td>0.350558</td>\n      <td>0.270158</td>\n      <td>2.035101</td>\n      <td>1.448286</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>1.531249</td>\n      <td>2.036184</td>\n      <td>3.165080</td>\n      <td>1.142034</td>\n      <td>1.383862</td>\n      <td>2.330548</td>\n      <td>3.722769</td>\n      <td>0.0</td>\n      <td>0.849463</td>\n      <td>0.799035</td>\n      <td>...</td>\n      <td>1.831066</td>\n      <td>3.691451</td>\n      <td>1.310009</td>\n      <td>1.665144</td>\n      <td>1.621052</td>\n      <td>0.964893</td>\n      <td>0.446503</td>\n      <td>0.000000</td>\n      <td>1.106691</td>\n      <td>2.237899</td>\n    </tr>\n  </tbody>\n</table>\n<p>30083 rows × 75 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "def compress_with_autoencoder(features):\n",
    "    scaler=pickle.load(open(f\"{CFG.models_dir}minmaxscaler_effnet_tuned.pickle\",\"rb\"))\n",
    "    X=scaler.transform(features)\n",
    "\n",
    "    autoencoder_dir=f\"{CFG.models_dir}autoencoder_tuned/\"\n",
    "    with open(f\"{autoencoder_dir}model.json\",\"rt\") as f:\n",
    "        model_json=f.read()\n",
    "    autoencoder=models.model_from_json(model_json)\n",
    "    autoencoder.load_weights(f\"{autoencoder_dir}ckpt\")\n",
    "\n",
    "    layer_name=\"dense_1\"\n",
    "    compressing_model=models.Model(inputs=autoencoder.input,outputs=autoencoder.get_layer(layer_name).output)\n",
    "\n",
    "    ae_pred=compressing_model.predict(X)\n",
    "    ae_pred_df=pd.DataFrame(ae_pred)\n",
    "\n",
    "    return ae_pred_df\n",
    "\n",
    "X=compress_with_autoencoder(features)\n",
    "valuless_columns=[4,  7,  9, 12, 13, 20, 25, 27, 30, 31, 36, 44, 47, 48, 51, 54, 64, 65, 71, 73, 74, 75, 89, 92, 97]\n",
    "X=X.drop(columns=valuless_columns)\n",
    "display(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0]\tvalid_0's auc: 0.931295\n",
      "regularization_factors, val_score: 0.943094:  55%|#####5    | 11/20 [00:38<00:33,  3.75s/it]\u001b[32m[I 2021-03-10 05:09:55,390]\u001b[0m Trial 53 finished with value: 0.9360360190453532 and parameters: {'lambda_l1': 1.057180282765046e-08, 'lambda_l2': 0.00476326831230927}. Best is trial 46 with value: 0.9430938833776777.\u001b[0m\n",
      "regularization_factors, val_score: 0.943094:  55%|#####5    | 11/20 [00:38<00:33,  3.75s/it][100]\tvalid_0's auc: 0.936036\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.936909\n",
      "regularization_factors, val_score: 0.943104:  60%|######    | 12/20 [00:42<00:30,  3.78s/it]\u001b[32m[I 2021-03-10 05:09:59,244]\u001b[0m Trial 54 finished with value: 0.9431040385781558 and parameters: {'lambda_l1': 2.793362364758597e-08, 'lambda_l2': 3.6034667098380283e-06}. Best is trial 54 with value: 0.9431040385781558.\u001b[0m\n",
      "regularization_factors, val_score: 0.943104:  60%|######    | 12/20 [00:42<00:30,  3.78s/it][100]\tvalid_0's auc: 0.943104\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.935911\n",
      "regularization_factors, val_score: 0.943104:  65%|######5   | 13/20 [00:46<00:26,  3.82s/it]\u001b[32m[I 2021-03-10 05:10:03,173]\u001b[0m Trial 55 finished with value: 0.9396577987587443 and parameters: {'lambda_l1': 7.351670872162101e-06, 'lambda_l2': 1.1266293778068839e-08}. Best is trial 54 with value: 0.9431040385781558.\u001b[0m\n",
      "regularization_factors, val_score: 0.943104:  65%|######5   | 13/20 [00:46<00:26,  3.82s/it][100]\tvalid_0's auc: 0.939658\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.934479\n",
      "regularization_factors, val_score: 0.943104:  70%|#######   | 14/20 [00:50<00:22,  3.83s/it]\u001b[32m[I 2021-03-10 05:10:07,004]\u001b[0m Trial 56 finished with value: 0.9411114431700474 and parameters: {'lambda_l1': 1.3592843991641402e-08, 'lambda_l2': 5.985334252442941e-06}. Best is trial 54 with value: 0.9431040385781558.\u001b[0m\n",
      "regularization_factors, val_score: 0.943104:  70%|#######   | 14/20 [00:50<00:22,  3.83s/it][100]\tvalid_0's auc: 0.941111\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.931822\n",
      "regularization_factors, val_score: 0.943104:  75%|#######5  | 15/20 [00:54<00:19,  3.88s/it]\u001b[32m[I 2021-03-10 05:10:10,998]\u001b[0m Trial 57 finished with value: 0.933568305329159 and parameters: {'lambda_l1': 3.6289726296719792e-06, 'lambda_l2': 5.831835791919634e-07}. Best is trial 54 with value: 0.9431040385781558.\u001b[0m\n",
      "regularization_factors, val_score: 0.943104:  75%|#######5  | 15/20 [00:54<00:19,  3.88s/it][100]\tvalid_0's auc: 0.933568\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.937837\n",
      "regularization_factors, val_score: 0.943104:  80%|########  | 16/20 [00:58<00:15,  3.90s/it]\u001b[32m[I 2021-03-10 05:10:14,959]\u001b[0m Trial 58 finished with value: 0.9400023502035393 and parameters: {'lambda_l1': 7.627919865950284e-08, 'lambda_l2': 0.0003606852090980082}. Best is trial 54 with value: 0.9431040385781558.\u001b[0m\n",
      "regularization_factors, val_score: 0.943104:  80%|########  | 16/20 [00:58<00:15,  3.90s/it][100]\tvalid_0's auc: 0.940002\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.930641\n",
      "regularization_factors, val_score: 0.943104:  85%|########5 | 17/20 [01:02<00:12,  4.06s/it]\u001b[32m[I 2021-03-10 05:10:19,397]\u001b[0m Trial 59 finished with value: 0.935872085094777 and parameters: {'lambda_l1': 4.433154473778195e-05, 'lambda_l2': 1.3161428408388925e-08}. Best is trial 54 with value: 0.9431040385781558.\u001b[0m\n",
      "regularization_factors, val_score: 0.943104:  85%|########5 | 17/20 [01:02<00:12,  4.06s/it][100]\tvalid_0's auc: 0.935872\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.937136\n",
      "regularization_factors, val_score: 0.943104:  90%|######### | 18/20 [01:06<00:08,  4.03s/it]\u001b[32m[I 2021-03-10 05:10:23,353]\u001b[0m Trial 60 finished with value: 0.9401068036941718 and parameters: {'lambda_l1': 8.46199218895047e-07, 'lambda_l2': 7.242191388889048e-06}. Best is trial 54 with value: 0.9431040385781558.\u001b[0m\n",
      "regularization_factors, val_score: 0.943104:  90%|######### | 18/20 [01:06<00:08,  4.03s/it][100]\tvalid_0's auc: 0.940107\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.936515\n",
      "regularization_factors, val_score: 0.943104:  95%|#########5| 19/20 [01:10<00:03,  3.98s/it]\u001b[32m[I 2021-03-10 05:10:27,230]\u001b[0m Trial 61 finished with value: 0.9377979463283147 and parameters: {'lambda_l1': 1.928694228757405e-08, 'lambda_l2': 0.0006907419891579385}. Best is trial 54 with value: 0.9431040385781558.\u001b[0m\n",
      "regularization_factors, val_score: 0.943104:  95%|#########5| 19/20 [01:10<00:03,  3.98s/it][100]\tvalid_0's auc: 0.937798\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010844 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.928665\n",
      "regularization_factors, val_score: 0.943104: 100%|##########| 20/20 [01:14<00:00,  3.96s/it]\u001b[32m[I 2021-03-10 05:10:31,123]\u001b[0m Trial 62 finished with value: 0.9416126748507911 and parameters: {'lambda_l1': 8.463865968914192e-05, 'lambda_l2': 1.251119413689165e-07}. Best is trial 54 with value: 0.9431040385781558.\u001b[0m\n",
      "regularization_factors, val_score: 0.943104: 100%|##########| 20/20 [01:14<00:00,  3.73s/it]\n",
      "min_data_in_leaf, val_score: 0.943104:   0%|          | 0/5 [00:00<?, ?it/s][100]\tvalid_0's auc: 0.941613\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tvalid_0's auc: 0.932807\n",
      "min_data_in_leaf, val_score: 0.943104:  20%|##        | 1/5 [00:03<00:14,  3.72s/it]\u001b[32m[I 2021-03-10 05:10:34,875]\u001b[0m Trial 63 finished with value: 0.9389084900377483 and parameters: {'min_child_samples': 25}. Best is trial 63 with value: 0.9389084900377483.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.943104:  20%|##        | 1/5 [00:03<00:14,  3.72s/it][100]\tvalid_0's auc: 0.938908\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.937267\n",
      "min_data_in_leaf, val_score: 0.943104:  40%|####      | 2/5 [00:08<00:12,  4.21s/it]\u001b[32m[I 2021-03-10 05:10:39,425]\u001b[0m Trial 64 finished with value: 0.9391007134753707 and parameters: {'min_child_samples': 10}. Best is trial 64 with value: 0.9391007134753707.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.943104:  40%|####      | 2/5 [00:08<00:12,  4.21s/it][100]\tvalid_0's auc: 0.939101\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[50]\tvalid_0's auc: 0.940203\n",
      "min_data_in_leaf, val_score: 0.944287:  60%|######    | 3/5 [00:12<00:08,  4.30s/it]\u001b[32m[I 2021-03-10 05:10:43,837]\u001b[0m Trial 65 finished with value: 0.9442871194338621 and parameters: {'min_child_samples': 5}. Best is trial 65 with value: 0.9442871194338621.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.944287:  60%|######    | 3/5 [00:12<00:08,  4.30s/it][100]\tvalid_0's auc: 0.944287\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tvalid_0's auc: 0.934103\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's auc: 0.938221\n",
      "min_data_in_leaf, val_score: 0.944287:  80%|########  | 4/5 [00:16<00:04,  4.15s/it]\u001b[32m[I 2021-03-10 05:10:47,756]\u001b[0m Trial 66 finished with value: 0.938220837891084 and parameters: {'min_child_samples': 100}. Best is trial 65 with value: 0.9442871194338621.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.944287:  80%|########  | 4/5 [00:16<00:04,  4.15s/it][LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tvalid_0's auc: 0.93648\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "min_data_in_leaf, val_score: 0.944287: 100%|##########| 5/5 [00:20<00:00,  3.98s/it]\u001b[32m[I 2021-03-10 05:10:51,424]\u001b[0m Trial 67 finished with value: 0.9381773156033204 and parameters: {'min_child_samples': 50}. Best is trial 65 with value: 0.9442871194338621.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.944287: 100%|##########| 5/5 [00:20<00:00,  4.06s/it][100]\tvalid_0's auc: 0.938177\n",
      "Wall time: 3min 23s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from optuna.integration import lightgbm as lgb\n",
    "\n",
    "n=0\n",
    "train_idx=(fold[\"folds\"]!=n)\n",
    "val_idx=(fold[\"folds\"]==n)\n",
    "\n",
    "def optimize_params():\n",
    "    X_train=X[train_idx]\n",
    "    X_val=X[val_idx]\n",
    "    y_train=train[\"CVC Present\"][train_idx]\n",
    "    y_val=train[\"CVC Present\"][val_idx]\n",
    "\n",
    "    lgb_train=lgb.Dataset(X_train,label=y_train)\n",
    "    lgb_test=lgb.Dataset(X_val,label=y_val,reference=lgb_train)\n",
    "    \n",
    "    params={\n",
    "        \"task\":\"train\",\n",
    "        \"boosting_type\":\"gbdt\",\n",
    "        \"objective\":\"binary\",\n",
    "        \"metric\":\"auc\",\n",
    "        \"learning_rate\":0.1,\n",
    "        \"num_iterations\":100\n",
    "        # \"early_stopping_rounds\":200, #early_stopping_roundsを指定しないとbest_iterationは保存されない\n",
    "    }\n",
    "\n",
    "    opt=lgb.train(params,lgb_train,valid_sets=lgb_test, verbose_eval=50)\n",
    "    pickle.dump(opt.params,open(f\"{CFG.models_dir}lgbm_cvc/params_{n}.pickle\",\"wb\"))\n",
    "\n",
    "optimize_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "fold - 0\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\tvalid_0's auc: 0.92831\n",
      "[20]\tvalid_0's auc: 0.933375\n",
      "[30]\tvalid_0's auc: 0.936976\n",
      "[40]\tvalid_0's auc: 0.939663\n",
      "[50]\tvalid_0's auc: 0.940203\n",
      "[60]\tvalid_0's auc: 0.941834\n",
      "[70]\tvalid_0's auc: 0.943371\n",
      "[80]\tvalid_0's auc: 0.943742\n",
      "[90]\tvalid_0's auc: 0.943961\n",
      "[100]\tvalid_0's auc: 0.944287\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[92]\tvalid_0's auc: 0.945379\n",
      "\n",
      "fold - 1\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18921\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975091 -> initscore=3.667296\n",
      "[LightGBM] [Info] Start training from score 3.667296\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\tvalid_0's auc: 0.909731\n",
      "[20]\tvalid_0's auc: 0.916847\n",
      "[30]\tvalid_0's auc: 0.920761\n",
      "[40]\tvalid_0's auc: 0.924773\n",
      "[50]\tvalid_0's auc: 0.928152\n",
      "[60]\tvalid_0's auc: 0.930657\n",
      "[70]\tvalid_0's auc: 0.929939\n",
      "[80]\tvalid_0's auc: 0.930229\n",
      "[90]\tvalid_0's auc: 0.932026\n",
      "[100]\tvalid_0's auc: 0.931831\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[90]\tvalid_0's auc: 0.932026\n",
      "\n",
      "fold - 2\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 21999, number of negative: 563\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18917\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975047 -> initscore=3.665473\n",
      "[LightGBM] [Info] Start training from score 3.665473\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\tvalid_0's auc: 0.901897\n",
      "[20]\tvalid_0's auc: 0.913839\n",
      "[30]\tvalid_0's auc: 0.921857\n",
      "[40]\tvalid_0's auc: 0.924507\n",
      "[50]\tvalid_0's auc: 0.927311\n",
      "[60]\tvalid_0's auc: 0.932564\n",
      "[70]\tvalid_0's auc: 0.934157\n",
      "[80]\tvalid_0's auc: 0.934971\n",
      "[90]\tvalid_0's auc: 0.936168\n",
      "[100]\tvalid_0's auc: 0.936524\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's auc: 0.936827\n",
      "\n",
      "fold - 3\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\kamim\\anaconda3\\envs\\py38\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Info] Number of positive: 22000, number of negative: 563\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18922\n",
      "[LightGBM] [Info] Number of data points in the train set: 22563, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.975048 -> initscore=3.665518\n",
      "[LightGBM] [Info] Start training from score 3.665518\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\tvalid_0's auc: 0.906684\n",
      "[20]\tvalid_0's auc: 0.922208\n",
      "[30]\tvalid_0's auc: 0.923249\n",
      "[40]\tvalid_0's auc: 0.928945\n",
      "[50]\tvalid_0's auc: 0.930343\n",
      "[60]\tvalid_0's auc: 0.934657\n",
      "[70]\tvalid_0's auc: 0.935045\n",
      "[80]\tvalid_0's auc: 0.935754\n",
      "[90]\tvalid_0's auc: 0.937765\n",
      "[100]\tvalid_0's auc: 0.937724\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's auc: 0.937909\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "       CVC Present\nfold_0    0.945379\nfold_1    0.932026\nfold_2    0.936827\nfold_3    0.937909",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CVC Present</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>fold_0</th>\n      <td>0.945379</td>\n    </tr>\n    <tr>\n      <th>fold_1</th>\n      <td>0.932026</td>\n    </tr>\n    <tr>\n      <th>fold_2</th>\n      <td>0.936827</td>\n    </tr>\n    <tr>\n      <th>fold_3</th>\n      <td>0.937909</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "CVC Present    0.938035\ndtype: float64"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "\n",
    "def get_pred(train,val):\n",
    "\n",
    "    X_train,y_train=train\n",
    "    X_val,y_val=val\n",
    "\n",
    "    lgb_train=lightgbm.Dataset(X_train,label=y_train)\n",
    "    lgb_test=lightgbm.Dataset(X_val,label=y_val,reference=lgb_train)\n",
    "\n",
    "    params=pickle.load(open(f\"{CFG.models_dir}lgbm_cvc_presence/params_0.pickle\",\"rb\"))\n",
    "    params[\"early_stopping_rounds\"]=50\n",
    "\n",
    "    lgbm_model=lightgbm.train(params,lgb_train,valid_sets=lgb_test,verbose_eval=10)\n",
    "    pred=lgbm_model.predict(X_val)\n",
    "    auc=roc_auc_score(y_val,pred)\n",
    "\n",
    "    return pred,auc\n",
    "\n",
    "\n",
    "results=pd.DataFrame(columns=[\"CVC Present\"])\n",
    "\n",
    "for n in range(CFG.n_folds):\n",
    "    print(f\"\\nfold - {n}\")\n",
    "    train_idx=(fold[\"folds\"]!=n)\n",
    "    val_idx=(fold[\"folds\"]==n)\n",
    "\n",
    "    train_data=X[train_idx],train[\"CVC Present\"][train_idx]\n",
    "    val_data=X[val_idx],train[\"CVC Present\"][val_idx]\n",
    "\n",
    "    _,auc=get_pred(train=train_data,val=val_data)\n",
    "        \n",
    "    results.loc[f\"fold_{n}\",\"CVC Present\"]=auc\n",
    "\n",
    "\n",
    "display(results,results.mean(axis=0))"
   ]
  }
 ]
}