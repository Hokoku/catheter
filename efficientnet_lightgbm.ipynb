{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from optuna.integration import lightgbm as lgb\n",
    "import sys\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "## CFG"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_input_dir=\"../input/efficientnet_output_straight/\"\n",
    "dataset_dir=\"../input/ranzcr-clip-catheter-line-classification/\"\n",
    "models_dir=\"./models/\"\n",
    "\n",
    "n_folds=4\n",
    "target_cols=['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal', 'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', 'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal', 'Swan Ganz Catheter Present']\n"
   ]
  },
  {
   "source": [
    "## PCAを行う"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 30083/30083 [00:28<00:00, 1062.18it/s]\n",
      " (2560,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_list=[]\n",
    "uid_list=[]\n",
    "\n",
    "for file_name in tqdm(os.listdir(numpy_input_dir)):\n",
    "    features=np.load(numpy_input_dir+file_name)[0]\n",
    "    array_list.append(features)\n",
    "    uid_list.append(os.path.splitext(file_name)[0])\n",
    "\n",
    "print(\"\\n\",features.shape)"
   ]
  },
  {
   "source": [
    "np.array()を介してDataFrameにすることで高速になる"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           0         1         2         3         4         5         6     \\\n",
       "0      0.396515 -0.054081 -0.015864 -0.106942 -0.060530  0.121100 -0.227624   \n",
       "1      0.164006  0.140693  0.032105 -0.176430 -0.083020  0.047258 -0.017787   \n",
       "2      0.002677  0.376625 -0.225340 -0.070885 -0.051425  0.377899 -0.063307   \n",
       "3     -0.060532  0.177028  0.072606 -0.004313 -0.055582  0.114975  0.007457   \n",
       "4      0.183061  0.271571  0.215534 -0.051617 -0.083200  0.140492 -0.045176   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "30078 -0.057766  0.534434  0.078688  0.108312  0.194719  0.133313  0.076421   \n",
       "30079 -0.116204 -0.021042  0.059986 -0.098724 -0.132803  0.080919 -0.158249   \n",
       "30080 -0.064648  0.428484 -0.025759 -0.100176 -0.231685  0.123197  0.082175   \n",
       "30081  0.335530  0.154176  0.348478  0.058384 -0.039007  0.251613 -0.002622   \n",
       "30082 -0.084583  0.023031  0.242420 -0.015395 -0.158491 -0.055243 -0.240911   \n",
       "\n",
       "           7         8         9     ...      2550      2551      2552  \\\n",
       "0      0.119775 -0.090709 -0.079812  ...  0.168812 -0.003200  0.125735   \n",
       "1     -0.090380  0.048646  0.035545  ...  0.261479 -0.145750 -0.110239   \n",
       "2     -0.041903 -0.083185 -0.138833  ...  0.111567 -0.160592  0.377506   \n",
       "3      0.102090  0.064961  0.071155  ...  0.569302 -0.009915  0.210871   \n",
       "4      0.281804  0.024921  0.037777  ...  0.553023 -0.149797  0.425991   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "30078  0.149076  0.127926 -0.080206  ...  0.381842 -0.090594  0.357778   \n",
       "30079  0.273661  0.007932 -0.158452  ...  0.152788 -0.164298  0.494543   \n",
       "30080  0.046571  0.167706 -0.004723  ...  0.221434 -0.137481  0.269945   \n",
       "30081  0.030806  0.233821  0.127677  ...  0.550035  0.038085  0.252633   \n",
       "30082  0.046635 -0.062602  0.066049  ...  0.242505 -0.098591  0.534921   \n",
       "\n",
       "           2553      2554      2555      2556      2557      2558      2559  \n",
       "0     -0.217831 -0.186626  0.168541 -0.043564 -0.115363 -0.184646  0.245675  \n",
       "1     -0.054668 -0.216384 -0.007575  0.334953 -0.193806  0.160989 -0.130775  \n",
       "2      0.044940 -0.054765 -0.200121  0.141178 -0.182268 -0.091767  0.397546  \n",
       "3     -0.088671 -0.218210 -0.147902  0.417571 -0.120849  0.276294  0.028954  \n",
       "4     -0.147488 -0.218458  0.299213  0.466869 -0.127596  0.423303  0.073130  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "30078 -0.138506 -0.131539  0.060329  0.879204  0.100835  0.299556  0.220035  \n",
       "30079 -0.111861 -0.191273  0.219035  0.041929 -0.065143  0.029123  0.231337  \n",
       "30080  0.005036 -0.252102  0.196983 -0.048806 -0.153831  0.412776  0.585521  \n",
       "30081 -0.123222 -0.154719  0.016147  0.176843 -0.006309  0.308135  0.190591  \n",
       "30082 -0.207189 -0.182932  0.170409  0.076778 -0.000521 -0.072177  0.325911  \n",
       "\n",
       "[30083 rows x 2560 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>2550</th>\n      <th>2551</th>\n      <th>2552</th>\n      <th>2553</th>\n      <th>2554</th>\n      <th>2555</th>\n      <th>2556</th>\n      <th>2557</th>\n      <th>2558</th>\n      <th>2559</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.396515</td>\n      <td>-0.054081</td>\n      <td>-0.015864</td>\n      <td>-0.106942</td>\n      <td>-0.060530</td>\n      <td>0.121100</td>\n      <td>-0.227624</td>\n      <td>0.119775</td>\n      <td>-0.090709</td>\n      <td>-0.079812</td>\n      <td>...</td>\n      <td>0.168812</td>\n      <td>-0.003200</td>\n      <td>0.125735</td>\n      <td>-0.217831</td>\n      <td>-0.186626</td>\n      <td>0.168541</td>\n      <td>-0.043564</td>\n      <td>-0.115363</td>\n      <td>-0.184646</td>\n      <td>0.245675</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.164006</td>\n      <td>0.140693</td>\n      <td>0.032105</td>\n      <td>-0.176430</td>\n      <td>-0.083020</td>\n      <td>0.047258</td>\n      <td>-0.017787</td>\n      <td>-0.090380</td>\n      <td>0.048646</td>\n      <td>0.035545</td>\n      <td>...</td>\n      <td>0.261479</td>\n      <td>-0.145750</td>\n      <td>-0.110239</td>\n      <td>-0.054668</td>\n      <td>-0.216384</td>\n      <td>-0.007575</td>\n      <td>0.334953</td>\n      <td>-0.193806</td>\n      <td>0.160989</td>\n      <td>-0.130775</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.002677</td>\n      <td>0.376625</td>\n      <td>-0.225340</td>\n      <td>-0.070885</td>\n      <td>-0.051425</td>\n      <td>0.377899</td>\n      <td>-0.063307</td>\n      <td>-0.041903</td>\n      <td>-0.083185</td>\n      <td>-0.138833</td>\n      <td>...</td>\n      <td>0.111567</td>\n      <td>-0.160592</td>\n      <td>0.377506</td>\n      <td>0.044940</td>\n      <td>-0.054765</td>\n      <td>-0.200121</td>\n      <td>0.141178</td>\n      <td>-0.182268</td>\n      <td>-0.091767</td>\n      <td>0.397546</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.060532</td>\n      <td>0.177028</td>\n      <td>0.072606</td>\n      <td>-0.004313</td>\n      <td>-0.055582</td>\n      <td>0.114975</td>\n      <td>0.007457</td>\n      <td>0.102090</td>\n      <td>0.064961</td>\n      <td>0.071155</td>\n      <td>...</td>\n      <td>0.569302</td>\n      <td>-0.009915</td>\n      <td>0.210871</td>\n      <td>-0.088671</td>\n      <td>-0.218210</td>\n      <td>-0.147902</td>\n      <td>0.417571</td>\n      <td>-0.120849</td>\n      <td>0.276294</td>\n      <td>0.028954</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.183061</td>\n      <td>0.271571</td>\n      <td>0.215534</td>\n      <td>-0.051617</td>\n      <td>-0.083200</td>\n      <td>0.140492</td>\n      <td>-0.045176</td>\n      <td>0.281804</td>\n      <td>0.024921</td>\n      <td>0.037777</td>\n      <td>...</td>\n      <td>0.553023</td>\n      <td>-0.149797</td>\n      <td>0.425991</td>\n      <td>-0.147488</td>\n      <td>-0.218458</td>\n      <td>0.299213</td>\n      <td>0.466869</td>\n      <td>-0.127596</td>\n      <td>0.423303</td>\n      <td>0.073130</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>-0.057766</td>\n      <td>0.534434</td>\n      <td>0.078688</td>\n      <td>0.108312</td>\n      <td>0.194719</td>\n      <td>0.133313</td>\n      <td>0.076421</td>\n      <td>0.149076</td>\n      <td>0.127926</td>\n      <td>-0.080206</td>\n      <td>...</td>\n      <td>0.381842</td>\n      <td>-0.090594</td>\n      <td>0.357778</td>\n      <td>-0.138506</td>\n      <td>-0.131539</td>\n      <td>0.060329</td>\n      <td>0.879204</td>\n      <td>0.100835</td>\n      <td>0.299556</td>\n      <td>0.220035</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>-0.116204</td>\n      <td>-0.021042</td>\n      <td>0.059986</td>\n      <td>-0.098724</td>\n      <td>-0.132803</td>\n      <td>0.080919</td>\n      <td>-0.158249</td>\n      <td>0.273661</td>\n      <td>0.007932</td>\n      <td>-0.158452</td>\n      <td>...</td>\n      <td>0.152788</td>\n      <td>-0.164298</td>\n      <td>0.494543</td>\n      <td>-0.111861</td>\n      <td>-0.191273</td>\n      <td>0.219035</td>\n      <td>0.041929</td>\n      <td>-0.065143</td>\n      <td>0.029123</td>\n      <td>0.231337</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>-0.064648</td>\n      <td>0.428484</td>\n      <td>-0.025759</td>\n      <td>-0.100176</td>\n      <td>-0.231685</td>\n      <td>0.123197</td>\n      <td>0.082175</td>\n      <td>0.046571</td>\n      <td>0.167706</td>\n      <td>-0.004723</td>\n      <td>...</td>\n      <td>0.221434</td>\n      <td>-0.137481</td>\n      <td>0.269945</td>\n      <td>0.005036</td>\n      <td>-0.252102</td>\n      <td>0.196983</td>\n      <td>-0.048806</td>\n      <td>-0.153831</td>\n      <td>0.412776</td>\n      <td>0.585521</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>0.335530</td>\n      <td>0.154176</td>\n      <td>0.348478</td>\n      <td>0.058384</td>\n      <td>-0.039007</td>\n      <td>0.251613</td>\n      <td>-0.002622</td>\n      <td>0.030806</td>\n      <td>0.233821</td>\n      <td>0.127677</td>\n      <td>...</td>\n      <td>0.550035</td>\n      <td>0.038085</td>\n      <td>0.252633</td>\n      <td>-0.123222</td>\n      <td>-0.154719</td>\n      <td>0.016147</td>\n      <td>0.176843</td>\n      <td>-0.006309</td>\n      <td>0.308135</td>\n      <td>0.190591</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>-0.084583</td>\n      <td>0.023031</td>\n      <td>0.242420</td>\n      <td>-0.015395</td>\n      <td>-0.158491</td>\n      <td>-0.055243</td>\n      <td>-0.240911</td>\n      <td>0.046635</td>\n      <td>-0.062602</td>\n      <td>0.066049</td>\n      <td>...</td>\n      <td>0.242505</td>\n      <td>-0.098591</td>\n      <td>0.534921</td>\n      <td>-0.207189</td>\n      <td>-0.182932</td>\n      <td>0.170409</td>\n      <td>0.076778</td>\n      <td>-0.000521</td>\n      <td>-0.072177</td>\n      <td>0.325911</td>\n    </tr>\n  </tbody>\n</table>\n<p>30083 rows × 2560 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df=pd.DataFrame(np.array(array_list))\n",
    "df_uid=pd.DataFrame(np.array(uid_list),columns=[\"StudyInstanceUID\"])\n",
    "df_features=pd.DataFrame(np.array(array_list))\n",
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 6.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_components=75\n",
    "pca=PCA(n_components=n_components)\n",
    "pca.fit(df_features)\n",
    "pickle.dump(pca,open(f\"{models_dir}pca_model_{n_components}.pickle\",\"wb\"))\n",
    "features_pca=pca.transform(df_features)\n",
    "pd.concat([df_uid,pd.DataFrame(features_pca)],axis=1).to_csv(f\"{models_dir}features_pca_{n_components}.csv\",index=False)"
   ]
  },
  {
   "source": [
    "累積寄与率を出力する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_ratio=pca.explained_variance_ratio_\n",
    "ev_ratio=np.hstack([0,ev_ratio.cumsum()])\n",
    "pd.DataFrame(ev_ratio).to_csv(\"df_pca.csv\")"
   ]
  },
  {
   "source": [
    "## PCAで得られた特徴量をLightGBMにかける"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### CV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(dataset_dir+\"train.csv\")\n",
    "\n",
    "group_kfold=GroupKFold(n_splits=n_folds)\n",
    "folds=train.copy()\n",
    "\n",
    "for n,(train_index,test_index) in enumerate(group_kfold.split(train,groups=train[\"PatientID\"].values)):\n",
    "    folds.loc[test_index,\"fold\"]=n\n",
    "\n",
    "folds[\"fold\"]=folds[\"fold\"].astype(int)"
   ]
  },
  {
   "source": [
    "### Optunaでハイパーパラメータを最適化する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_count=75\n",
    "features_pca=pd.read_csv(f\"{models_dir}features_pca_{pca_count}.csv\").sort_values(\"StudyInstanceUID\")\n",
    "\n",
    "dataset=pd.merge(folds,features_pca,on=\"StudyInstanceUID\")\n",
    "\n",
    "fold=0\n",
    "\n",
    "dataset_train=dataset[dataset[\"fold\"]!=fold]\n",
    "dataset_test=dataset[dataset[\"fold\"]==fold]\n",
    "X_train=dataset_train.iloc[:,-pca_count:]\n",
    "X_test=dataset_test.iloc[:,-pca_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "verhead of testing was 0.012525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.831762\n",
      "regularization_factors, val_score: 0.852567:  25%|##5       | 5/20 [00:05<00:16,  1.08s/it]\u001b[32m[I 2021-02-19 19:19:34,268]\u001b[0m Trial 47 finished with value: 0.8323948356519978 and parameters: {'lambda_l1': 0.0009363080505649153, 'lambda_l2': 4.224612968862878}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  25%|##5       | 5/20 [00:05<00:16,  1.08s/it][100]\tvalid_0's auc: 0.832395\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.828758\n",
      "regularization_factors, val_score: 0.852567:  30%|###       | 6/20 [00:06<00:15,  1.09s/it]\u001b[32m[I 2021-02-19 19:19:35,372]\u001b[0m Trial 48 finished with value: 0.8320244225324244 and parameters: {'lambda_l1': 0.6346972309750085, 'lambda_l2': 0.17873608420749684}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  30%|###       | 6/20 [00:06<00:15,  1.09s/it][100]\tvalid_0's auc: 0.832024\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.831271\n",
      "regularization_factors, val_score: 0.852567:  35%|###5      | 7/20 [00:07<00:14,  1.09s/it]\u001b[32m[I 2021-02-19 19:19:36,449]\u001b[0m Trial 49 finished with value: 0.8404326236791151 and parameters: {'lambda_l1': 0.5317357523688145, 'lambda_l2': 0.005078681210679284}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  35%|###5      | 7/20 [00:07<00:14,  1.09s/it][100]\tvalid_0's auc: 0.840433\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.838812\n",
      "regularization_factors, val_score: 0.852567:  40%|####      | 8/20 [00:08<00:12,  1.08s/it]\u001b[32m[I 2021-02-19 19:19:37,509]\u001b[0m Trial 50 finished with value: 0.8465376678931326 and parameters: {'lambda_l1': 0.4821575598926928, 'lambda_l2': 8.528427187553258e-08}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  40%|####      | 8/20 [00:08<00:12,  1.08s/it][100]\tvalid_0's auc: 0.846538\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.841272\n",
      "regularization_factors, val_score: 0.852567:  45%|####5     | 9/20 [00:09<00:12,  1.09s/it]\u001b[32m[I 2021-02-19 19:19:38,637]\u001b[0m Trial 51 finished with value: 0.852566156136491 and parameters: {'lambda_l1': 2.7502133307166582e-08, 'lambda_l2': 4.067000273983743e-05}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  45%|####5     | 9/20 [00:09<00:12,  1.09s/it][100]\tvalid_0's auc: 0.852566\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.833373\n",
      "regularization_factors, val_score: 0.852567:  50%|#####     | 10/20 [00:10<00:10,  1.09s/it]\u001b[32m[I 2021-02-19 19:19:39,705]\u001b[0m Trial 52 finished with value: 0.8402918784715189 and parameters: {'lambda_l1': 7.519684936515654e-05, 'lambda_l2': 0.0018254736478579138}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  50%|#####     | 10/20 [00:10<00:10,  1.09s/it][100]\tvalid_0's auc: 0.840292\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.841272\n",
      "regularization_factors, val_score: 0.852567:  55%|#####5    | 11/20 [00:11<00:09,  1.09s/it]\u001b[32m[I 2021-02-19 19:19:40,804]\u001b[0m Trial 53 finished with value: 0.852566745028573 and parameters: {'lambda_l1': 1.1671345244834217e-08, 'lambda_l2': 1.4946797484116324e-08}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  55%|#####5    | 11/20 [00:11<00:09,  1.09s/it][100]\tvalid_0's auc: 0.852567\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007591 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.841272\n",
      "regularization_factors, val_score: 0.852567:  60%|######    | 12/20 [00:12<00:08,  1.08s/it]\u001b[32m[I 2021-02-19 19:19:41,861]\u001b[0m Trial 54 finished with value: 0.852566745028573 and parameters: {'lambda_l1': 1.1843596005038397e-08, 'lambda_l2': 1.1732590968433855e-08}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  60%|######    | 12/20 [00:13<00:08,  1.08s/it][100]\tvalid_0's auc: 0.852567\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.841272\n",
      "regularization_factors, val_score: 0.852567:  65%|######5   | 13/20 [00:14<00:07,  1.08s/it]\u001b[32m[I 2021-02-19 19:19:42,952]\u001b[0m Trial 55 finished with value: 0.852566745028573 and parameters: {'lambda_l1': 5.564417454465777e-07, 'lambda_l2': 1.5654033926618795e-07}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  65%|######5   | 13/20 [00:14<00:07,  1.08s/it][100]\tvalid_0's auc: 0.852567\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.841272\n",
      "regularization_factors, val_score: 0.852567:  70%|#######   | 14/20 [00:15<00:06,  1.08s/it]\u001b[32m[I 2021-02-19 19:19:44,029]\u001b[0m Trial 56 finished with value: 0.852566745028573 and parameters: {'lambda_l1': 3.165949571656901e-07, 'lambda_l2': 1.7907675897532692e-07}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  70%|#######   | 14/20 [00:15<00:06,  1.08s/it][100]\tvalid_0's auc: 0.852567\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.841272\n",
      "regularization_factors, val_score: 0.852567:  75%|#######5  | 15/20 [00:16<00:05,  1.10s/it]\u001b[32m[I 2021-02-19 19:19:45,171]\u001b[0m Trial 57 finished with value: 0.852566156136491 and parameters: {'lambda_l1': 6.038883032149327e-07, 'lambda_l2': 3.6924764891671124e-06}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  75%|#######5  | 15/20 [00:16<00:05,  1.10s/it][100]\tvalid_0's auc: 0.852566\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.841272\n",
      "regularization_factors, val_score: 0.852567:  80%|########  | 16/20 [00:17<00:04,  1.09s/it]\u001b[32m[I 2021-02-19 19:19:46,237]\u001b[0m Trial 58 finished with value: 0.852566745028573 and parameters: {'lambda_l1': 1.0649620124460387e-08, 'lambda_l2': 8.96476054189892e-07}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  80%|########  | 16/20 [00:17<00:04,  1.09s/it][100]\tvalid_0's auc: 0.852567\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.841272\n",
      "regularization_factors, val_score: 0.852567:  85%|########5 | 17/20 [00:18<00:03,  1.08s/it]\u001b[32m[I 2021-02-19 19:19:47,304]\u001b[0m Trial 59 finished with value: 0.852566745028573 and parameters: {'lambda_l1': 4.2895058003168285e-06, 'lambda_l2': 1.0019790920138426e-08}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  85%|########5 | 17/20 [00:18<00:03,  1.08s/it][100]\tvalid_0's auc: 0.852567\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.841272\n",
      "regularization_factors, val_score: 0.852567:  90%|######### | 18/20 [00:19<00:02,  1.08s/it]\u001b[32m[I 2021-02-19 19:19:48,369]\u001b[0m Trial 60 finished with value: 0.852566745028573 and parameters: {'lambda_l1': 7.335011128735257e-06, 'lambda_l2': 1.6419475136634328e-08}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  90%|######### | 18/20 [00:19<00:02,  1.08s/it][100]\tvalid_0's auc: 0.852567\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.845785\n",
      "regularization_factors, val_score: 0.852567:  95%|#########5| 19/20 [00:20<00:01,  1.08s/it]\u001b[32m[I 2021-02-19 19:19:49,468]\u001b[0m Trial 61 finished with value: 0.8524360109863707 and parameters: {'lambda_l1': 0.0064554860655093626, 'lambda_l2': 1.1613618038374575e-06}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567:  95%|#########5| 19/20 [00:20<00:01,  1.08s/it][100]\tvalid_0's auc: 0.852436\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.84337\n",
      "regularization_factors, val_score: 0.852567: 100%|##########| 20/20 [00:21<00:00,  1.16s/it]\u001b[32m[I 2021-02-19 19:19:50,792]\u001b[0m Trial 62 finished with value: 0.8524501443963385 and parameters: {'lambda_l1': 3.4190249227081773e-06, 'lambda_l2': 0.0001466568892430899}. Best is trial 45 with value: 0.852566745028573.\u001b[0m\n",
      "regularization_factors, val_score: 0.852567: 100%|##########| 20/20 [00:21<00:00,  1.10s/it]\n",
      "min_data_in_leaf, val_score: 0.852567:   0%|          | 0/5 [00:00<?, ?it/s][100]\tvalid_0's auc: 0.85245\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.819462\n",
      "min_data_in_leaf, val_score: 0.852567:  20%|##        | 1/5 [00:00<00:03,  1.09it/s]\u001b[32m[I 2021-02-19 19:19:51,730]\u001b[0m Trial 63 finished with value: 0.8347545262245422 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.8347545262245422.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.852567:  20%|##        | 1/5 [00:00<00:03,  1.09it/s][100]\tvalid_0's auc: 0.834755\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009015 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.826984\n",
      "min_data_in_leaf, val_score: 0.852567:  40%|####      | 2/5 [00:01<00:02,  1.11it/s]\u001b[32m[I 2021-02-19 19:19:52,621]\u001b[0m Trial 64 finished with value: 0.8311923180205688 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.8347545262245422.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.852567:  40%|####      | 2/5 [00:01<00:02,  1.11it/s][100]\tvalid_0's auc: 0.831192\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.837324\n",
      "min_data_in_leaf, val_score: 0.852567:  60%|######    | 3/5 [00:02<00:01,  1.10it/s]\u001b[32m[I 2021-02-19 19:19:53,532]\u001b[0m Trial 65 finished with value: 0.8482036435930898 and parameters: {'min_child_samples': 10}. Best is trial 65 with value: 0.8482036435930898.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.852567:  60%|######    | 3/5 [00:02<00:01,  1.10it/s][100]\tvalid_0's auc: 0.848204\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011921 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.836887\n",
      "min_data_in_leaf, val_score: 0.852567:  80%|########  | 4/5 [00:03<00:00,  1.11it/s]\u001b[32m[I 2021-02-19 19:19:54,417]\u001b[0m Trial 66 finished with value: 0.8460912876949822 and parameters: {'min_child_samples': 25}. Best is trial 65 with value: 0.8482036435930898.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.852567:  80%|########  | 4/5 [00:03<00:00,  1.11it/s][100]\tvalid_0's auc: 0.846091\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 21965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19125\n",
      "[LightGBM] [Info] Number of data points in the train set: 22562, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026460 -> initscore=-3.605288\n",
      "[LightGBM] [Info] Start training from score -3.605288\n",
      "[50]\tvalid_0's auc: 0.83531\n",
      "min_data_in_leaf, val_score: 0.852567: 100%|##########| 5/5 [00:04<00:00,  1.13it/s]\u001b[32m[I 2021-02-19 19:19:55,288]\u001b[0m Trial 67 finished with value: 0.8415008739158497 and parameters: {'min_child_samples': 100}. Best is trial 65 with value: 0.8482036435930898.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.852567: 100%|##########| 5/5 [00:04<00:00,  1.11it/s][100]\tvalid_0's auc: 0.841501\n",
      "Wall time: 16min 27s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def optimize_params():\n",
    "    for i in range(1,12):\n",
    "        y_train=dataset_train.iloc[:,i]\n",
    "        y_test=dataset_test.iloc[:,i]\n",
    "\n",
    "        lgb_train=lgb.Dataset(X_train,label=y_train)\n",
    "        lgb_test=lgb.Dataset(X_test,label=y_test,reference=lgb_train)\n",
    "        \n",
    "        params={\n",
    "            \"task\":\"train\",\n",
    "            \"boosting_type\":\"gbdt\",\n",
    "            \"objective\":\"binary\",\n",
    "            \"metric\":\"auc\",\n",
    "            # \"early_stopping_rounds\":200, #early_stopping_roundsを指定しないとbest_iterationは保存されない\n",
    "        }\n",
    "\n",
    "        opt=lgb.train(params,lgb_train,valid_sets=lgb_test,num_boost_round=100,verbose_eval=50)\n",
    "        pickle.dump(opt.params,open(f\"{models_dir}pca_{pca_count}/groupkfold/lgb_params_{i}.pickle\",\"wb\"))\n",
    "\n",
    "optimize_params()"
   ]
  },
  {
   "source": [
    "### PCA - 200\n",
    "最適化に55min 54s\n",
    "\n",
    "### pca - 100\n",
    "最適化に26min 18s  \n",
    "31min 51s\n",
    "\n",
    "### pca - 75\n",
    "最適化に20min 2s  \n",
    "23min 9s\n",
    "16min 27s\n",
    "\n",
    "### pca - 50\n",
    "最適化に13min 58s\n",
    "\n",
    "### pca - 5\n",
    "最適化に3min 54s"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 得られたパラメータを用いて予測する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "           ETT - Abnormal ETT - Borderline ETT - Normal NGT - Abnormal  \\\npca_75 - 0       0.808277         0.806389     0.874581       0.744426   \npca_75 - 1       0.672400         0.835630     0.858072       0.666539   \npca_75 - 2       0.733085         0.814242     0.872941       0.747333   \npca_75 - 3       0.618248         0.809592     0.885922       0.701075   \n\n           NGT - Borderline NGT - Incompletely Imaged NGT - Normal  \\\npca_75 - 0         0.699056                  0.824968     0.858151   \npca_75 - 1         0.693132                  0.824185     0.866158   \npca_75 - 2         0.723470                  0.845974     0.842066   \npca_75 - 3         0.732380                  0.842144     0.857316   \n\n           CVC - Abnormal CVC - Borderline CVC - Normal  \\\npca_75 - 0       0.596288         0.586559     0.553155   \npca_75 - 1       0.573148         0.562599     0.546986   \npca_75 - 2       0.571967         0.580694     0.541147   \npca_75 - 3       0.572106         0.583416     0.546906   \n\n           Swan Ganz Catheter Present  \npca_75 - 0                   0.852567  \npca_75 - 1                   0.839570  \npca_75 - 2                   0.832271  \npca_75 - 3                   0.838534  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ETT - Abnormal</th>\n      <th>ETT - Borderline</th>\n      <th>ETT - Normal</th>\n      <th>NGT - Abnormal</th>\n      <th>NGT - Borderline</th>\n      <th>NGT - Incompletely Imaged</th>\n      <th>NGT - Normal</th>\n      <th>CVC - Abnormal</th>\n      <th>CVC - Borderline</th>\n      <th>CVC - Normal</th>\n      <th>Swan Ganz Catheter Present</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>pca_75 - 0</th>\n      <td>0.808277</td>\n      <td>0.806389</td>\n      <td>0.874581</td>\n      <td>0.744426</td>\n      <td>0.699056</td>\n      <td>0.824968</td>\n      <td>0.858151</td>\n      <td>0.596288</td>\n      <td>0.586559</td>\n      <td>0.553155</td>\n      <td>0.852567</td>\n    </tr>\n    <tr>\n      <th>pca_75 - 1</th>\n      <td>0.672400</td>\n      <td>0.835630</td>\n      <td>0.858072</td>\n      <td>0.666539</td>\n      <td>0.693132</td>\n      <td>0.824185</td>\n      <td>0.866158</td>\n      <td>0.573148</td>\n      <td>0.562599</td>\n      <td>0.546986</td>\n      <td>0.839570</td>\n    </tr>\n    <tr>\n      <th>pca_75 - 2</th>\n      <td>0.733085</td>\n      <td>0.814242</td>\n      <td>0.872941</td>\n      <td>0.747333</td>\n      <td>0.723470</td>\n      <td>0.845974</td>\n      <td>0.842066</td>\n      <td>0.571967</td>\n      <td>0.580694</td>\n      <td>0.541147</td>\n      <td>0.832271</td>\n    </tr>\n    <tr>\n      <th>pca_75 - 3</th>\n      <td>0.618248</td>\n      <td>0.809592</td>\n      <td>0.885922</td>\n      <td>0.701075</td>\n      <td>0.732380</td>\n      <td>0.842144</td>\n      <td>0.857316</td>\n      <td>0.572106</td>\n      <td>0.583416</td>\n      <td>0.546906</td>\n      <td>0.838534</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "pca_75 - 0    0.745856\npca_75 - 1    0.721675\npca_75 - 2    0.736836\npca_75 - 3    0.726149\ndtype: float64"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0.7326286997407299"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "def get_pred(train,test,col_name:str,pca_count:int):\n",
    "    X_train=train.iloc[:,-pca_count:]\n",
    "    X_test=test.iloc[:,-pca_count:]\n",
    "    y_train=train[col_name]\n",
    "    y_test=test[col_name]\n",
    "\n",
    "    col_index=test.columns.get_loc(col_name)\n",
    "\n",
    "    lgb_train=lgb.Dataset(X_train,label=y_train)\n",
    "    lgb_test=lgb.Dataset(X_test,label=y_test,reference=lgb_train)\n",
    "\n",
    "    params=pickle.load(open(f\"{models_dir}pca_{pca_count}/groupkfold/lgb_params_{col_index}.pickle\",\"rb\"))\n",
    "    params[\"early_stopping_rounds\"]=100\n",
    "    params[\"verbose\"]=-1\n",
    "\n",
    "    model=lightgbm.train(params,lgb_train,valid_sets=lgb_test,verbose_eval=200)\n",
    "    pred=model.predict(X_test)\n",
    "    auc=roc_auc_score(y_test,pred)\n",
    "\n",
    "    return pred,auc\n",
    "\n",
    "\n",
    "results=pd.DataFrame(columns=target_cols)\n",
    "\n",
    "features_pca=pd.read_csv(f\"{models_dir}features_pca_{pca_count}.csv\")\n",
    "dataset=pd.merge(folds,features_pca,on=\"StudyInstanceUID\")\n",
    "\n",
    "\n",
    "for n in range(n_folds):\n",
    "    train_n=dataset[dataset[\"fold\"]!=n]\n",
    "    test_n=dataset[dataset[\"fold\"]==n]\n",
    "\n",
    "\n",
    "    for col_name in target_cols:\n",
    "        pred,auc=get_pred(train=train_n,test=test_n,col_name=col_name,pca_count=pca_count)\n",
    "        \n",
    "        results.loc[f\"pca_{pca_count} - {n}\",col_name]=auc\n",
    "\n",
    "\n",
    "display(results)\n",
    "display(results.mean(axis=1),results.mean(axis=1).mean())"
   ]
  },
  {
   "source": [
    "## モデルを保存する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's auc: 0.808758\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\tvalid_0's auc: 0.807283\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[89]\tvalid_0's auc: 0.874637\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\tvalid_0's auc: 0.746204\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.699056\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's auc: 0.825209\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.858151\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's auc: 0.597386\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.586559\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[83]\tvalid_0's auc: 0.555288\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[90]\tvalid_0's auc: 0.853667\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "\n",
    "for col_name in target_cols:\n",
    "    y_train=dataset_train.loc[:,col_name]\n",
    "    y_test=dataset_test.loc[:,col_name]\n",
    "\n",
    "    col_index=dataset_train.columns.get_loc(col_name)\n",
    "\n",
    "    lgb_train=lgb.Dataset(X_train,label=y_train)\n",
    "    lgb_test=lgb.Dataset(X_test,label=y_test,reference=lgb_train)\n",
    "    \n",
    "    params=pickle.load(open(f\"{models_dir}pca_{pca_count}/groupkfold/lgb_params_{col_index}.pickle\",\"rb\"))    \n",
    "    params[\"early_stopping_rounds\"]=100\n",
    "    params[\"verbose\"]=-1\n",
    "\n",
    "    model=lightgbm.train(params,lgb_train,valid_sets=lgb_test,verbose_eval=200)\n",
    "    pickle.dump(model,open(f\"{models_dir}pca_{pca_count}/groupkfold/lgb_model_{col_index}.pickle\",\"wb\"))"
   ]
  }
 ]
}