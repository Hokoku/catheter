{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3da45907e5cd41bdac7b692b424c19725633d98a853f93b11b400b1d8951ac30"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.io import FixedLenFeature\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug=True\n",
    "    dataset_dir=\"../input/ranzcr-clip-catheter-line-classification/\"\n",
    "    batch_size=4 if debug else 256\n",
    "    n_epochs=2 if debug else 20\n",
    "    n_folds=10\n",
    "    target_cols=['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal', 'NGT - Abnormal', 'NGT - Borderline',\n",
    "       'NGT - Incompletely Imaged', 'NGT - Normal', 'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal', 'Swan Ganz Catheter Present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(f\"{CFG.dataset_dir}train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description={\n",
    "    \"CVC - Abnormal\":FixedLenFeature([],tf.int64),\n",
    "    \"CVC - Borderline\":FixedLenFeature([],tf.int64),\n",
    "    \"CVC - Normal\":FixedLenFeature([],tf.int64),\n",
    "    \"ETT - Abnormal\":FixedLenFeature([],tf.int64),\n",
    "    \"ETT - Borderline\":FixedLenFeature([],tf.int64),\n",
    "    \"ETT - Normal\":FixedLenFeature([],tf.int64),\n",
    "    \"NGT - Abnormal\":FixedLenFeature([],tf.int64),\n",
    "    \"NGT - Borderline\":FixedLenFeature([],tf.int64),\n",
    "    \"NGT - Incompletely Imaged\":FixedLenFeature([],tf.int64),\n",
    "    \"NGT - Normal\":FixedLenFeature([],tf.int64),\n",
    "    \"Swan Ganz Catheter Present\":FixedLenFeature([],tf.int64),\n",
    "    \"StudyInstanceUID\":FixedLenFeature([],tf.string),\n",
    "    \"image\":FixedLenFeature([],tf.string),\n",
    "}\n",
    "\n",
    "def parse_examples(example):\n",
    "    return tf.io.parse_example(example,feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method dset_split.train_filter of <__main__.dset_split object at 0x000002408893C040>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method dset_split.train_filter of <__main__.dset_split object at 0x000002408893C040>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method dset_split.val_filter of <__main__.dset_split object at 0x000002408893C040>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method dset_split.val_filter of <__main__.dset_split object at 0x000002408893C040>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function preprocessing at 0x00000240885DF940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function preprocessing at 0x00000240885DF940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "class dset_split():\n",
    "    def __init__(self,n_folds:int):\n",
    "        self.n_folds=n_folds\n",
    "        self.train=pd.read_csv(\"../input/ranzcr-clip-catheter-line-classification/train.csv\")\n",
    "    \n",
    "    def set_fold(self,fold:int):\n",
    "        train_idx,val_idx=list(GroupKFold(n_splits=self.n_folds).split(self.train,groups=self.train[\"PatientID\"]))[fold]\n",
    "        train_uid_array=self.train.iloc[train_idx,0].values\n",
    "        val_uid_array=self.train.iloc[val_idx,0].values\n",
    "        self.train_uid_tensor=tf.convert_to_tensor(train_uid_array,dtype=tf.string)\n",
    "        self.val_uid_tensor=tf.convert_to_tensor(val_uid_array,dtype=tf.string)\n",
    "    \n",
    "    def train_filter(self,parsed_record):\n",
    "        filtered=tf.math.equal(parsed_record[\"StudyInstanceUID\"],self.train_uid_tensor)\n",
    "        return tf.math.reduce_any(filtered)\n",
    "\n",
    "    def val_filter(self,parsed_record):\n",
    "        filtered=tf.math.equal(parsed_record[\"StudyInstanceUID\"],self.val_uid_tensor)\n",
    "        return tf.math.reduce_any(filtered)\n",
    "\n",
    "\n",
    "AUTO=tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def preprocessing(parsed_record):\n",
    "    image=tf.io.decode_png(parsed_record[\"image\"],channels=3) # decode_imageではresizeでエラーが出る\n",
    "    image=tf.cast(image,tf.float32)\n",
    "    image=tf.reshape(image,[299,299,3]) # without this, TPU will not run\n",
    "    image/=255.0 # normalization\n",
    "\n",
    "    label=[]\n",
    "    for col_name in CFG.target_cols:\n",
    "        label.append(parsed_record[col_name])\n",
    "    \n",
    "    return image,label\n",
    "\n",
    "def augment_(img,label):\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    return img,label\n",
    "\n",
    "def build_datasets(repeat=False):\n",
    "    record_name=\"../input/ranzcr_299x299.tfrec\" if CFG.debug else \"gs://km_kaggle_catheter/train_resized.tfrec\"\n",
    "    raw_dataset=tf.data.TFRecordDataset([record_name])\n",
    "    dset=raw_dataset.map(parse_examples,num_parallel_calls=AUTO)\n",
    "    dset=dset.cache() # apply time-consuming process before cache()\n",
    "    train_dset=dset.filter(splitter.train_filter)\n",
    "    val_dset=dset.filter(splitter.val_filter)\n",
    "    return train_dset,val_dset\n",
    "\n",
    "\n",
    "def dset_postprocessing(dset,repeat=False,augument=True,shuffle=1024):\n",
    "    dset=dset.map(preprocessing,num_parallel_calls=AUTO) # memory-consuming process should be placed after cache()\n",
    "    dset=dset.map(augment_,num_parallel_calls=AUTO) if augument else dset\n",
    "    dset=dset.repeat() if repeat else dset\n",
    "    dset=dset.shuffle(shuffle) if shuffle else dset\n",
    "    dset=dset.batch(CFG.batch_size).prefetch(AUTO) # Warning: putting preprocessing after batch() causes an error\n",
    "    return dset\n",
    "\n",
    "\n",
    "splitter=dset_split(n_folds=CFG.n_folds)\n",
    "splitter.set_fold(0)\n",
    "train_dset,val_dset=build_datasets(splitter)\n",
    "train_dset=dset_postprocessing(train_dset)\n",
    "val_dset=dset_postprocessing(val_dset,augment=False,shuffle=False,repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nxception (Functional)        (None, 2048)              20861480  \n_________________________________________________________________\ndense (Dense)                (None, 11)                22539     \n=================================================================\nTotal params: 20,884,019\nTrainable params: 20,829,491\nNon-trainable params: 54,528\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "xception=Xception(include_top=False,weights=\"imagenet\",input_shape=(299,299,3),pooling=\"avg\")\n",
    "xception_dense=keras.Sequential([\n",
    "    xception,\n",
    "    keras.layers.Dense(11,activation=\"sigmoid\")\n",
    "])\n",
    "xception_dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam=keras.optimizers.Adam(learning_rate=1e-3)\n",
    "xception_dense.compile(optimizer=adam,loss=\"binary_crossentropy\",metrics=[keras.metrics.AUC(multi_label=True,name=\"auc\")])\n",
    "\n",
    "save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "fit_callbacks=[\n",
    "    callbacks.EarlyStopping(monitor=\"val_auc\",patience=8,mode=\"max\"),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_auc\",patience=4,min_lr=1e-7,mode=\"max\",factor=0.5,verbose=1),\n",
    "    callbacks.ModelCheckpoint(\"ckpt\",monitor=\"val_auc\",mode=\"max\",save_best_only=True,options=save_locally)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = train_paths.shape[0] // BATCH_SIZE\n",
    "history = xception_dense.fit(\n",
    "    train_dataset, \n",
    "    epochs=CFG.n_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=fit_callbacks,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.save_model(xception_dense,\"last_ckpt\",options=save_locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(history.history)\n",
    "hist_df.to_csv('history.csv')"
   ]
  }
 ]
}