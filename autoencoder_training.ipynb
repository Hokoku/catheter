{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3da45907e5cd41bdac7b692b424c19725633d98a853f93b11b400b1d8951ac30"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/30083 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9cc56b7e0d354ad3bab1157a6e2233ec"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "dataset_dir=\"../input/ranzcr-clip-catheter-line-classification/\"\n",
    "train=pd.read_csv(f\"{dataset_dir}train.csv\")\n",
    "\n",
    "npz=np.load(\"../input/effnet_tuned_output.npz\")\n",
    "features_list=[npz[uid] for uid in tqdm(train[\"StudyInstanceUID\"])]\n",
    "features=np.array(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(features,save_scaler=True):\n",
    "    scaler=MinMaxScaler()\n",
    "    scaler.fit(features)\n",
    "    if save_scaler:\n",
    "        pickle.dump(scaler,open(\"./models/minmaxscaler_effnet_tuned.pickle\",\"wb\"))\n",
    "    X=scaler.transform(features)\n",
    "    return X\n",
    "\n",
    "X=pd.DataFrame(normalize(features,save_scaler=False))"
   ]
  },
  {
   "source": [
    "## Dropout\n",
    "直前の層に対してドロップアウトを適用する, 与えられた確率に従ってノードの一部を無効化する\n",
    "\n",
    "## Sigmoid\n",
    "活性化関数の1つ, \\[0,1]の範囲を出力する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=train.copy()\n",
    "group_kfold=GroupKFold(n_splits=10)\n",
    "for n,(train_idx,val_idx) in enumerate(group_kfold.split(train,groups=train[\"PatientID\"])):\n",
    "    fold.loc[val_idx,\"folds\"]=n\n",
    "fold[\"folds\"]=fold[\"folds\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "           0         1         2         3         4         5         6     \\\n0      0.190440  0.385044  0.443073  0.310448  0.346378  0.100379  0.092547   \n1      0.206376  0.344567  0.164791  0.288545  0.321263  0.444134  0.422946   \n2      0.226800  0.252439  0.242553  0.289306  0.322075  0.223481  0.261045   \n3      0.194851  0.191336  0.341818  0.273755  0.151161  0.027027  0.271664   \n4      0.287443  0.426359  0.383683  0.361628  0.238865  0.493671  0.223484   \n...         ...       ...       ...       ...       ...       ...       ...   \n30078  0.305132  0.244647  0.263042  0.293138  0.289404  0.203950  0.146409   \n30079  0.344102  0.206995  0.246667  0.204177  0.110451  0.176185  0.236174   \n30080  0.204613  0.183264  0.221124  0.176001  0.288095  0.234859  0.130213   \n30081  0.153266  0.271216  0.264500  0.270118  0.553315  0.195615  0.260337   \n30082  0.291793  0.313166  0.250715  0.389500  0.148199  0.303245  0.401956   \n\n           7         8         9     ...      2550      2551      2552  \\\n0      0.436334  0.156383  0.402474  ...  0.337346  0.326803  0.372029   \n1      0.451133  0.157484  0.204861  ...  0.235174  0.411999  0.225011   \n2      0.265227  0.253216  0.223972  ...  0.229770  0.377200  0.246991   \n3      0.462765  0.286703  0.270541  ...  0.292065  0.227323  0.311901   \n4      0.452848  0.250816  0.364319  ...  0.369150  0.213200  0.345335   \n...         ...       ...       ...  ...       ...       ...       ...   \n30078  0.364405  0.221869  0.226957  ...  0.244204  0.408309  0.266780   \n30079  0.367238  0.195823  0.212698  ...  0.255028  0.166039  0.246962   \n30080  0.395777  0.184253  0.172003  ...  0.292246  0.436881  0.179868   \n30081  0.370405  0.289186  0.236477  ...  0.289630  0.553747  0.251055   \n30082  0.298786  0.501746  0.204110  ...  0.429975  0.204596  0.245315   \n\n           2553      2554      2555      2556      2557      2558      2559  \n0      0.612123  0.039223  0.261128  0.387774  0.152095  0.375815  0.373878  \n1      0.209855  0.008203  0.224002  0.226538  0.651499  0.141680  0.172821  \n2      0.349057  0.477504  0.249797  0.207370  0.375446  0.269789  0.228729  \n3      0.415648  0.327432  0.314091  0.281705  0.519249  0.249901  0.312677  \n4      0.267122  0.239928  0.305450  0.357422  0.368382  0.334064  0.341459  \n...         ...       ...       ...       ...       ...       ...       ...  \n30078  0.296242  0.007972  0.205312  0.218823  0.213217  0.285859  0.241543  \n30079  0.251577  0.382749  0.228371  0.221398  0.462461  0.208752  0.215964  \n30080  0.262722  0.009253  0.232791  0.212802  0.258772  0.149610  0.217615  \n30081  0.444548  0.398964  0.270383  0.212072  0.436539  0.241500  0.287819  \n30082  0.185759  0.124842  0.465615  0.274862  0.584861  0.355920  0.281578  \n\n[27074 rows x 2560 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>2550</th>\n      <th>2551</th>\n      <th>2552</th>\n      <th>2553</th>\n      <th>2554</th>\n      <th>2555</th>\n      <th>2556</th>\n      <th>2557</th>\n      <th>2558</th>\n      <th>2559</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.190440</td>\n      <td>0.385044</td>\n      <td>0.443073</td>\n      <td>0.310448</td>\n      <td>0.346378</td>\n      <td>0.100379</td>\n      <td>0.092547</td>\n      <td>0.436334</td>\n      <td>0.156383</td>\n      <td>0.402474</td>\n      <td>...</td>\n      <td>0.337346</td>\n      <td>0.326803</td>\n      <td>0.372029</td>\n      <td>0.612123</td>\n      <td>0.039223</td>\n      <td>0.261128</td>\n      <td>0.387774</td>\n      <td>0.152095</td>\n      <td>0.375815</td>\n      <td>0.373878</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.206376</td>\n      <td>0.344567</td>\n      <td>0.164791</td>\n      <td>0.288545</td>\n      <td>0.321263</td>\n      <td>0.444134</td>\n      <td>0.422946</td>\n      <td>0.451133</td>\n      <td>0.157484</td>\n      <td>0.204861</td>\n      <td>...</td>\n      <td>0.235174</td>\n      <td>0.411999</td>\n      <td>0.225011</td>\n      <td>0.209855</td>\n      <td>0.008203</td>\n      <td>0.224002</td>\n      <td>0.226538</td>\n      <td>0.651499</td>\n      <td>0.141680</td>\n      <td>0.172821</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.226800</td>\n      <td>0.252439</td>\n      <td>0.242553</td>\n      <td>0.289306</td>\n      <td>0.322075</td>\n      <td>0.223481</td>\n      <td>0.261045</td>\n      <td>0.265227</td>\n      <td>0.253216</td>\n      <td>0.223972</td>\n      <td>...</td>\n      <td>0.229770</td>\n      <td>0.377200</td>\n      <td>0.246991</td>\n      <td>0.349057</td>\n      <td>0.477504</td>\n      <td>0.249797</td>\n      <td>0.207370</td>\n      <td>0.375446</td>\n      <td>0.269789</td>\n      <td>0.228729</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.194851</td>\n      <td>0.191336</td>\n      <td>0.341818</td>\n      <td>0.273755</td>\n      <td>0.151161</td>\n      <td>0.027027</td>\n      <td>0.271664</td>\n      <td>0.462765</td>\n      <td>0.286703</td>\n      <td>0.270541</td>\n      <td>...</td>\n      <td>0.292065</td>\n      <td>0.227323</td>\n      <td>0.311901</td>\n      <td>0.415648</td>\n      <td>0.327432</td>\n      <td>0.314091</td>\n      <td>0.281705</td>\n      <td>0.519249</td>\n      <td>0.249901</td>\n      <td>0.312677</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.287443</td>\n      <td>0.426359</td>\n      <td>0.383683</td>\n      <td>0.361628</td>\n      <td>0.238865</td>\n      <td>0.493671</td>\n      <td>0.223484</td>\n      <td>0.452848</td>\n      <td>0.250816</td>\n      <td>0.364319</td>\n      <td>...</td>\n      <td>0.369150</td>\n      <td>0.213200</td>\n      <td>0.345335</td>\n      <td>0.267122</td>\n      <td>0.239928</td>\n      <td>0.305450</td>\n      <td>0.357422</td>\n      <td>0.368382</td>\n      <td>0.334064</td>\n      <td>0.341459</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>0.305132</td>\n      <td>0.244647</td>\n      <td>0.263042</td>\n      <td>0.293138</td>\n      <td>0.289404</td>\n      <td>0.203950</td>\n      <td>0.146409</td>\n      <td>0.364405</td>\n      <td>0.221869</td>\n      <td>0.226957</td>\n      <td>...</td>\n      <td>0.244204</td>\n      <td>0.408309</td>\n      <td>0.266780</td>\n      <td>0.296242</td>\n      <td>0.007972</td>\n      <td>0.205312</td>\n      <td>0.218823</td>\n      <td>0.213217</td>\n      <td>0.285859</td>\n      <td>0.241543</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>0.344102</td>\n      <td>0.206995</td>\n      <td>0.246667</td>\n      <td>0.204177</td>\n      <td>0.110451</td>\n      <td>0.176185</td>\n      <td>0.236174</td>\n      <td>0.367238</td>\n      <td>0.195823</td>\n      <td>0.212698</td>\n      <td>...</td>\n      <td>0.255028</td>\n      <td>0.166039</td>\n      <td>0.246962</td>\n      <td>0.251577</td>\n      <td>0.382749</td>\n      <td>0.228371</td>\n      <td>0.221398</td>\n      <td>0.462461</td>\n      <td>0.208752</td>\n      <td>0.215964</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>0.204613</td>\n      <td>0.183264</td>\n      <td>0.221124</td>\n      <td>0.176001</td>\n      <td>0.288095</td>\n      <td>0.234859</td>\n      <td>0.130213</td>\n      <td>0.395777</td>\n      <td>0.184253</td>\n      <td>0.172003</td>\n      <td>...</td>\n      <td>0.292246</td>\n      <td>0.436881</td>\n      <td>0.179868</td>\n      <td>0.262722</td>\n      <td>0.009253</td>\n      <td>0.232791</td>\n      <td>0.212802</td>\n      <td>0.258772</td>\n      <td>0.149610</td>\n      <td>0.217615</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>0.153266</td>\n      <td>0.271216</td>\n      <td>0.264500</td>\n      <td>0.270118</td>\n      <td>0.553315</td>\n      <td>0.195615</td>\n      <td>0.260337</td>\n      <td>0.370405</td>\n      <td>0.289186</td>\n      <td>0.236477</td>\n      <td>...</td>\n      <td>0.289630</td>\n      <td>0.553747</td>\n      <td>0.251055</td>\n      <td>0.444548</td>\n      <td>0.398964</td>\n      <td>0.270383</td>\n      <td>0.212072</td>\n      <td>0.436539</td>\n      <td>0.241500</td>\n      <td>0.287819</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>0.291793</td>\n      <td>0.313166</td>\n      <td>0.250715</td>\n      <td>0.389500</td>\n      <td>0.148199</td>\n      <td>0.303245</td>\n      <td>0.401956</td>\n      <td>0.298786</td>\n      <td>0.501746</td>\n      <td>0.204110</td>\n      <td>...</td>\n      <td>0.429975</td>\n      <td>0.204596</td>\n      <td>0.245315</td>\n      <td>0.185759</td>\n      <td>0.124842</td>\n      <td>0.465615</td>\n      <td>0.274862</td>\n      <td>0.584861</td>\n      <td>0.355920</td>\n      <td>0.281578</td>\n    </tr>\n  </tbody>\n</table>\n<p>27074 rows × 2560 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "target_fold=0\n",
    "train_idx=(fold[\"folds\"]!=target_fold)\n",
    "val_idx=(fold[\"folds\"]==target_fold)\n",
    "\n",
    "X_train=X[train_idx]\n",
    "X_val=X[val_idx]\n",
    "display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 1800)              4609800   \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               180100    \n_________________________________________________________________\ndense_2 (Dense)              (None, 1800)              181800    \n_________________________________________________________________\ndropout (Dropout)            (None, 1800)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 2560)              4610560   \n=================================================================\nTotal params: 9,582,260\nTrainable params: 9,582,260\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape=(2560,)\n",
    "encoding_dim=100\n",
    "\n",
    "model=keras.Sequential([\n",
    "    Dense(encoding_dim*18,activation=\"relu\",input_shape=input_shape),\n",
    "    Dense(encoding_dim,activation=\"relu\"),\n",
    "    Dense(encoding_dim*18,activation=\"relu\"),\n",
    "    Dropout(0.1),\n",
    "    Dense(input_shape[0],activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/400\n",
      "218/847 [======>.......................] - ETA: 58s - loss: 0.0155"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e51a49a6f22d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m ]\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./models/autoencoder_effnet_tuned\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "adam=keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=adam,loss=\"mse\")\n",
    "\n",
    "fit_callbacks=[\n",
    "    callbacks.EarlyStopping(monitor=\"val_loss\",patience=20,mode=\"min\"),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\",patience=30,min_lr=1e-7,mode=\"min\",factor=0.5,verbose=1),\n",
    "    callbacks.ModelCheckpoint(\"./models/autoencoder_tuned/ckpt\",monitor=\"val_loss\",mode=\"min\",\n",
    "        save_weights_only=True,save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(X_train,X_train,epochs=200,shuffle=True,validation_data=(X_val,X_val),callbacks=fit_callbacks)\n",
    "\n",
    "model.save(\"./models/autoencoder_effnet_tuned\")\n",
    "\n",
    "score=model.evaluate(X_val,X_val,verbose=0)\n",
    "print(f\"\\nscore: {score}\")"
   ]
  },
  {
   "source": [
    "## score\n",
    "lr e-3: 0.0138  \n",
    "lr e-4, layers \\*2: 0.013181226328015327  \n",
    "lr e-3, layers \\*4 \\*2 : 0.014  \n",
    "lr e-4, layers \\*4: 0.013148400001227856  \n",
    "lr e-4, layers \\*4, normalize: 0.0025459351018071175    \n",
    "lr e-4, layers \\*8, normalize: 0.0024564287159591913  \n",
    "lr 1e-4, layers \\*16, norm: 0.0023377728648483753  \n",
    "lr 1e-4, layers \\*16 \\*4, norm: 0.002483354415744543  \n",
    "lr 1e-3, layers \\*16 \\*4, norm: 0.0037389046046882868  \n",
    "lr 1e-3, layers \\*16 \\*2, norm: 0.0035295113921165466  \n",
    "lr 1e-4, layers \\*16 \\*2, norm: 0.0023819475900381804  \n",
    "lr 1e-4, layers \\*18, norm: **0.0021908902563154697**  \n",
    "lr 1e-4, layers \\*20, norm: 0.002232010243460536  \n",
    "lr 1e-4, layers none, norm: 0.0029496345669031143  \n",
    "lr 1e-4, layers \\*18 dropout, norm:  \n",
    "lr 1e-4, layers \\*18, dropout 0.2, norm: 0.0023323060013353825  \n",
    "lr 1e-4, layers \\*18, dropout 0.1, norm: 0.002210501581430435  \n",
    "lr reducer, layers dropuout norm same: 0.0022298386320471764  \n",
    "lr reducer ,epochs 500, others same: 0.0021655228920280933  \n",
    "groupkfold, others same: 0.0022280211560428143  \n",
    "groupkfold, dropout 0.05, others same: 0.002246299758553505  \n",
    "groupkfold, batchnorm, without dropout: 0.003952326253056526  \n",
    "groupkfold, without batchnorm dropout: 0.0022722359281033278  \n",
    "groupkfold splits10, dropout 0.1, no batchnorm, others same: **0.002182652475312352**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}