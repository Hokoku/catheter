{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3da45907e5cd41bdac7b692b424c19725633d98a853f93b11b400b1d8951ac30"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 30083/30083 [00:48<00:00, 621.20it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_dir=\"../input/ranzcr-clip-catheter-line-classification/\"\n",
    "train=pd.read_csv(f\"{dataset_dir}train.csv\")\n",
    "n_folds=10\n",
    "\n",
    "npz=np.load(\"../input/effnet_best_output.npz\")\n",
    "features_list=[npz[uid] for uid in tqdm(train[\"StudyInstanceUID\"])]\n",
    "features=np.array(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(features,save_scaler=True):\n",
    "    scaler=MinMaxScaler()\n",
    "    scaler.fit(features)\n",
    "    if save_scaler:\n",
    "        pickle.dump(scaler,open(\"./models/minmaxscaler_effnet_best.pickle\",\"wb\"))\n",
    "    X=scaler.transform(features)\n",
    "    return X\n",
    "\n",
    "X=pd.DataFrame(normalize(features,save_scaler=True))"
   ]
  },
  {
   "source": [
    "## Dropout\n",
    "直前の層に対してドロップアウトを適用する, 与えられた確率に従ってノードの一部を無効化する\n",
    "\n",
    "## Sigmoid\n",
    "活性化関数の1つ, \\[0,1]の範囲を出力する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold(train):\n",
    "    fold=train.copy()\n",
    "    splitter=GroupKFold(n_splits=n_folds)\n",
    "    for n,(train_idx,val_idx) in enumerate(splitter.split(train,groups=train[\"PatientID\"])):\n",
    "        fold.loc[val_idx,\"folds\"]=n\n",
    "    fold[\"folds\"]=fold[\"folds\"].astype(int)\n",
    "    return fold\n",
    "\n",
    "fold=get_fold(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "           0         1         2         3         4         5         6     \\\n0      0.637893  0.131938  0.104526  0.063978  0.072000  0.029705  0.060279   \n1      0.024593  0.048640  0.046269  0.021631  0.201261  0.559943  0.127162   \n2      0.450534  0.011428  0.271354  0.319025  0.032320  0.020797  0.016488   \n3      0.354269  0.044961  0.113954  0.234826  0.074535  0.033150  0.047977   \n4      0.457785  0.012760  0.302983  0.305707  0.050934  0.712111  0.018399   \n...         ...       ...       ...       ...       ...       ...       ...   \n30078  0.005345  0.013495  0.097013  0.038049  0.033289  0.342629  0.089725   \n30079  0.458205  0.059922  0.298080  0.332596  0.054697  0.579604  0.044275   \n30080  0.007546  0.098454  0.047721  0.012569  0.100689  0.032487  0.075171   \n30081  0.500305  0.042702  0.189077  0.358666  0.015022  0.049566  0.008496   \n30082  0.017609  0.007843  0.066084  0.039125  0.083923  0.393373  0.043457   \n\n           7         8         9     ...      2550      2551      2552  \\\n0      0.622218  0.247831  0.295754  ...  0.215813  0.064823  0.248789   \n1      0.080956  0.017294  0.063737  ...  0.035085  0.395627  0.100634   \n2      0.423592  0.192775  0.378316  ...  0.273190  0.098082  0.599870   \n3      0.286969  0.139299  0.225637  ...  0.140248  0.147160  0.476094   \n4      0.521195  0.248089  0.446346  ...  0.342668  0.061332  0.531756   \n...         ...       ...       ...  ...       ...       ...       ...   \n30078  0.226423  0.023725  0.402143  ...  0.208133  0.212049  0.092566   \n30079  0.480555  0.256309  0.362362  ...  0.294675  0.045970  0.626358   \n30080  0.195573  0.010546  0.178106  ...  0.055352  0.225094  0.072842   \n30081  0.269850  0.173320  0.250008  ...  0.168985  0.061154  0.493127   \n30082  0.184529  0.008917  0.315131  ...  0.125139  0.098803  0.246455   \n\n           2553      2554      2555      2556      2557      2558      2559  \n0      0.222172  0.018390  0.115120  0.523893  0.046372  0.015596  0.088412  \n1      0.029711  0.008340  0.471350  0.099166  0.095559  0.350444  0.269512  \n2      0.176126  0.269714  0.020955  0.590382  0.680041  0.260838  0.350846  \n3      0.229894  0.165611  0.026377  0.267940  0.351040  0.215919  0.058292  \n4      0.183002  0.227087  0.373302  0.671849  0.622573  0.217722  0.469270  \n...         ...       ...       ...       ...       ...       ...       ...  \n30078  0.008017  0.203990  0.489665  0.103791  0.049335  0.280249  0.107862  \n30079  0.224780  0.268795  0.384345  0.598721  0.654069  0.359651  0.298022  \n30080  0.013784  0.005990  0.251083  0.061592  0.054397  0.353068  0.035504  \n30081  0.166074  0.195504  0.025113  0.394580  0.609913  0.219251  0.190061  \n30082  0.009958  0.382796  0.281436  0.124760  0.049361  0.644474  0.403664  \n\n[27074 rows x 2560 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>2550</th>\n      <th>2551</th>\n      <th>2552</th>\n      <th>2553</th>\n      <th>2554</th>\n      <th>2555</th>\n      <th>2556</th>\n      <th>2557</th>\n      <th>2558</th>\n      <th>2559</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.637893</td>\n      <td>0.131938</td>\n      <td>0.104526</td>\n      <td>0.063978</td>\n      <td>0.072000</td>\n      <td>0.029705</td>\n      <td>0.060279</td>\n      <td>0.622218</td>\n      <td>0.247831</td>\n      <td>0.295754</td>\n      <td>...</td>\n      <td>0.215813</td>\n      <td>0.064823</td>\n      <td>0.248789</td>\n      <td>0.222172</td>\n      <td>0.018390</td>\n      <td>0.115120</td>\n      <td>0.523893</td>\n      <td>0.046372</td>\n      <td>0.015596</td>\n      <td>0.088412</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.024593</td>\n      <td>0.048640</td>\n      <td>0.046269</td>\n      <td>0.021631</td>\n      <td>0.201261</td>\n      <td>0.559943</td>\n      <td>0.127162</td>\n      <td>0.080956</td>\n      <td>0.017294</td>\n      <td>0.063737</td>\n      <td>...</td>\n      <td>0.035085</td>\n      <td>0.395627</td>\n      <td>0.100634</td>\n      <td>0.029711</td>\n      <td>0.008340</td>\n      <td>0.471350</td>\n      <td>0.099166</td>\n      <td>0.095559</td>\n      <td>0.350444</td>\n      <td>0.269512</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.450534</td>\n      <td>0.011428</td>\n      <td>0.271354</td>\n      <td>0.319025</td>\n      <td>0.032320</td>\n      <td>0.020797</td>\n      <td>0.016488</td>\n      <td>0.423592</td>\n      <td>0.192775</td>\n      <td>0.378316</td>\n      <td>...</td>\n      <td>0.273190</td>\n      <td>0.098082</td>\n      <td>0.599870</td>\n      <td>0.176126</td>\n      <td>0.269714</td>\n      <td>0.020955</td>\n      <td>0.590382</td>\n      <td>0.680041</td>\n      <td>0.260838</td>\n      <td>0.350846</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.354269</td>\n      <td>0.044961</td>\n      <td>0.113954</td>\n      <td>0.234826</td>\n      <td>0.074535</td>\n      <td>0.033150</td>\n      <td>0.047977</td>\n      <td>0.286969</td>\n      <td>0.139299</td>\n      <td>0.225637</td>\n      <td>...</td>\n      <td>0.140248</td>\n      <td>0.147160</td>\n      <td>0.476094</td>\n      <td>0.229894</td>\n      <td>0.165611</td>\n      <td>0.026377</td>\n      <td>0.267940</td>\n      <td>0.351040</td>\n      <td>0.215919</td>\n      <td>0.058292</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.457785</td>\n      <td>0.012760</td>\n      <td>0.302983</td>\n      <td>0.305707</td>\n      <td>0.050934</td>\n      <td>0.712111</td>\n      <td>0.018399</td>\n      <td>0.521195</td>\n      <td>0.248089</td>\n      <td>0.446346</td>\n      <td>...</td>\n      <td>0.342668</td>\n      <td>0.061332</td>\n      <td>0.531756</td>\n      <td>0.183002</td>\n      <td>0.227087</td>\n      <td>0.373302</td>\n      <td>0.671849</td>\n      <td>0.622573</td>\n      <td>0.217722</td>\n      <td>0.469270</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30078</th>\n      <td>0.005345</td>\n      <td>0.013495</td>\n      <td>0.097013</td>\n      <td>0.038049</td>\n      <td>0.033289</td>\n      <td>0.342629</td>\n      <td>0.089725</td>\n      <td>0.226423</td>\n      <td>0.023725</td>\n      <td>0.402143</td>\n      <td>...</td>\n      <td>0.208133</td>\n      <td>0.212049</td>\n      <td>0.092566</td>\n      <td>0.008017</td>\n      <td>0.203990</td>\n      <td>0.489665</td>\n      <td>0.103791</td>\n      <td>0.049335</td>\n      <td>0.280249</td>\n      <td>0.107862</td>\n    </tr>\n    <tr>\n      <th>30079</th>\n      <td>0.458205</td>\n      <td>0.059922</td>\n      <td>0.298080</td>\n      <td>0.332596</td>\n      <td>0.054697</td>\n      <td>0.579604</td>\n      <td>0.044275</td>\n      <td>0.480555</td>\n      <td>0.256309</td>\n      <td>0.362362</td>\n      <td>...</td>\n      <td>0.294675</td>\n      <td>0.045970</td>\n      <td>0.626358</td>\n      <td>0.224780</td>\n      <td>0.268795</td>\n      <td>0.384345</td>\n      <td>0.598721</td>\n      <td>0.654069</td>\n      <td>0.359651</td>\n      <td>0.298022</td>\n    </tr>\n    <tr>\n      <th>30080</th>\n      <td>0.007546</td>\n      <td>0.098454</td>\n      <td>0.047721</td>\n      <td>0.012569</td>\n      <td>0.100689</td>\n      <td>0.032487</td>\n      <td>0.075171</td>\n      <td>0.195573</td>\n      <td>0.010546</td>\n      <td>0.178106</td>\n      <td>...</td>\n      <td>0.055352</td>\n      <td>0.225094</td>\n      <td>0.072842</td>\n      <td>0.013784</td>\n      <td>0.005990</td>\n      <td>0.251083</td>\n      <td>0.061592</td>\n      <td>0.054397</td>\n      <td>0.353068</td>\n      <td>0.035504</td>\n    </tr>\n    <tr>\n      <th>30081</th>\n      <td>0.500305</td>\n      <td>0.042702</td>\n      <td>0.189077</td>\n      <td>0.358666</td>\n      <td>0.015022</td>\n      <td>0.049566</td>\n      <td>0.008496</td>\n      <td>0.269850</td>\n      <td>0.173320</td>\n      <td>0.250008</td>\n      <td>...</td>\n      <td>0.168985</td>\n      <td>0.061154</td>\n      <td>0.493127</td>\n      <td>0.166074</td>\n      <td>0.195504</td>\n      <td>0.025113</td>\n      <td>0.394580</td>\n      <td>0.609913</td>\n      <td>0.219251</td>\n      <td>0.190061</td>\n    </tr>\n    <tr>\n      <th>30082</th>\n      <td>0.017609</td>\n      <td>0.007843</td>\n      <td>0.066084</td>\n      <td>0.039125</td>\n      <td>0.083923</td>\n      <td>0.393373</td>\n      <td>0.043457</td>\n      <td>0.184529</td>\n      <td>0.008917</td>\n      <td>0.315131</td>\n      <td>...</td>\n      <td>0.125139</td>\n      <td>0.098803</td>\n      <td>0.246455</td>\n      <td>0.009958</td>\n      <td>0.382796</td>\n      <td>0.281436</td>\n      <td>0.124760</td>\n      <td>0.049361</td>\n      <td>0.644474</td>\n      <td>0.403664</td>\n    </tr>\n  </tbody>\n</table>\n<p>27074 rows × 2560 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "target_fold=0\n",
    "train_idx=(fold[\"folds\"]!=target_fold)\n",
    "val_idx=(fold[\"folds\"]==target_fold)\n",
    "\n",
    "X_train=X[train_idx]\n",
    "X_val=X[val_idx]\n",
    "display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 1800)              4609800   \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               180100    \n_________________________________________________________________\ndense_2 (Dense)              (None, 1800)              181800    \n_________________________________________________________________\ndropout (Dropout)            (None, 1800)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 2560)              4610560   \n=================================================================\nTotal params: 9,582,260\nTrainable params: 9,582,260\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape=(2560,)\n",
    "encoding_dim=100\n",
    "\n",
    "model=keras.Sequential([\n",
    "    Dense(encoding_dim*18,activation=\"relu\",input_shape=input_shape),\n",
    "    Dense(encoding_dim,activation=\"relu\"),\n",
    "    Dense(encoding_dim*18,activation=\"relu\"),\n",
    "    Dropout(0.1),\n",
    "    Dense(input_shape[0],activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      " 63/847 [=>............................] - ETA: 1:07 - loss: 0.0445"
     ]
    }
   ],
   "source": [
    "adam=keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=adam,loss=\"mse\")\n",
    "\n",
    "fit_callbacks=[\n",
    "    callbacks.EarlyStopping(monitor=\"val_loss\",patience=50,mode=\"min\"),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\",patience=30,min_lr=1e-7,mode=\"min\",factor=0.5,verbose=1),\n",
    "    callbacks.ModelCheckpoint(\"./models/autoencoder_best/ckpt\",monitor=\"val_loss\",mode=\"min\",\n",
    "        save_weights_only=True,save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(X_train,X_train,epochs=150,shuffle=True,validation_data=(X_val,X_val),callbacks=fit_callbacks)\n",
    "\n",
    "score=model.evaluate(X_val,X_val,verbose=0)\n",
    "print(f\"\\nscore: {score}\")"
   ]
  },
  {
   "source": [
    "## score\n",
    "lr e-3: 0.0138  \n",
    "lr e-4, layers \\*2: 0.013181226328015327  \n",
    "lr e-3, layers \\*4 \\*2 : 0.014  \n",
    "lr e-4, layers \\*4: 0.013148400001227856  \n",
    "lr e-4, layers \\*4, normalize: 0.0025459351018071175    \n",
    "lr e-4, layers \\*8, normalize: 0.0024564287159591913  \n",
    "lr 1e-4, layers \\*16, norm: 0.0023377728648483753  \n",
    "lr 1e-4, layers \\*16 \\*4, norm: 0.002483354415744543  \n",
    "lr 1e-3, layers \\*16 \\*4, norm: 0.0037389046046882868  \n",
    "lr 1e-3, layers \\*16 \\*2, norm: 0.0035295113921165466  \n",
    "lr 1e-4, layers \\*16 \\*2, norm: 0.0023819475900381804  \n",
    "lr 1e-4, layers \\*18, norm: **0.0021908902563154697**  \n",
    "lr 1e-4, layers \\*20, norm: 0.002232010243460536  \n",
    "lr 1e-4, layers none, norm: 0.0029496345669031143  \n",
    "lr 1e-4, layers \\*18 dropout, norm:  \n",
    "lr 1e-4, layers \\*18, dropout 0.2, norm: 0.0023323060013353825  \n",
    "lr 1e-4, layers \\*18, dropout 0.1, norm: 0.002210501581430435  \n",
    "lr reducer, layers dropuout norm same: 0.0022298386320471764  \n",
    "lr reducer ,epochs 500, others same: 0.0021655228920280933  \n",
    "groupkfold, others same: 0.0022280211560428143  \n",
    "groupkfold, dropout 0.05, others same: 0.002246299758553505  \n",
    "groupkfold, batchnorm, without dropout: 0.003952326253056526  \n",
    "groupkfold, without batchnorm dropout: 0.0022722359281033278  \n",
    "groupkfold splits10, dropout 0.1, no batchnorm, others same: **0.002182652475312352**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}